{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from locale import atof, setlocale, LC_NUMERIC, LC_ALL\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "#setlocale(LC_ALL, 'fr_FR.UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GetData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class GetData:\n",
    "    \"\"\" Read data from csv and load it in a dataframe\n",
    "    accepted arguments : path to file , separator, chunksize and filter\n",
    "    option to load csv by filtering on house type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,path =\"../data/valeursfoncieres-2021.txt\",sep = \"|\", chunksize = 100000):\n",
    "        self.path = path\n",
    "        self.sep = sep\n",
    "        self.chunksize = chunksize\n",
    "\n",
    "\n",
    "    def read_csv(self, filtering_column='Code type local', filter=[1]):\n",
    "        \"\"\" pass option on which column to filter and filter value\n",
    "        if several filter value, pass the as a list\"\"\"\n",
    "        iter_csv = pd.read_csv(self.path,\n",
    "                               sep=self.sep,\n",
    "                               iterator=True,\n",
    "                               chunksize=self.chunksize,\n",
    "                               low_memory=False)\n",
    "        self.df = pd.concat([\n",
    "            chunk[chunk[filtering_column].isin(filter)] for chunk in iter_csv\n",
    "        ])\n",
    "        return self.df\n",
    "\n",
    "    def enrichissement_coordinates(self,df):\n",
    "        pass\n",
    "\n",
    "    def enrichissement_insee(self,df):\n",
    "        pass\n",
    "\n",
    "    def loading_data_db (self,df):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout l'objectif de cette étape est de tirer le maximum du dataset, en reconstituant ce qui correspond réellement à une transaction. \n",
    "Nous avons réalisé cela en regroupant chaque transaction selon 3 clés : Parcelle / date / montant car ceux ci sont répété sur les lignes communes. \n",
    "Ensuite nous avons utilisé la methode apply qui pour chaque ligne du dataset aggrégé applique une fonction lambda dans laquelle on inséré une série dont les paramétres étaient la ligne aggrée. cela nous a permis de réaliser des fonctions plus complexes au niveau des aggrégats et de couvrir les différents cas ammenant à la mutltiplication des lignes \n",
    "Les cas plus complexes à retrouver sont par exemple la nature culture basée principale en se basant sur la superficie de celle ci (--- piste https://stackoverflow.com/questions/23394476/keep-other-columns-when-doing-groupby \n",
    "L'autre cas complexe était de compter le nombre de dépendances différentes ainsi que le nombre de maisons différentes au sein d'une même aggrégation -**ajouter cas ou on a plusieurs dépendances et plusieurs terrains -- trop complexe** : \n",
    "* #dépendance - count du terme dépendance valuecount sorted[0]  / nunique type local si nunique nat culture = 1 & len value count ==2 else count terme dependance valuecount sorted[0]   if len_value count ==2 else 0\n",
    "* #maisons - count terme maison value count sorted [1] / nunqiue type local si nunique nat culture = 1 & len value count ==2 else count terme maison value count if len value_count ==2 else 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from house_prediction_package.data import GetData\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from more_itertools import chunked\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#sans doute à supprimer au lancement final du modele\n",
    "#from locale import atof, setlocale, LC_NUMERIC, LC_ALL\n",
    "\n",
    "#setlocale(LC_ALL, 'fr_FR.UTF-8')\n",
    "\n",
    "class Preprocessing :\n",
    "\n",
    "    def __init__(self,df) :\n",
    "        # self.df = get_data().read_csv()\n",
    "        self.df = df\n",
    "\n",
    "    def clean_columns(self,\n",
    "                      columns=[\n",
    "                          'Code service CH', 'Reference document',\n",
    "                          '1 Articles CGI', '2 Articles CGI', '3 Articles CGI',\n",
    "                          '4 Articles CGI', '5 Articles CGI', 'No Volume',\n",
    "                          'Identifiant local'\n",
    "                      ]):\n",
    "        \"\"\" drop useless columns\n",
    "        Customisation of columns to drop must be entered as a list\n",
    "        \"\"\"\n",
    "        # suppression of 100% empty columns - these columns are officially not completed in this db\n",
    "        self.df = self.df.drop(columns,axis=1)\n",
    "        # suppression of columns poorly completed\n",
    "        columns_to_drop = [column for column in self.df.columns if ((self.df[column].isnull().value_counts().sort_index()[0]/self.df.shape[0])*100) < 2 ]\n",
    "        self.df= self.df.drop(columns_to_drop,axis=1)\n",
    "        # replacement of , by . in numerical variables & deletion of non numrical caracters in num columns : \n",
    "        columns_num = ['Valeur fonciere', 'Surface Carrez du 1er lot', 'Nombre de lots',\n",
    "        'Surface reelle bati', 'Nombre pieces principales', 'Surface terrain']\n",
    "        # transformation des , en . pour réaliser des opérations sur les nombres et suppressions des caracteres non numériques au sein de ces colonnes \n",
    "        for column in columns_num : \n",
    "            self.df[column]=self.df[column].apply(lambda s: s.replace(\",\",\".\") if isinstance(s,str) else s)\n",
    "            self.df[column] = pd.to_numeric(self.df[column], errors = 'coerce')\n",
    "        # suppression of nan value on target variable\n",
    "        self.df= self.df.dropna(subset=['Valeur fonciere'])\n",
    "        #self.df['Surface Carrez du 1er lot'] = self.df['Surface Carrez du 1er lot'].apply(\n",
    "        #    lambda x: atof(x))        \n",
    "        # pre processing avant groupby mais attention sortir valeures foncieres avant de mettre en POO\n",
    "        ob_columns= self.df.dtypes[self.df.dtypes == 'O'].index\n",
    "        num_columns = self.df.dtypes[(self.df.dtypes == 'int')\n",
    "                                     | (self.df.dtypes == 'float')].index\n",
    "        non_num_col = ['No disposition', 'No voie', 'Code postal', 'Code commune',\n",
    "       'Prefixe de section', 'No plan','Code type local']\n",
    "        num_columns = [value for value in num_columns if value not in non_num_col]\n",
    "        for column in ob_columns :\n",
    "            self.df[column]=self.df[column].replace(np.nan,'',regex=True)\n",
    "        #à adapter in v2\n",
    "        \n",
    "        self.df[num_columns] = self.df[num_columns].apply(pd.to_numeric,\n",
    "                                                              errors='coerce')\n",
    "        \n",
    "        #drop duplicates\n",
    "        self.df = self.df.drop_duplicates().reset_index(drop= True)\n",
    "        # by returning self, we can do method chaining like preprocessing(df).clean_columns().create_identifier()\n",
    "        return self.df\n",
    "\n",
    "    def create_identifier(self) :\n",
    "        \"\"\" Create a 'unique' identifier allowing us to group several lines corresponding to a unique transaction\n",
    "        \"\"\"\n",
    "        variables_to_clean = [\n",
    "            \"Code departement\", \"Code commune\", \"Prefixe de section\",\n",
    "            \"Section\", \"No plan\"\n",
    "            ]\n",
    "        size_variables= [2,3,3,2,4]\n",
    "        for i,j in zip(variables_to_clean,size_variables):\n",
    "            chunked_data = chunked(self.df[i], 10000, strict=False)\n",
    "            values = {\"Prefixe de section\": '000'}\n",
    "            self.df= self.df.fillna(value=values)\n",
    "            if i == \"Prefixe de section\" :\n",
    "                self.df[i] = self.df[i].apply(str).apply(lambda x: x[:3])\n",
    "            new_variable = [\n",
    "                str(value).replace(\".\",\"\").zfill(j) for sublist in list(chunked_data)\n",
    "                for value in sublist\n",
    "            ]\n",
    "            self.df[f\"clean_{i.replace(' ','_').lower()}\"] = new_variable\n",
    "            self.df= self.df.drop([i],axis=1)\n",
    "        self.df[\"parcelle_cadastrale\"] = self.df[[\n",
    "            \"clean_code_departement\", \"clean_code_commune\", \"clean_prefixe_de_section\",\n",
    "            \"clean_section\", \"clean_no_plan\"]].apply(lambda x: \"\".join(x), axis=1)\n",
    "        self.df[\"parcelle_cad_section\"]=self.df[\"parcelle_cadastrale\"].str[:10]\n",
    "        self.df = self.df.drop([\n",
    "            \"clean_prefixe_de_section\", \"clean_section\", \"clean_no_plan\"\n",
    "        ], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def aggregate_transactions(self):\n",
    "        self.df = self.df.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "            \"num_voie\" : x[\"No voie\"].max()\n",
    "            ,\"B_T_Q\" : x[\"B/T/Q\"].max()\n",
    "            ,\"type_de_voie\": x[\"Type de voie\"].max()\n",
    "            ,\"voie\": x[\"Voie\"].max()\n",
    "            ,\"code_postal\": x[\"Code postal\"].max()\n",
    "            ,\"commune\": max(x[\"Commune\"])\n",
    "            ,\"clean_code_departement\": x[\"clean_code_departement\"].max()\n",
    "            ,\"clean_code_commune\": max(x[\"clean_code_commune\"])\n",
    "            ,\"surface_carrez_lot_1\" :  x[\"Surface Carrez du 1er lot\"].sum()/((x[\"Surface reelle bati\"].count()/x[\"Nature culture\"].nunique()))\n",
    "            ,\"Nb_lots\": x[\"Nombre de lots\"].max()\n",
    "            ,\"surface_terrain\" : ((x[\"Surface terrain\"].sum()/x[\"Surface reelle bati\"].count()) if (int(x[\"Surface terrain\"].nunique()) ==1 and int(x[\"Nature culture\"].nunique()) == 1 )else x[\"Surface terrain\"].sum())\n",
    "            ,\"surface_reelle_bati\" : (x[\"Surface reelle bati\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if (int(x[\"Nature culture\"].nunique() > 1)) else x[\"Surface reelle bati\"].sum())\n",
    "            ,\"nb_pieces_principales\" : (x[\"Nombre pieces principales\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if int(x[\"Nature culture\"].nunique()) > 1 else x[\"Nombre pieces principales\"].sum())      \n",
    "            ,\"dependance\" : x[\"Type local\"].unique()\n",
    "            ,\"main_type_terrain\" : x[\"Nature culture\"].max()\n",
    "            ,\"parcelle_cadastrale\": x[\"parcelle_cadastrale\"].max()}))\n",
    "        self.df = self.df.replace(np.inf, np.nan)\n",
    "        #drop rows with only dependances transactions as we focus on houses\n",
    "        self.df = self.df[self.df.dependance.apply(\n",
    "            lambda x: x.all() != \"Dépendance\")].reset_index(drop=True)\n",
    "        self.df[\"dependance\"] = self.df.dependance.apply(sorted, 1)\n",
    "        self.df[[\"Dependance\",\n",
    "                 \"Maison\"]] = pd.DataFrame(self.df.dependance.tolist(),\n",
    "                                           index=self.df.index)\n",
    "        self.df[\"Dependance\"] = [1 if value ==\"Dépendance\"else 0 for value in self.df[\"Dependance\"]]\n",
    "        self.df= self.df.drop([\"dependance\",\"Maison\"],axis =1)\n",
    "        return self.df\n",
    "\n",
    "    # to do : function calling enrichissement from data\n",
    "\n",
    "\n",
    "    def feature_generation (self):\n",
    "        # convert the 'Date' column to datetime format\n",
    "        self.df[\"month\"] = pd.to_datetime(\n",
    "            self.df[\"Date mutation\"],format=\"%d/%m/%Y\").dt.month\n",
    "        self.df= self.df.drop([\"Date mutation\"], axis = 1)\n",
    "        ## attention à ne faire qu'après avoir enrichi avec variables insee\n",
    "        dict_type_voie = dict()\n",
    "        for value in self.df[\"type_de_voie\"].value_counts()[self.df[\"type_de_voie\"].value_counts()<300 ].index.values :\n",
    "            dict_type_voie[value] = \"Autres\"\n",
    "        self.df=self.df.replace({\"type_voie\" : dict_type_voie})\n",
    "        self.df[\"type_de_voie\"]= self.df[\"type_de_voie\"].replace(np.nan,'vide')\n",
    "        return self.df\n",
    "\n",
    "    def zscore (self) :\n",
    "        # Calculate the z-score from scratch\n",
    "        #self.df['Valeur fonciere']= df['Valeur fonciere'].apply(lambda x: atof(x))\n",
    "        standard_deviation = self.df[\"Valeur fonciere\"].std(ddof=0)\n",
    "        mean_value = self.df[\"Valeur fonciere\"].mean()\n",
    "        zscores = [(value - mean_value) / standard_deviation\n",
    "                for value in self.df[\"Valeur fonciere\"]]\n",
    "        self.df[\"zscores\"]= zscores\n",
    "        # absolute value of zscore and if sup x then 1  :\n",
    "        self.df[\"outlier\"] = [\n",
    "            1 if (abs(value) > 3) else 0 for value in self.df[\"zscores\"]\n",
    "        ]\n",
    "        self.df=self.df[self.df[\"outlier\"] == 0].reset_index(drop=True)\n",
    "        self.df = self.df.drop([\"zscores\",\"outlier\"], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def split_x_y (self):\n",
    "        columns_model = [\"type_de_voie\",\n",
    "            \"clean_code_departement\",\n",
    "            \"clean_code_commune\",\n",
    "            \"code_postal\",\n",
    "            \"surface_terrain\",\n",
    "            \"surface_reelle_bati\", \"nb_pieces_principales\",\n",
    "            \"main_type_terrain\",  \"Dependance\",\n",
    "            \"month\"]\n",
    "        # Séparation des variables catégorielles et numériques\n",
    "        categorical_features = [\n",
    "            \"type_de_voie\", \"clean_code_departement\", \"clean_code_commune\",\n",
    "            \"code_postal\", \"main_type_terrain\", \"Dependance\", \"month\"\n",
    "        ]\n",
    "        numerical_features = [\n",
    "            \"surface_terrain\", \"surface_reelle_bati\", \"nb_pieces_principales\"\n",
    "        ]\n",
    "        for column in categorical_features:\n",
    "            self.df[column] = self.df[column].replace(np.nan, \"\").apply(str)\n",
    "        X = self.df[columns_model]\n",
    "        y =self.df[\"Valeur fonciere\"]\n",
    "        # selection des variables\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            test_size=0.33,\n",
    "                                                            random_state=42)\n",
    "        return self.df,categorical_features, numerical_features, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "#from house_prediction_package.preprocessing import Preprocessing\n",
    "#from house_prediction_package.data import GetData\n",
    "\n",
    "\n",
    "class Pipeline :\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        # self.categorical_features = categorical_features\n",
    "        # self.numerical_features = numerical_features\n",
    "        # self.X_train = X_train\n",
    "        # self.y_train = y_train\n",
    "        # option 2\n",
    "        #appeler les méthodes\n",
    "        self.df, self.categorical_features, self.numerical_features, self.X_train, self.X_test, self.y_train, self.y_test = Preprocessing(\n",
    "            df).feature_generation().zscore().split_x_y()\n",
    "\n",
    "    def pipeline(self):\n",
    "        # création des pipelines de pré-processing pour les variables numériques et catégorielles\n",
    "        #ajout d'un parametre pour gerer les valeures non connues dans onehotencoder - il les passe à 0(autres options disponibles)\n",
    "        numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "        categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, self.numerical_features),\n",
    "            (categorical_pipeline, self.categorical_features))\n",
    "        model = make_pipeline(preprocessor, LinearRegression())\n",
    "        fitted_model = model.fit(self.X_train, self.y_train)\n",
    "        return fitted_model, self.X_train, self.y_train,self.X_test, self.y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zone de tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model sans aggrégation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = GetData().read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test = Preprocessing(df).split_x_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())\n",
    "fitted_model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_model_wo_aggregations.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model maison avec aggrégation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= GetData().read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).clean_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= Preprocessing(df).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).aggregate_transactions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"aggregatedfile_houses.csv\", sep='|', encoding=\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).feature_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).zscore() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test =  Preprocessing(df).split_x_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())\n",
    "fitted_model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[X_test.surface_terrain== np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "X_test = X_test.replace(np.inf, np.nan)\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R(MSE): %.2f\" % math.sqrt(np.mean((test_y_hat - y_test)**2)))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score r²:  0.4806680640761509      \n",
    "Mean absolute error: 83460.59     \n",
    "Residual  of squares (MSE): 46023819610.40     \n",
    "R(MSE): 214531.63    \n",
    "R2-score: -0.02    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_model_aggregations.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test model maison/dep with aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= GetData().read_csv(filter=[1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).clean_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= Preprocessing(df).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).aggregate_transactions()\n",
    "\n",
    "df.to_csv(\"aggregatedfile_houses_dep.csv\", sep='|', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = Preprocessing(df).feature_generation()\n",
    "\n",
    "df = Preprocessing(df).zscore() \n",
    "\n",
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test  = Preprocessing(df).split_x_y()\n",
    "\n",
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R(MSE): %.2f\" % math.sqrt(np.mean((test_y_hat - y_test)**2)))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))\n",
    "\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_dep_model_aggregations.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrichissement lat long iris insee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lit le nouveau csv aggrégé qui nous a fait réduire le nombre de lignes de 1 M à 430 K \n",
    "Ensuite, on utilise l'api ban \n",
    "enrichissement 20 h trop long\n",
    "test avec csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"aggregatedfile_houses_dep.csv\", sep = \"|\", index_col=0,dtype ={\"parcelle_cad_section\":  str  \n",
    ",\"Date mutation\": object \n",
    ",\"Valeur fonciere\"  : np.float32\n",
    ",\"num_voie\" : np.float32\n",
    ",\"B_T_Q\" : object \n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"code_postal\": np.float64\n",
    ",\"commune\": object \n",
    ",\"clean_code_departement\": object \n",
    ",\"clean_code_commune\": object  \n",
    ",\"surface_carrez_lot_1\": np.float32\n",
    ",\"Nb_lots\": np.int32  \n",
    ",\"surface_terrain\":  np.float32\n",
    ",\"surface_reelle_bati\":np.float32\n",
    ",\"nb_pieces_principales\":np.float32\n",
    ",\"main_type_terrain\":object \n",
    ",\"parcelle_cadastrale\": object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DataFrame.apply() and lambda function\\n\",\n",
    "# df_adresses['No voie']= df_adresses['No voie'].astype(int)\\n\",\n",
    "# on ne peut pas passer les num voies/code postaux  en int/string sans d'abord nettoyer les nan values \\n\",\n",
    "df[\"voie\"]=df[\"voie\"].replace(\" \",\"+\")\n",
    "df[\"adresse\"] = df[[\"num_voie\", \"type_de_voie\", \"voie\"]].apply(lambda x: \"+\".join(x.astype(str)), axis=1)\n",
    "#df['clean_code_commune'] = df[[\"clean_code_departement\",\"clean_code_commune\"]].apply(lambda x: \"\".join(x.astype(str)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_code_commun =[]\n",
    "for dep,comm in zip(df[\"clean_code_departement\"],df['clean_code_commune']):\n",
    "    if len(dep) == 3: \n",
    "        full_code_commun.append(dep + comm[1:3])\n",
    "    else : \n",
    "        full_code_commun.append(dep + comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_code_commune'] = full_code_commun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"adresse\"]= df[[\"adresse\",\"clean_code_commune\"]].apply(lambda x : \"&citycode=\".join(x.astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "# reassemblage by pd.concat possible mais on s'en fiche car on va fonctionner sur des'petits df' \n",
    "#pour enrichissement puis insertion en bdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df[0]['adresse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#long=[]\n",
    "#lat= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = 'ok'\n",
    "start_time = datetime.now()\n",
    "\n",
    "for j in range(2,len(list_df)):\n",
    "    if test == 'ok':\n",
    "        for value in list_df[j]['adresse']:\n",
    "            try : \n",
    "                long.append(requests.get(f'http://localhost:7878/search?q={value}').json()['features'][0]['geometry']['coordinates'][0])\n",
    "                lat.append(requests.get(f'http://localhost:7878/search/?q={value}').json()['features'][0]['geometry']['coordinates'][1])\n",
    "            except  : \n",
    "                lat.append('not found')\n",
    "                long.append('not found')            \n",
    "    test= input(f\"iteration {j}, pour passer à l'itération {j+1} taper ok  : \")\n",
    "    f=j\n",
    "end_time = datetime.now()\n",
    "print('Duration: {} et arret à la {}'.format(end_time - start_time, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iteration 2, pour passer à l'itération 3 taper ok  : ok    \n",
    "iteration 3, pour passer à l'itération 4 taper ok  : ok    \n",
    "iteration 4, pour passer à l'itération 5 taper ok  : ok    \n",
    "iteration 5, pour passer à l'itération 6 taper ok  : ok    \n",
    "iteration 6, pour passer à l'itération 7 taper ok  : ok    \n",
    "iteration 7, pour passer à l'itération 8 taper ok  : ok    \n",
    "iteration 8, pour passer à l'itération 9 taper ok  : ok    \n",
    "iteration 9, pour passer à l'itération 10 taper ok  : ok    \n",
    "iteration 10, pour passer à l'itération 11 taper ok  : ok    \n",
    "iteration 12, pour passer à l'itération 13 taper ok  : ok    \n",
    "iteration 13, pour passer à l'itération 14 taper ok  : ko    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'lat.sav'\n",
    "pickle.dump(lat, open(filename, 'wb'))\n",
    "filename2= 'long.sav'\n",
    "pickle.dump(lat, open(filename2, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "requests.get(\"http://localhost:7878/search/?q=27.0+RUE+DES PINS&citycode=97424\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération CSV enrichi en masse et récupération IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/geolocgeocoded.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_commune\": object, \"latitude\": np.float32, \"longitude\": np.float32, \"result_score\" : np.float32 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"num_voie\",\"type_de_voie\",\"voie\",\"commune\",\"clean_code_commune\",\"latitude\", \"longitude\",\"result_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pour lancer pyris : \n",
    "1. d'abord activer lancer postgres sql \n",
    " * sudo service postgresql start\n",
    "2.  puis depuis le dossier  house pred /api / pyris : \n",
    " * gunicorn -b 127.0.0.1:5555 pyris.api.run:app \n",
    "\n",
    "Qu'est ce qu'IRIS : \"Afin de préparer la diffusion du recensement de la population de 1999, l'INSEE avait développé un découpage du territoire en mailles de taille homogène appelées IRIS2000. Un sigle qui signifiait « Ilots Regroupés pour l'Information Statistique » et qui faisait référence à la taille visée de 2 000 habitants par maille élémentaire.\" source : https://www.insee.fr/fr/metadonnees/definition/c1523    \n",
    "API fonctionne avec Postgis ( extension de postgres pour les données géospatiales).    \n",
    "Pour qu'elle fonctionne, j'ai donc télécharger les fichiers suivants : \n",
    " * contours iris \n",
    " * références iris \n",
    " * divers statistiques INSEE à l'échelon IRIS \n",
    "     * statistiques activités -\n",
    "     * statistiques démographiques - ménages et evol pop \n",
    "     * statistiques scolaires\n",
    "     * statistiques logements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['coordinates'] = df[[\"latitude\",\"longitude\"]].apply(lambda x : \"&lon=\".join(x.astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api url \n",
    "url = \"http://127.0.0.1:5555/api/coords?geojson=false&lat=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50000  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "# reassemblage by pd.concat possible mais on s'en fiche car on va fonctionner sur des'petits df' \n",
    "#pour enrichissement puis insertion en bdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'ok'\n",
    "start_time = datetime.now()\n",
    "\n",
    "for j in range(0,len(list_df)):\n",
    "    if test == 'ok':\n",
    "        for value in list_df[j]['coordinates']:\n",
    "            try : \n",
    "                IRIS.append(requests.get(f'{url}{value}').json()['complete_code'])\n",
    "            except  : \n",
    "                IRIS.append('not found')\n",
    "    test= input(f\"iteration {j}, pour passer à l'itération {j+1} taper ok  : \")\n",
    "    f=j\n",
    "end_time = datetime.now()\n",
    "print('Duration: {} et arret à la {}'.format(end_time - start_time, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS= pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IRIS']= IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[df['IRIS'] =='not found']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recherche par ville sur les valeures non trouvées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in df[df['IRIS'] =='not found'].iterrows() : \n",
    "    print('######',index,'*****', value['clean_code_commune'])\n",
    "    try :\n",
    "        df.at[index,'IRIS'] = requests.get(f\"http://127.0.0.1:5555/api/city/code/{value['clean_code_commune']}\").json()[0]\n",
    "    except : \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche api open data soft \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les dernieres valeures non trouvées appartiennent aux dom tom Martinique Guadeloupe Guyanne et Réunion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://data.opendatasoft.com/api/records/1.0/search/?dataset=iris-millesime-france%40lareunion&q=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/api/records/1.0/search/?dataset=iris-millesime-france&q=97101&sort=year&facet=com_arm_name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appel d'un api externe pour la réunion : \n",
    "** ajouter un prefiltre dans le code **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://data.opendatasoft.com/explore/dataset/iris-millesime-france%40lareunion/api/?disjunctive.reg_name&disjunctive.dep_name&disjunctive.arrdep_name&disjunctive.ze2020_name&disjunctive.bv2012_name&disjunctive.epci_name&disjunctive.ept_name&disjunctive.com_name&disjunctive.com_arm_name&disjunctive.iris_name&sort=year&q=97424&geofilter.polygon=&geofilter.distance="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in df[df['IRIS'] =='not found'].iterrows() : \n",
    "    print('######',index,'*****', value['clean_code_commune'])\n",
    "    try :\n",
    "        df.at[index,'IRIS'] = requests.get(f\"{url}{value['clean_code_commune']}&sort=year&facet=com_arm_name\").json()['records'][0]['fields']['iris_code']\n",
    "    except : \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appel d'un api externe pour la guadeloupe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://regionguadeloupe.opendatasoft.com/explore/dataset/iris-millesime-france/information/?disjunctive.reg_name&disjunctive.dep_name&disjunctive.arrdep_name&disjunctive.ze2020_name&disjunctive.bv2012_name&disjunctive.epci_name&disjunctive.ept_name&disjunctive.com_name&disjunctive.com_arm_name&disjunctive.iris_name&sort=year&q=97101&dataChart=eyJxdWVyaWVzIjpbeyJjb25maWciOnsiZGF0YXNldCI6ImlyaXMtbWlsbGVzaW1lLWZyYW5jZSIsIm9wdGlvbnMiOnsiZGlzanVuY3RpdmUucmVnX25hbWUiOnRydWUsImRpc2p1bmN0aXZlLmRlcF9uYW1lIjp0cnVlLCJkaXNqdW5jdGl2ZS5hcnJkZXBfbmFtZSI6dHJ1ZSwiZGlzanVuY3RpdmUuemUyMDIwX25hbWUiOnRydWUsImRpc2p1bmN0aXZlLmJ2MjAxMl9uYW1lIjp0cnVlLCJkaXNqdW5jdGl2ZS5lcGNpX25hbWUiOnRydWUsImRpc2p1bmN0aXZlLmVwdF9uYW1lIjp0cnVlLCJkaXNqdW5jdGl2ZS5jb21fbmFtZSI6dHJ1ZSwiZGlzanVuY3RpdmUuY29tX2FybV9uYW1lIjp0cnVlLCJkaXNqdW5jdGl2ZS5pcmlzX25hbWUiOnRydWUsInNvcnQiOiJ5ZWFyIiwicSI6Im1hcmllIGdhbGFudGUgZ3JhbmQgYm91cmciLCJyZWZpbmUuemUyMDIwX25hbWUiOiJNYXJpZS1HYWxhbnRlIiwicmVmaW5lLmNvbV9uYW1lIjoiR3JhbmQtQm91cmcifX0sImNoYXJ0cyI6W3siYWxpZ25Nb250aCI6dHJ1ZSwidHlwZSI6ImxpbmUiLCJmdW5jIjoiQ09VTlQiLCJzY2llbnRpZmljRGlzcGxheSI6dHJ1ZSwiY29sb3IiOiIjRUQ5QTlBIn1dLCJ4QXhpcyI6InllYXIiLCJtYXhwb2ludHMiOiIiLCJ0aW1lc2NhbGUiOiJ5ZWFyIiwic29ydCI6IiJ9XSwiZGlzcGxheUxlZ2VuZCI6dHJ1ZSwiYWxpZ25Nb250aCI6dHJ1ZX0%3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://regionguadeloupe.opendatasoft.com/api/records/1.0/search/?dataset=iris-millesime-france&q=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, value in df[df['IRIS'] =='not found'].iterrows() : \n",
    "    print('######',index,'*****', value['clean_code_commune'])\n",
    "    if value['clean_code_commune'][0:3] == '971' :\n",
    "        try :\n",
    "            df.at[index,'IRIS'] = requests.get(f\"{url}{value['clean_code_commune']}&sort=year&facet=com_arm_name\").json()['records'][0]['fields']['iris_code']\n",
    "        except : \n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appel d'un autre api pour martinique et guyanne \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://public.opendatasoft.com/explore/dataset/georef-france-iris/api/?disjunctive.reg_name&disjunctive.dep_name&disjunctive.arrdep_name&disjunctive.ze2020_name&disjunctive.bv2012_name&disjunctive.epci_name&disjunctive.ept_name&disjunctive.com_name&disjunctive.com_arm_name&disjunctive.iris_name&sort=year&q=97201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://public.opendatasoft.com/api/records/1.0/search/?dataset=georef-france-iris&q=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, value in df[df['IRIS'] =='not found'].iterrows() : \n",
    "    print('######',index,'*****', value['clean_code_commune'])\n",
    "    try :\n",
    "        df.at[index,'IRIS'] = requests.get(f\"{url}{value['clean_code_commune']}&sort=year&facet=com_arm_name\").json()['records'][0]['fields']['iris_code']\n",
    "    except : \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reconstitution df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init = pd.read_csv('../data/geolocgeocoded.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_commune\": object, \"latitude\": np.float32, \"longitude\": np.float32, \"result_score\" : np.float32 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init= df_init[[\"num_voie\",\"type_de_voie\",\"voie\",\"commune\",\"clean_code_commune\",\"latitude\", \"longitude\",\"result_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour code final garder plutot l'id de résultat qui correspond à lat et long trouvé et dédoublonner selon cet id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init['coordinates'] = df_init[[\"latitude\",\"longitude\"]].apply(lambda x : \"&lon=\".join(x.astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init = df_init.merge(df[['coordinates','IRIS']], left_on='coordinates', right_on='coordinates',\n",
    "          suffixes=('_left', '_right'),  how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init.to_csv(\"aggregatedfile_houses_dep_WITH_IRIS.csv\", sep='|', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrichissement variables INSEE sur CODES IRIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('aggregatedfile_houses_dep_WITH_IRIS.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_commune\": object, \"latitude\": np.float32, \"longitude\": np.float32, \"result_score\" : np.float32, \n",
    "'IRIS': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"http://127.0.0.1:5555/api/insee/activite/distribution/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inititing lists : \n",
    "actif_15_24 = []\n",
    "actif_25_54 = []\n",
    "actif_55_64 = []\n",
    "chomage_15_24 = []\n",
    "chomage_25_54 = []\n",
    "chomage_55_64 = []\n",
    "taux_chomage_15_24 = []\n",
    "taux_chomage_25_54 = []\n",
    "taux_chomage_55_64 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in df.IRIS.unique() : \n",
    "    try:\n",
    "        result = requests.get(f'http://127.0.0.1:5555/api/insee/activite/distribution/{value}?by=age').json()['data']\n",
    "        for variable in result.keys():\n",
    "            try: \n",
    "                if variable == 'taux_chomage_15_24':\n",
    "                    taux_chomage_15_24.append(result[variable])\n",
    "                elif variable == 'taux_chomage_25_54':\n",
    "                    taux_chomage_25_54.append(result[variable])\n",
    "                elif variable == 'taux_chomage_55_64':\n",
    "                    taux_chomage_55_64.append(result[variable])\n",
    "            except : \n",
    "                if variable == 'taux_chomage_15_24':\n",
    "                    taux_chomage_15_24.append('not_found')\n",
    "                elif variable == 'taux_chomage_25_54':\n",
    "                    taux_chomage_25_54.append('not found')\n",
    "                elif variable == 'taux_chomage_55_64':\n",
    "                    taux_chomage_55_64.append('not_found')\n",
    "    except: \n",
    "        taux_chomage_15_24.append('not_found')\n",
    "        taux_chomage_25_54.append('not found')\n",
    "        taux_chomage_55_64.append('not_found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requests.get(f'http://127.0.0.1:5555/api/insee/activite/distribution/010010000?by=age').json()['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stat=pd.DataFrame({'IRIS': df.IRIS.unique(),'taux_chomage_15_24':taux_chomage_15_24,\"taux_chomage_25_54\":taux_chomage_25_54,\"taux_chomage_55_64\":taux_chomage_55_64 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_residence_30m2= []\n",
    "main_residence_30_40m2= []\n",
    "main_residence_40_60m2= []\n",
    "main_residence_60_80m2= []\n",
    "main_residence_80_100m2= []\n",
    "main_residence_100_120m2= []\n",
    "main_residence_120m2= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in df_stat['IRIS'] : \n",
    "    try:\n",
    "        result= requests.get(f'http://127.0.0.1:5555/api/insee/logement/distribution/{value}?by=area').json()['data']\n",
    "        for variable in result.keys():\n",
    "            try: \n",
    "                if variable == 'main_residence_30m2':\n",
    "                    main_residence_30m2.append(result[variable])\n",
    "                elif variable == 'main_residence_30_40m2':\n",
    "                    main_residence_30_40m2.append(result[variable])\n",
    "                elif variable == 'main_residence_40_60m2':\n",
    "                    main_residence_40_60m2.append(result[variable])\n",
    "                elif variable == 'main_residence_60_80m2':\n",
    "                    main_residence_60_80m2.append(result[variable])\n",
    "                elif variable == 'main_residence_80_100m2':\n",
    "                    main_residence_80_100m2.append(result[variable])\n",
    "                elif variable == 'main_residence_100_120m2':\n",
    "                    main_residence_100_120m2.append(result[variable])\n",
    "                elif variable == 'main_residence_120m2':\n",
    "                    main_residence_120m2.append(result[variable])\n",
    "            except : \n",
    "                if variable == 'main_residence_30m2':\n",
    "                    main_residence_30m2.append('not_found')\n",
    "                elif variable == 'main_residence_30_40m2':\n",
    "                    main_residence_30_40m2.append('not found')\n",
    "                elif variable == 'main_residence_40_60m2':\n",
    "                    main_residence_40_60m2.append('not_found')\n",
    "                elif variable == 'main_residence_60_80m2':\n",
    "                    main_residence_60_80m2.append('not found')\n",
    "                elif variable == 'main_residence_80_100m2':\n",
    "                    main_residence_80_100m2.append('not_found')\n",
    "                elif variable == 'main_residence_30_40m2':\n",
    "                    main_residence_100_120m2.append('not found')\n",
    "                elif variable == 'main_residence_120m2':\n",
    "                    main_residence_120m2.append('not_found')\n",
    "    except: \n",
    "        main_residence_30m2.append('not_found')\n",
    "        main_residence_30_40m2.append('not found')\n",
    "        main_residence_40_60m2.append('not_found')\n",
    "        main_residence_60_80m2.append('not_found')\n",
    "        main_residence_80_100m2.append('not found')\n",
    "        main_residence_100_120m2.append('not_found')\n",
    "        main_residence_120m2.append('not_found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat['main_residence_30m2'] = main_residence_30m2\n",
    "df_stat['main_residence_30_40m2'] = main_residence_30_40m2\n",
    "df_stat['main_residence_40_60m2'] = main_residence_40_60m2\n",
    "df_stat['main_residence_60_80m2'] = main_residence_60_80m2\n",
    "df_stat['main_residence_80_100m2'] = main_residence_80_100m2\n",
    "df_stat['main_residence_100_120m2'] = main_residence_100_120m2\n",
    "df_stat['main_residence_120m2'] = main_residence_120m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat.to_csv(\"IRIS_STATS_INSEE.csv\", sep='|', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat= pd.read_csv('IRIS_STATS_INSEE.csv', sep='|', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrichissement logement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logement= []\n",
    "main_residence= []\n",
    "second_residence= []\n",
    "unoccupied= []\n",
    "house= []\n",
    "appartment= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(f'http://127.0.0.1:5555/api/insee/logement/010040201').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in df_stat['IRIS']: \n",
    "    try:\n",
    "        result= requests.get(f'http://127.0.0.1:5555/api/insee/logement/{value}').json()\n",
    "        for variable in result.keys():\n",
    "            try: \n",
    "                if variable == 'logement':\n",
    "                    logement.append(result[variable])\n",
    "                elif variable == 'main_residence':\n",
    "                    main_residence.append(result[variable])\n",
    "                elif variable == 'second_residence':\n",
    "                    second_residence.append(result[variable])\n",
    "                elif variable == 'unoccupied':\n",
    "                    unoccupied.append(result[variable])\n",
    "                elif variable == 'house':\n",
    "                    house.append(result[variable])\n",
    "                elif variable == 'appartment':\n",
    "                    appartment.append(result[variable])\n",
    "            except : \n",
    "                if variable == 'logement':\n",
    "                    logement.append('not_found')\n",
    "                elif variable == 'main_residence':\n",
    "                    main_residence.append('not found')\n",
    "                elif variable == 'second_residence':\n",
    "                    second_residence.append('not_found')\n",
    "                elif variable == 'unoccupied':\n",
    "                    unoccupied.append('not found')\n",
    "                elif variable == 'house':\n",
    "                    house.append('not_found')\n",
    "                elif variable == 'appartment':\n",
    "                    appartment.append('not found')\n",
    "    except: \n",
    "        logement.append('not_found')\n",
    "        main_residence.append('not found')\n",
    "        second_residence.append('not_found')\n",
    "        unoccupied.append('not_found')\n",
    "        house.append('not found')\n",
    "        appartment.append('not_found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IRIS'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[~df['IRIS'].isin(df_stat['IRIS'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liste des bases de données niveau IRIS \n",
    "https://www.insee.fr/fr/information/2383389 :\n",
    "\n",
    "\n",
    "* spécial Filosofi : https://www.insee.fr/fr/statistiques/6049648#dictionnaire\n",
    "\n",
    "* https://www.insee.fr/fr/statistiques/5650714#consulter\n",
    "\n",
    "* https://www.insee.fr/fr/statistiques/5650712#dictionnaire\n",
    "\n",
    "* https://www.insee.fr/fr/statistiques/5650708#dictionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyse data logement - variables à garder :\n",
    "https://www.insee.fr/fr/statistiques/5650749#dictionnaire\n",
    " 92 variables - variables suivantes gardées : \n",
    " - variables inter logements - nbre de logements,logements vacants nbre type apparte, nbre hlm, nbre maisons, nbre residences princ (je n'ai pas pris le nombre de résidence principale de type maison ou appartements redondant avec données déjà prise : nbre maison, nbre appartements)\n",
    " - variables intra logements - nbre pieces, superficies, garage....\n",
    " - variables ménages - residence par propio /locataires , emménagement 2 ans , 4 ans e\n",
    " \n",
    "choix de ne pas prendre les variables sur période de fabrication des logements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat= pd.read_csv('../data/base-ic-logement-2018.CSV',sep=';', dtype= {\n",
    "\"IRIS\" : object\n",
    ",\"COM\": object \n",
    ",\"TYP_IRIS\":  object \n",
    ",\"MODIF_IRIS\": object \n",
    ",\"LAB_IRIS\": object\n",
    ",'P18_RP_ELEC': np.float32, 'P18_RP_EAUCH':np.float32, 'P18_RP_BDWC':np.float32, 'P18_RP_CHOS':np.float32,\n",
    "       'P18_RP_CLIM':np.float32, 'P18_RP_TTEGOU':np.float32, 'P18_RP_GARL':np.float32, 'P18_RP_VOIT1P':np.float32,\n",
    "       'P18_RP_VOIT1':np.float32, 'P18_RP_VOIT2P':np.float32, 'P18_RP_HABFOR':np.float32, 'P18_RP_CASE':np.float32,\n",
    "       'P18_RP_MIBOIS':np.float32,'P18_RP_MIDUR':np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stat[['IRIS']].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat=df_stat[['IRIS', 'LAB_IRIS','P18_LOG', 'P18_RP', 'P18_RSECOCC', 'P18_LOGVAC', 'P18_MAISON', 'P18_APPART','P18_RP_1P', \n",
    "         'P18_RP_2P', 'P18_RP_3P', 'P18_RP_4P', 'P18_RP_5PP','P18_RP_M30M2', 'P18_RP_3040M2', 'P18_RP_4060M2',\n",
    "       'P18_RP_6080M2', 'P18_RP_80100M2', 'P18_RP_100120M2', 'P18_RP_120M2P','P18_RP_GARL','P18_RP_PROP', \n",
    "         'P18_RP_LOC', 'P18_RP_LOCHLMV','P18_RP_GRAT','P18_MEN_ANEM0002', 'P18_MEN_ANEM0204',\n",
    "       'P18_MEN_ANEM0509', 'P18_MEN_ANEM10P']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat[['P18_LOG','P18_RP','P18_RP_GARL','P18_RP_PROP', 'P18_RP_GRAT',\n",
    "         'P18_RP_LOC', 'P18_RP_LOCHLMV','P18_MEN_ANEM0002', 'P18_MEN_ANEM0204',\n",
    "       'P18_MEN_ANEM0509', 'P18_MEN_ANEM10P']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_featuring (df) : \n",
    "    df[\"Taux_RP\"] = df[\"P18_RP\"]/df[\"P18_LOG\"]\n",
    "    df[\"Taux_LV\"] = df[\"P18_LOGVAC\"]/df[\"P18_LOG\"]\n",
    "    df[\"Taux_MAI\"] = df[\"P18_MAISON\"]/df[\"P18_LOG\"]\n",
    "    # APPARTEMENT PAS UTILE CAR SOIT MAISON SOIT APPARTEMENT\n",
    "    #nb pieces\n",
    "    df[\"Taux_RP_1P\"] = df[\"P18_RP_1P\"]/df[\"P18_RP\"]\n",
    "    df[\"Taux_RP_2P\"] = df[\"P18_RP_2P\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_3P\"] = df[\"P18_RP_3P\"]/df[\"P18_RP\"]    \n",
    "    df[\"Taux_RP_4P\"] = df[\"P18_RP_4P\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_5P\"] = df[\"P18_RP_5PP\"]/df[\"P18_RP\"] \n",
    "    #superficie\n",
    "    df[\"Taux_RP_30\"] = df[\"P18_RP_M30M2\"]/df[\"P18_RP\"]\n",
    "    df[\"Taux_RP_40\"] = df[\"P18_RP_3040M2\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_60\"] = df[\"P18_RP_4060M2\"]/df[\"P18_RP\"]    \n",
    "    df[\"Taux_RP_80\"] = df[\"P18_RP_6080M2\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_100\"] = df[\"P18_RP_80100M2\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_120\"] = df[\"P18_RP_100120M2\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_P120\"] = df[\"P18_RP_120M2P\"]/df[\"P18_RP\"] \n",
    "    #occupation\n",
    "    df[\"Taux_RP_GAR\"] = df[\"P18_RP_GARL\"]/df[\"P18_RP\"]\n",
    "    df[\"Taux_RP_PROPRIO\"] = df[\"P18_RP_PROP\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_GRATUIT\"] = df[\"P18_RP_GRAT\"]/df[\"P18_RP\"]    \n",
    "    df[\"Taux_RP_LOC\"] = df[\"P18_RP_LOC\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_HML\"] = df[\"P18_RP_LOCHLMV\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_AM02\"] = df[\"P18_MEN_ANEM0002\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_AM04\"] = df[\"P18_MEN_ANEM0204\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_AM09\"] = df[\"P18_MEN_ANEM0509\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_AM09P\"] = df[\"P18_MEN_ANEM10P\"]/df[\"P18_RP\"]  \n",
    "    df =df.drop(['P18_LOG', 'P18_RP', 'P18_RSECOCC', 'P18_LOGVAC', 'P18_MAISON', 'P18_APPART','P18_RP_1P', \n",
    "         'P18_RP_2P', 'P18_RP_3P', 'P18_RP_4P', 'P18_RP_5PP','P18_RP_M30M2', 'P18_RP_3040M2', 'P18_RP_4060M2',\n",
    "       'P18_RP_6080M2', 'P18_RP_80100M2', 'P18_RP_100120M2', 'P18_RP_120M2P','P18_RP_GARL','P18_RP_PROP', \n",
    "         'P18_RP_LOC', 'P18_RP_LOCHLMV','P18_RP_GRAT','P18_MEN_ANEM0002', 'P18_MEN_ANEM0204',\n",
    "       'P18_MEN_ANEM0509', 'P18_MEN_ANEM10P'], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dans mon code générique pour l'API ne pas supprimer les lignes mais tout mettre en bdd \n",
    "\n",
    "la suppression des lignes est pour l'apprentissage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = columns_featuring(df_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = df_stat[df_stat['IRIS'].isin(df['IRIS'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df avec stat sur logement mais pas iris\n",
    "df_ini = pd.read_csv('aggregatedfile_houses_dep.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_departement\" : object\n",
    ",\"clean_code_commune\": object, \"latitude\": np.float32, \"longitude\": np.float32, \"result_score\" : np.float32 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df avec iris mais pas de stat sur les logements\n",
    "df= pd.read_csv('aggregatedfile_houses_dep_WITH_IRIS.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_commune\": object, \"latitude\": np.float32, \"longitude\": np.float32, \"result_score\" : np.float32, \n",
    "'IRIS': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ini.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['IRIS']].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test avant concat pour voir si ordre lignes préservé\n",
    "df['Diff'] = np.where( df['voie_a'] == df['voie'] , '1', '0')\n",
    "#df['Diff'].value_counts()\n",
    "df[df['Diff'] == '0'][['voie','voie_a']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df[['IRIS']],df_ini],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enrichissement df avec infos logements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_stat, left_on='IRIS', right_on='IRIS',\n",
    "          suffixes=('_left', '_right'),  how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test model avec enrichissement logements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from house_prediction_package.data import GetData\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from more_itertools import chunked\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#sans doute à supprimer au lancement final du modele\n",
    "#from locale import atof, setlocale, LC_NUMERIC, LC_ALL\n",
    "\n",
    "#setlocale(LC_ALL, 'fr_FR.UTF-8')\n",
    "\n",
    "class Preprocessing :\n",
    "\n",
    "    def __init__(self,df) :\n",
    "        # self.df = get_data().read_csv()\n",
    "        self.df = df\n",
    "\n",
    "    def clean_columns(self,\n",
    "                      columns=[\n",
    "                          'Code service CH', 'Reference document',\n",
    "                          '1 Articles CGI', '2 Articles CGI', '3 Articles CGI',\n",
    "                          '4 Articles CGI', '5 Articles CGI', 'No Volume',\n",
    "                          'Identifiant local'\n",
    "                      ]):\n",
    "        \"\"\" drop useless columns\n",
    "        Customisation of columns to drop must be entered as a list\n",
    "        \"\"\"\n",
    "        # suppression of 100% empty columns - these columns are officially not completed in this db\n",
    "        self.df = self.df.drop(columns,axis=1)\n",
    "        # suppression of columns poorly completed\n",
    "        columns_to_drop = [column for column in self.df.columns if ((self.df[column].isnull().value_counts().sort_index()[0]/self.df.shape[0])*100) < 2 ]\n",
    "        self.df= self.df.drop(columns_to_drop,axis=1)\n",
    "        # replacement of , by . in numerical variables & deletion of non numrical caracters in num columns : \n",
    "        columns_num = ['Valeur fonciere', 'Surface Carrez du 1er lot', 'Nombre de lots',\n",
    "        'Surface reelle bati', 'Nombre pieces principales', 'Surface terrain']\n",
    "        # transformation des , en . pour réaliser des opérations sur les nombres et suppressions des caracteres non numériques au sein de ces colonnes \n",
    "        for column in columns_num : \n",
    "            self.df[column]=self.df[column].apply(lambda s: s.replace(\",\",\".\") if isinstance(s,str) else s)\n",
    "            self.df[column] = pd.to_numeric(self.df[column], errors = 'coerce')\n",
    "        # suppression of nan value on target variable\n",
    "        self.df= self.df.dropna(subset=['Valeur fonciere'])\n",
    "        #self.df['Surface Carrez du 1er lot'] = self.df['Surface Carrez du 1er lot'].apply(\n",
    "        #    lambda x: atof(x))        \n",
    "        # pre processing avant groupby mais attention sortir valeures foncieres avant de mettre en POO\n",
    "        ob_columns= self.df.dtypes[self.df.dtypes == 'O'].index\n",
    "        num_columns = self.df.dtypes[(self.df.dtypes == 'int')\n",
    "                                     | (self.df.dtypes == 'float')].index\n",
    "        non_num_col = ['No disposition', 'No voie', 'Code postal', 'Code commune',\n",
    "       'Prefixe de section', 'No plan','Code type local']\n",
    "        num_columns = [value for value in num_columns if value not in non_num_col]\n",
    "        for column in ob_columns :\n",
    "            self.df[column]=self.df[column].replace(np.nan,'',regex=True)\n",
    "        #à adapter in v2\n",
    "        \n",
    "        self.df[num_columns] = self.df[num_columns].apply(pd.to_numeric,\n",
    "                                                              errors='coerce')\n",
    "        \n",
    "        #drop duplicates\n",
    "        self.df = self.df.drop_duplicates().reset_index(drop= True)\n",
    "        # by returning self, we can do method chaining like preprocessing(df).clean_columns().create_identifier()\n",
    "        return self.df\n",
    "\n",
    "    def create_identifier(self) :\n",
    "        \"\"\" Create a 'unique' identifier allowing us to group several lines corresponding to a unique transaction\n",
    "        \"\"\"\n",
    "        variables_to_clean = [\n",
    "            \"Code departement\", \"Code commune\", \"Prefixe de section\",\n",
    "            \"Section\", \"No plan\"\n",
    "            ]\n",
    "        size_variables= [2,3,3,2,4]\n",
    "        for i,j in zip(variables_to_clean,size_variables):\n",
    "            chunked_data = chunked(self.df[i], 10000, strict=False)\n",
    "            values = {\"Prefixe de section\": '000'}\n",
    "            self.df= self.df.fillna(value=values)\n",
    "            if i == \"Prefixe de section\" :\n",
    "                self.df[i] = self.df[i].apply(str).apply(lambda x: x[:3])\n",
    "            new_variable = [\n",
    "                str(value).replace(\".\",\"\").zfill(j) for sublist in list(chunked_data)\n",
    "                for value in sublist\n",
    "            ]\n",
    "            self.df[f\"clean_{i.replace(' ','_').lower()}\"] = new_variable\n",
    "            self.df= self.df.drop([i],axis=1)\n",
    "        self.df[\"parcelle_cadastrale\"] = self.df[[\n",
    "            \"clean_code_departement\", \"clean_code_commune\", \"clean_prefixe_de_section\",\n",
    "            \"clean_section\", \"clean_no_plan\"]].apply(lambda x: \"\".join(x), axis=1)\n",
    "        self.df[\"parcelle_cad_section\"]=self.df[\"parcelle_cadastrale\"].str[:10]\n",
    "        self.df = self.df.drop([\n",
    "            \"clean_prefixe_de_section\", \"clean_section\", \"clean_no_plan\"\n",
    "        ], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def aggregate_transactions(self):\n",
    "        self.df = self.df.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "            \"num_voie\" : x[\"No voie\"].max()\n",
    "            ,\"B_T_Q\" : x[\"B/T/Q\"].max()\n",
    "            ,\"type_de_voie\": x[\"Type de voie\"].max()\n",
    "            ,\"voie\": x[\"Voie\"].max()\n",
    "            ,\"code_postal\": x[\"Code postal\"].max()\n",
    "            ,\"commune\": max(x[\"Commune\"])\n",
    "            ,\"clean_code_departement\": x[\"clean_code_departement\"].max()\n",
    "            ,\"clean_code_commune\": max(x[\"clean_code_commune\"])\n",
    "            ,\"surface_carrez_lot_1\" :  x[\"Surface Carrez du 1er lot\"].sum()/((x[\"Surface reelle bati\"].count()/x[\"Nature culture\"].nunique()))\n",
    "            ,\"Nb_lots\": x[\"Nombre de lots\"].max()\n",
    "            ,\"surface_terrain\" : ((x[\"Surface terrain\"].sum()/x[\"Surface reelle bati\"].count()) if (int(x[\"Surface terrain\"].nunique()) ==1 and int(x[\"Nature culture\"].nunique()) == 1 )else x[\"Surface terrain\"].sum())\n",
    "            ,\"surface_reelle_bati\" : (x[\"Surface reelle bati\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if (int(x[\"Nature culture\"].nunique() > 1)) else x[\"Surface reelle bati\"].sum())\n",
    "            ,\"nb_pieces_principales\" : (x[\"Nombre pieces principales\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if int(x[\"Nature culture\"].nunique()) > 1 else x[\"Nombre pieces principales\"].sum())      \n",
    "            ,\"dependance\" : x[\"Type local\"].unique()\n",
    "            ,\"main_type_terrain\" : x[\"Nature culture\"].max()\n",
    "            ,\"parcelle_cadastrale\": x[\"parcelle_cadastrale\"].max()}))\n",
    "        self.df = self.df.replace(np.inf, np.nan)\n",
    "        #drop rows with only dependances transactions as we focus on houses\n",
    "        self.df = self.df[self.df.dependance.apply(\n",
    "            lambda x: x.all() != \"Dépendance\")].reset_index(drop=True)\n",
    "        self.df[\"dependance\"] = self.df.dependance.apply(sorted, 1)\n",
    "        self.df[[\"Dependance\",\n",
    "                 \"Maison\"]] = pd.DataFrame(self.df.dependance.tolist(),\n",
    "                                           index=self.df.index)\n",
    "        self.df[\"Dependance\"] = [1 if value ==\"Dépendance\"else 0 for value in self.df[\"Dependance\"]]\n",
    "        self.df= self.df.drop([\"dependance\",\"Maison\"],axis =1)\n",
    "        return self.df\n",
    "\n",
    "    # to do : function calling enrichissement from data\n",
    "\n",
    "\n",
    "    def feature_generation (self):\n",
    "        # convert the 'Date' column to datetime format\n",
    "        self.df[\"month\"] = pd.to_datetime(\n",
    "            self.df[\"Date mutation\"],format=\"%d/%m/%Y\").dt.month\n",
    "        self.df= self.df.drop([\"Date mutation\"], axis = 1)\n",
    "        ## attention à ne faire qu'après avoir enrichi avec variables insee\n",
    "        dict_type_voie = dict()\n",
    "        for value in self.df[\"type_de_voie\"].value_counts()[self.df[\"type_de_voie\"].value_counts()<300 ].index.values :\n",
    "            dict_type_voie[value] = \"Autres\"\n",
    "        self.df=self.df.replace({\"type_voie\" : dict_type_voie})\n",
    "        self.df[\"type_de_voie\"]= self.df[\"type_de_voie\"].replace(np.nan,'vide')\n",
    "        return self.df\n",
    "\n",
    "    def zscore (self) :\n",
    "        # Calculate the z-score from scratch\n",
    "        #self.df['Valeur fonciere']= df['Valeur fonciere'].apply(lambda x: atof(x))\n",
    "        standard_deviation = self.df[\"Valeur fonciere\"].std(ddof=0)\n",
    "        mean_value = self.df[\"Valeur fonciere\"].mean()\n",
    "        zscores = [(value - mean_value) / standard_deviation\n",
    "                for value in self.df[\"Valeur fonciere\"]]\n",
    "        self.df[\"zscores\"]= zscores\n",
    "        # absolute value of zscore and if sup x then 1  :\n",
    "        self.df[\"outlier\"] = [\n",
    "            1 if (abs(value) > 0.25) else 0 for value in self.df[\"zscores\"]\n",
    "        ]\n",
    "        self.df=self.df[self.df[\"outlier\"] == 0].reset_index(drop=True)\n",
    "        self.df = self.df.drop([\"zscores\",\"outlier\"], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def split_x_y (self):\n",
    "        columns_model = [\"type_de_voie\",\n",
    "            \"clean_code_departement\",\n",
    "            \"clean_code_commune\",\n",
    "            \"code_postal\",\n",
    "            \"surface_terrain\",\n",
    "            \"surface_reelle_bati\", \"nb_pieces_principales\",\n",
    "            \"main_type_terrain\",  \"Dependance\",'Taux_RP', 'Taux_LV', 'Taux_MAI',\n",
    "       'Taux_RP_1P', 'Taux_RP_2P', 'Taux_RP_3P', 'Taux_RP_4P', 'Taux_RP_5P',\n",
    "       'Taux_RP_30', 'Taux_RP_40', 'Taux_RP_60', 'Taux_RP_80', 'Taux_RP_100',\n",
    "       'Taux_RP_120', 'Taux_RP_P120', 'Taux_RP_GAR', 'Taux_RP_PROPRIO',\n",
    "       'Taux_RP_GRATUIT', 'Taux_RP_LOC', 'Taux_RP_HML', 'Taux_RP_AM02',\n",
    "       'Taux_RP_AM04', 'Taux_RP_AM09', 'Taux_RP_AM09P',\n",
    "            \"month\"]\n",
    "        # Séparation des variables catégorielles et numériques\n",
    "        categorical_features = [\n",
    "            \"type_de_voie\", \"clean_code_departement\", \"clean_code_commune\",\n",
    "            \"code_postal\", \"main_type_terrain\", \"Dependance\", \"month\"\n",
    "        ]\n",
    "        numerical_features = [\n",
    "            \"surface_terrain\", \"surface_reelle_bati\", \"nb_pieces_principales\",'Taux_RP', 'Taux_LV', 'Taux_MAI',\n",
    "       'Taux_RP_1P', 'Taux_RP_2P', 'Taux_RP_3P', 'Taux_RP_4P', 'Taux_RP_5P',\n",
    "       'Taux_RP_30', 'Taux_RP_40', 'Taux_RP_60', 'Taux_RP_80', 'Taux_RP_100',\n",
    "       'Taux_RP_120', 'Taux_RP_P120', 'Taux_RP_GAR', 'Taux_RP_PROPRIO',\n",
    "       'Taux_RP_GRATUIT', 'Taux_RP_LOC', 'Taux_RP_HML', 'Taux_RP_AM02',\n",
    "       'Taux_RP_AM04', 'Taux_RP_AM09', 'Taux_RP_AM09P'\n",
    "        ]\n",
    "        for column in categorical_features:\n",
    "            self.df[column] = self.df[column].replace(np.nan, \"\").apply(str)\n",
    "        X = self.df[columns_model]\n",
    "        y =self.df[\"Valeur fonciere\"]\n",
    "        # selection des variables\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            test_size=0.33,\n",
    "                                                            random_state=42)\n",
    "        return self.df,categorical_features, numerical_features, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).feature_generation()\n",
    "\n",
    "df = Preprocessing(df).zscore() \n",
    "#df = Preprocessing(df).zscore() \n",
    "\n",
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test  = Preprocessing(df).split_x_y()\n",
    "\n",
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R(MSE): %.2f\" % math.sqrt(np.mean((test_y_hat - y_test)**2)))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))\n",
    "\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_dep_model_aggregations_logement.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "double application du zscore pr les deux idées :      \n",
    "score avec les colonnes génériques sans calcul des taux : \n",
    "\n",
    "* Score r²:  0.6286664529682898     \n",
    "* Mean absolute error: 56530.92   \n",
    "* Residual  of squares (MSE): 6222012343.59    \n",
    "* R(MSE): 78879.73    \n",
    "* R2-score: 0.43    \n",
    "\n",
    "score avec  colonnes featuring et calcul des taux : \n",
    "\n",
    "* Score r²:  0.6653430369461752\n",
    "* Mean absolute error: 64268.84\n",
    "* Residual  of squares (MSE): 9148083298.02\n",
    "* R(MSE): 95645.61\n",
    "* R2-score: 0.51\n",
    "\n",
    "application d'un zscore de 1 et calcul des taux : \n",
    "* Score r²:  0.6039277500731881\n",
    "* Mean absolute error: 75588.18\n",
    "* Residual  of squares (MSE): 21660448082.09\n",
    "* R(MSE): 147174.89\n",
    "* R2-score: 0.35\n",
    "\n",
    "application d'un zscore de 0.5 et calcul des taux : \n",
    "* Score r²:  0.6423394140464491\n",
    "* Mean absolute error: 71624.55\n",
    "* Residual  of squares (MSE): 14824202415.15\n",
    "* R(MSE): 121754.68\n",
    "* R2-score: 0.48\n",
    "\n",
    "\n",
    "application d'un zscore de 0.25 et calcul des taux (VF max : 1.3 M€): \n",
    "* Score r²:  0.6597730148379957\n",
    "* Mean absolute error: 66801.81\n",
    "* Residual  of squares (MSE): 10643126458.30\n",
    "* R(MSE): 103165.53\n",
    "* R2-score: 0.51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrichissemnt activites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat= pd.read_csv('../data/base-ic-activite-residents-2018.CSV',sep=';', dtype= {\n",
    "\"IRIS\" : object\n",
    ",\"COM\": object \n",
    ",\"TYP_IRIS\":  object \n",
    ",\"MODIF_IRIS\": object \n",
    ",\"LAB_IRIS\": object\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat=df_stat[[\"IRIS\",\"P18_POP1564\", \"P18_POP1524\",\"P18_POP2554\",\"P18_POP5564\", \"P18_ACT1564\",\"P18_ACTOCC1564\",\"P18_CHOM1564\",\n",
    "         \"C18_ACT1564\", \"C18_ACT1564_CS1\",\"C18_ACT1564_CS3\",\"C18_ACT1564_CS2\",\"C18_ACT1564_CS4\",\"C18_ACTOCC1564\",\n",
    "        \"C18_ACTOCC1564_CS1\" ,\"C18_ACTOCC1564_CS2\",\"C18_ACTOCC1564_CS3\", \"C18_ACTOCC1564_CS4\",\"P18_ACTOCC15P_ILT1\",\"C18_ACTOCC15P\",\n",
    "         \"C18_ACTOCC15P_PAS\" ,\"C18_ACTOCC15P_MAR\" ,\"C18_ACTOCC15P_VELO\" ,\"C18_ACTOCC15P_2ROUESMOT\" ,\"C18_ACTOCC15P_VOIT\" ,\n",
    "         \"C18_ACTOCC15P_TCOM\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_featuring_act (df) : \n",
    "    # repartition population \n",
    "    df[\"Taux_1524\"] = df[\"P18_POP1524\"]/df[\"P18_POP1564\"]\n",
    "    df[\"Taux_2554\"] = df[\"P18_POP2554\"]/df[\"P18_POP1564\"]\n",
    "    df[\"Taux_5564\"] = df[\"P18_POP5564\"]/df[\"P18_POP1564\"]\n",
    "    # statistiques économiques\n",
    "    df[\"Taux_P_Act\"] = df[\"P18_ACT1564\"]/df[\"P18_POP1564\"]\n",
    "    df[\"Taux_P_ActOct\"] = df[\"P18_ACTOCC1564\"]/df[\"P18_ACT1564\"] \n",
    "    df[\"Taux_P_CHO\"] = df[\"P18_CHOM1564\"]/df[\"P18_ACT1564\"]\n",
    "    \n",
    "    df[\"Taux_CS1\"] = df[\"C18_ACT1564_CS1\"]/df[\"C18_ACT1564\"] \n",
    "    df[\"Taux_CS2\"] = df[\"C18_ACT1564_CS2\"]/df[\"C18_ACT1564\"] \n",
    "    df[\"Taux_CS3\"] = df[\"C18_ACT1564_CS3\"]/df[\"C18_ACT1564\"] \n",
    "    df[\"Taux_CS4\"] = df[\"C18_ACT1564_CS4\"]/df[\"C18_ACT1564\"]  \n",
    "    \n",
    "    # Statistiques sur transport travail\n",
    "    df[\"Taux_Travail_Commune\"] = df[\"P18_ACTOCC15P_ILT1\"]/df[\"P18_ACTOCC1564\"]  \n",
    "    df[\"Taux_TT\"] = df[\"C18_ACTOCC15P_PAS\"]/df[\"C18_ACTOCC15P\"]\n",
    "    df[\"Taux_Mar\"] = df[\"C18_ACTOCC15P_MAR\"]/df[\"C18_ACTOCC15P\"] \n",
    "    df[\"Taux_Velo\"] = df[\"C18_ACTOCC15P_VELO\"]/df[\"C18_ACTOCC15P\"]\n",
    "    df[\"Taux_2Roues\"] = df[\"C18_ACTOCC15P_2ROUESMOT\"]/df[\"C18_ACTOCC15P\"] \n",
    "    df[\"Taux_Voit\"] = df[\"C18_ACTOCC15P_VOIT\"]/df[\"C18_ACTOCC15P\"] \n",
    "    df[\"Taux_TCOM\"] = df[\"C18_ACTOCC15P_TCOM\"]/df[\"C18_ACTOCC15P\"] \n",
    "   \n",
    "    \n",
    "    df =df.drop([\"P18_POP1564\", \"P18_POP1524\",\"P18_POP2554\",\"P18_POP5564\", \"P18_ACT1564\",\"P18_ACTOCC1564\",\"P18_CHOM1564\",\n",
    "         \"C18_ACT1564\", \"C18_ACT1564_CS1\",\"C18_ACT1564_CS3\",\"C18_ACT1564_CS2\",\"C18_ACT1564_CS4\",\"C18_ACTOCC1564\",\n",
    "        \"C18_ACTOCC1564_CS1\" ,\"C18_ACTOCC1564_CS2\",\"C18_ACTOCC1564_CS3\", \"C18_ACTOCC1564_CS4\",\"P18_ACTOCC15P_ILT1\",\"C18_ACTOCC15P\",\n",
    "         \"C18_ACTOCC15P_PAS\" ,\"C18_ACTOCC15P_MAR\" ,\"C18_ACTOCC15P_VELO\" ,\"C18_ACTOCC15P_2ROUESMOT\" ,\"C18_ACTOCC15P_VOIT\" ,\n",
    "         \"C18_ACTOCC15P_TCOM\"], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = columns_featuring_act(df_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = df_stat[df_stat['IRIS'].isin(df['IRIS'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_stat, left_on='IRIS', right_on='IRIS',\n",
    "          suffixes=('_left', '_right'),  how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"aggregatedfile_houses_dep_WITH_IRIS_STATS.csv\", sep='|', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test model enrichissement logement et activité \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('aggregatedfile_houses_dep_WITH_IRIS_STATS.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_commune\": object\n",
    ",\"clean_code_departement\" : object\n",
    ",\"IRIS\": object\n",
    ",\"LAB_IRIS\" : object\n",
    ",\"Taux_RP\" : np.float32\n",
    ",\"parcelle_cad_section\" : object })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from house_prediction_package.data import GetData\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from more_itertools import chunked\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#sans doute à supprimer au lancement final du modele\n",
    "#from locale import atof, setlocale, LC_NUMERIC, LC_ALL\n",
    "\n",
    "#setlocale(LC_ALL, 'fr_FR.UTF-8')\n",
    "\n",
    "class Preprocessing :\n",
    "\n",
    "    def __init__(self,df) :\n",
    "        # self.df = get_data().read_csv()\n",
    "        self.df = df\n",
    "\n",
    "    def clean_columns(self,\n",
    "                      columns=[\n",
    "                          'Code service CH', 'Reference document',\n",
    "                          '1 Articles CGI', '2 Articles CGI', '3 Articles CGI',\n",
    "                          '4 Articles CGI', '5 Articles CGI', 'No Volume',\n",
    "                          'Identifiant local'\n",
    "                      ]):\n",
    "        \"\"\" drop useless columns\n",
    "        Customisation of columns to drop must be entered as a list\n",
    "        \"\"\"\n",
    "        # suppression of 100% empty columns - these columns are officially not completed in this db\n",
    "        self.df = self.df.drop(columns,axis=1)\n",
    "        # suppression of columns poorly completed\n",
    "        columns_to_drop = [column for column in self.df.columns if ((self.df[column].isnull().value_counts().sort_index()[0]/self.df.shape[0])*100) < 2 ]\n",
    "        self.df= self.df.drop(columns_to_drop,axis=1)\n",
    "        # replacement of , by . in numerical variables & deletion of non numrical caracters in num columns : \n",
    "        columns_num = ['Valeur fonciere', 'Surface Carrez du 1er lot', 'Nombre de lots',\n",
    "        'Surface reelle bati', 'Nombre pieces principales', 'Surface terrain']\n",
    "        # transformation des , en . pour réaliser des opérations sur les nombres et suppressions des caracteres non numériques au sein de ces colonnes \n",
    "        for column in columns_num : \n",
    "            self.df[column]=self.df[column].apply(lambda s: s.replace(\",\",\".\") if isinstance(s,str) else s)\n",
    "            self.df[column] = pd.to_numeric(self.df[column], errors = 'coerce')\n",
    "        # suppression of nan value on target variable\n",
    "        self.df= self.df.dropna(subset=['Valeur fonciere'])\n",
    "        #self.df['Surface Carrez du 1er lot'] = self.df['Surface Carrez du 1er lot'].apply(\n",
    "        #    lambda x: atof(x))        \n",
    "        # pre processing avant groupby mais attention sortir valeures foncieres avant de mettre en POO\n",
    "        ob_columns= self.df.dtypes[self.df.dtypes == 'O'].index\n",
    "        num_columns = self.df.dtypes[(self.df.dtypes == 'int')\n",
    "                                     | (self.df.dtypes == 'float')].index\n",
    "        non_num_col = ['No disposition', 'No voie', 'Code postal', 'Code commune',\n",
    "       'Prefixe de section', 'No plan','Code type local']\n",
    "        num_columns = [value for value in num_columns if value not in non_num_col]\n",
    "        for column in ob_columns :\n",
    "            self.df[column]=self.df[column].replace(np.nan,'',regex=True)\n",
    "        #à adapter in v2\n",
    "        \n",
    "        self.df[num_columns] = self.df[num_columns].apply(pd.to_numeric,\n",
    "                                                              errors='coerce')\n",
    "        \n",
    "        #drop duplicates\n",
    "        self.df = self.df.drop_duplicates().reset_index(drop= True)\n",
    "        # by returning self, we can do method chaining like preprocessing(df).clean_columns().create_identifier()\n",
    "        return self.df\n",
    "\n",
    "    def create_identifier(self) :\n",
    "        \"\"\" Create a 'unique' identifier allowing us to group several lines corresponding to a unique transaction\n",
    "        \"\"\"\n",
    "        variables_to_clean = [\n",
    "            \"Code departement\", \"Code commune\", \"Prefixe de section\",\n",
    "            \"Section\", \"No plan\"\n",
    "            ]\n",
    "        size_variables= [2,3,3,2,4]\n",
    "        for i,j in zip(variables_to_clean,size_variables):\n",
    "            chunked_data = chunked(self.df[i], 10000, strict=False)\n",
    "            values = {\"Prefixe de section\": '000'}\n",
    "            self.df= self.df.fillna(value=values)\n",
    "            if i == \"Prefixe de section\" :\n",
    "                self.df[i] = self.df[i].apply(str).apply(lambda x: x[:3])\n",
    "            new_variable = [\n",
    "                str(value).replace(\".\",\"\").zfill(j) for sublist in list(chunked_data)\n",
    "                for value in sublist\n",
    "            ]\n",
    "            self.df[f\"clean_{i.replace(' ','_').lower()}\"] = new_variable\n",
    "            self.df= self.df.drop([i],axis=1)\n",
    "        self.df[\"parcelle_cadastrale\"] = self.df[[\n",
    "            \"clean_code_departement\", \"clean_code_commune\", \"clean_prefixe_de_section\",\n",
    "            \"clean_section\", \"clean_no_plan\"]].apply(lambda x: \"\".join(x), axis=1)\n",
    "        self.df[\"parcelle_cad_section\"]=self.df[\"parcelle_cadastrale\"].str[:10]\n",
    "        self.df = self.df.drop([\n",
    "            \"clean_prefixe_de_section\", \"clean_section\", \"clean_no_plan\"\n",
    "        ], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def aggregate_transactions(self):\n",
    "        self.df = self.df.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "            \"num_voie\" : x[\"No voie\"].max()\n",
    "            ,\"B_T_Q\" : x[\"B/T/Q\"].max()\n",
    "            ,\"type_de_voie\": x[\"Type de voie\"].max()\n",
    "            ,\"voie\": x[\"Voie\"].max()\n",
    "            ,\"code_postal\": x[\"Code postal\"].max()\n",
    "            ,\"commune\": max(x[\"Commune\"])\n",
    "            ,\"clean_code_departement\": x[\"clean_code_departement\"].max()\n",
    "            ,\"clean_code_commune\": max(x[\"clean_code_commune\"])\n",
    "            ,\"surface_carrez_lot_1\" :  x[\"Surface Carrez du 1er lot\"].sum()/((x[\"Surface reelle bati\"].count()/x[\"Nature culture\"].nunique()))\n",
    "            ,\"Nb_lots\": x[\"Nombre de lots\"].max()\n",
    "            ,\"surface_terrain\" : ((x[\"Surface terrain\"].sum()/x[\"Surface reelle bati\"].count()) if (int(x[\"Surface terrain\"].nunique()) ==1 and int(x[\"Nature culture\"].nunique()) == 1 )else x[\"Surface terrain\"].sum())\n",
    "            ,\"surface_reelle_bati\" : (x[\"Surface reelle bati\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if (int(x[\"Nature culture\"].nunique() > 1)) else x[\"Surface reelle bati\"].sum())\n",
    "            ,\"nb_pieces_principales\" : (x[\"Nombre pieces principales\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if int(x[\"Nature culture\"].nunique()) > 1 else x[\"Nombre pieces principales\"].sum())      \n",
    "            ,\"dependance\" : x[\"Type local\"].unique()\n",
    "            ,\"main_type_terrain\" : x[\"Nature culture\"].max()\n",
    "            ,\"parcelle_cadastrale\": x[\"parcelle_cadastrale\"].max()}))\n",
    "        self.df = self.df.replace(np.inf, np.nan)\n",
    "        #drop rows with only dependances transactions as we focus on houses\n",
    "        self.df = self.df[self.df.dependance.apply(\n",
    "            lambda x: x.all() != \"Dépendance\")].reset_index(drop=True)\n",
    "        self.df[\"dependance\"] = self.df.dependance.apply(sorted, 1)\n",
    "        self.df[[\"Dependance\",\n",
    "                 \"Maison\"]] = pd.DataFrame(self.df.dependance.tolist(),\n",
    "                                           index=self.df.index)\n",
    "        self.df[\"Dependance\"] = [1 if value ==\"Dépendance\"else 0 for value in self.df[\"Dependance\"]]\n",
    "        self.df= self.df.drop([\"dependance\",\"Maison\"],axis =1)\n",
    "        return self.df\n",
    "\n",
    "    # to do : function calling enrichissement from data\n",
    "\n",
    "\n",
    "    def feature_generation (self):\n",
    "        # convert the 'Date' column to datetime format\n",
    "        self.df[\"month\"] = pd.to_datetime(\n",
    "            self.df[\"Date mutation\"],format=\"%d/%m/%Y\").dt.month\n",
    "        self.df= self.df.drop([\"Date mutation\"], axis = 1)\n",
    "        ## attention à ne faire qu'après avoir enrichi avec variables insee\n",
    "        dict_type_voie = dict()\n",
    "        for value in self.df[\"type_de_voie\"].value_counts()[self.df[\"type_de_voie\"].value_counts()<300 ].index.values :\n",
    "            dict_type_voie[value] = \"Autres\"\n",
    "        self.df=self.df.replace({\"type_voie\" : dict_type_voie})\n",
    "        self.df[\"type_de_voie\"]= self.df[\"type_de_voie\"].replace(np.nan,'vide')\n",
    "        return self.df\n",
    "\n",
    "    def zscore (self) :\n",
    "        # Calculate the z-score from scratch\n",
    "        #self.df['Valeur fonciere']= df['Valeur fonciere'].apply(lambda x: atof(x))\n",
    "        standard_deviation = self.df[\"Valeur fonciere\"].std(ddof=0)\n",
    "        mean_value = self.df[\"Valeur fonciere\"].mean()\n",
    "        zscores = [(value - mean_value) / standard_deviation\n",
    "                for value in self.df[\"Valeur fonciere\"]]\n",
    "        self.df[\"zscores\"]= zscores\n",
    "        # absolute value of zscore and if sup x then 1  :\n",
    "        self.df[\"outlier\"] = [\n",
    "            1 if (abs(value) > 0.2) else 0 for value in self.df[\"zscores\"]\n",
    "        ]\n",
    "        self.df=self.df[self.df[\"outlier\"] == 0].reset_index(drop=True)\n",
    "        self.df = self.df.drop([\"zscores\",\"outlier\"], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def split_x_y (self):\n",
    "        columns_model = [\"type_de_voie\",\n",
    "            \"clean_code_departement\",\n",
    "            \"clean_code_commune\",\n",
    "            \"code_postal\",\n",
    "            \"surface_terrain\",\n",
    "            \"surface_reelle_bati\", \"nb_pieces_principales\",\n",
    "            \"main_type_terrain\",  \"Dependance\",'Taux_RP', 'Taux_LV', 'Taux_MAI',\n",
    "       'Taux_RP_1P', 'Taux_RP_2P', 'Taux_RP_3P', 'Taux_RP_4P', 'Taux_RP_5P',\n",
    "       'Taux_RP_30', 'Taux_RP_40', 'Taux_RP_60', 'Taux_RP_80', 'Taux_RP_100',\n",
    "       'Taux_RP_120', 'Taux_RP_P120', 'Taux_RP_GAR', 'Taux_RP_PROPRIO',\n",
    "       'Taux_RP_GRATUIT', 'Taux_RP_LOC', 'Taux_RP_HML', 'Taux_RP_AM02',\n",
    "       'Taux_RP_AM04', 'Taux_RP_AM09', 'Taux_RP_AM09P','Taux_1524',\n",
    "       'Taux_2554', 'Taux_5564', 'Taux_P_Act', 'Taux_P_ActOct', 'Taux_P_CHO',\n",
    "       'Taux_CS1', 'Taux_CS2', 'Taux_CS3', 'Taux_CS4', 'Taux_Travail_Commune',\n",
    "       'Taux_TT', 'Taux_Mar', 'Taux_Velo', 'Taux_2Roues', 'Taux_Voit',\n",
    "       'Taux_TCOM',\n",
    "            \"month\"]\n",
    "        # Séparation des variables catégorielles et numériques\n",
    "        categorical_features = [\n",
    "            \"type_de_voie\", \"clean_code_departement\", \"clean_code_commune\",\n",
    "            \"code_postal\", \"main_type_terrain\", \"Dependance\", \"month\"\n",
    "        ]\n",
    "        numerical_features = [\n",
    "            \"surface_terrain\", \"surface_reelle_bati\", \"nb_pieces_principales\",'Taux_RP', 'Taux_LV', 'Taux_MAI',\n",
    "       'Taux_RP_1P', 'Taux_RP_2P', 'Taux_RP_3P', 'Taux_RP_4P', 'Taux_RP_5P',\n",
    "       'Taux_RP_30', 'Taux_RP_40', 'Taux_RP_60', 'Taux_RP_80', 'Taux_RP_100',\n",
    "       'Taux_RP_120', 'Taux_RP_P120', 'Taux_RP_GAR', 'Taux_RP_PROPRIO',\n",
    "       'Taux_RP_GRATUIT', 'Taux_RP_LOC', 'Taux_RP_HML', 'Taux_RP_AM02',\n",
    "       'Taux_RP_AM04', 'Taux_RP_AM09', 'Taux_RP_AM09P','Taux_1524',\n",
    "       'Taux_2554', 'Taux_5564', 'Taux_P_Act', 'Taux_P_ActOct', 'Taux_P_CHO',\n",
    "       'Taux_CS1', 'Taux_CS2', 'Taux_CS3', 'Taux_CS4', 'Taux_Travail_Commune',\n",
    "       'Taux_TT', 'Taux_Mar', 'Taux_Velo', 'Taux_2Roues', 'Taux_Voit',\n",
    "       'Taux_TCOM'\n",
    "        ]\n",
    "        for column in categorical_features:\n",
    "            self.df[column] = self.df[column].replace(np.nan, \"\").apply(str)\n",
    "        X = self.df[columns_model]\n",
    "        y =self.df[\"Valeur fonciere\"]\n",
    "        # selection des variables\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            test_size=0.33,\n",
    "                                                            random_state=42)\n",
    "        return self.df,categorical_features, numerical_features, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "#from house_prediction_package.preprocessing import Preprocessing\n",
    "#from house_prediction_package.data import GetData\n",
    "\n",
    "\n",
    "class Pipeline :\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        # self.categorical_features = categorical_features\n",
    "        # self.numerical_features = numerical_features\n",
    "        # self.X_train = X_train\n",
    "        # self.y_train = y_train\n",
    "        # option 2\n",
    "        #appeler les méthodes\n",
    "        self.df, self.categorical_features, self.numerical_features, self.X_train, self.X_test, self.y_train, self.y_test = Preprocessing(\n",
    "            df).feature_generation().zscore().split_x_y()\n",
    "\n",
    "    def pipeline(self):\n",
    "        # création des pipelines de pré-processing pour les variables numériques et catégorielles\n",
    "        #ajout d'un parametre pour gerer les valeures non connues dans onehotencoder - il les passe à 0(autres options disponibles)\n",
    "        numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "        categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, self.numerical_features),\n",
    "            (categorical_pipeline, self.categorical_features))\n",
    "        model = make_pipeline(preprocessor, LinearRegression())\n",
    "        fitted_model = model.fit(self.X_train, self.y_train)\n",
    "        return fitted_model, self.X_train, self.y_train,self.X_test, self.y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).feature_generation()\n",
    "\n",
    "df = Preprocessing(df).zscore() \n",
    "#df = Preprocessing(df).zscore() \n",
    "\n",
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test  = Preprocessing(df).split_x_y()\n",
    "\n",
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R(MSE): %.2f\" % math.sqrt(np.mean((test_y_hat - y_test)**2)))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))\n",
    "\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_dep_model_aggregations_logement_act.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result with zscore à 0.2 et ensemble des stats : \n",
    "* Score r²:  0.6722149896989466\n",
    "* Mean absolute error: 64178.07\n",
    "* Residual  of squares (MSE): 9194771971.12\n",
    "* R(MSE): 95889.37\n",
    "* R2-score: 0.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(preprocessor, RandomForestRegressor())\n",
    "\n",
    "fitted_model = model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R(MSE): %.2f\" % math.sqrt(np.mean((test_y_hat - y_test)**2)))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))\n",
    "\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_dep_model_aggregations_logement_act_forest.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifiers\n",
    "reg1 = GradientBoostingRegressor(random_state=1)\n",
    "reg2 = RandomForestRegressor(random_state=1)\n",
    "reg3 = LinearRegression()\n",
    "ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "ereg = ereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autres possibilités \n",
    "SGD regressor \n",
    "LInearSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification regroupement modalité code commune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(f'http://127.0.0.1:5555/api/insee/logement/distribution/010040201?by=area').json()['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cas de tests sur aggregations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= GetData().read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).clean_columns()\n",
    "\n",
    "df= Preprocessing(df).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "\"surface_terrain\" : ((x[\"Surface terrain\"].sum()/x[\"Surface reelle bati\"].count()) if (int(x[\"Surface terrain\"].nunique()) ==1 and int(x[\"Nature culture\"].nunique()) == 1 )else x[\"Surface terrain\"].sum())\n",
    "    ,\"Nature_culture\" : x[\"Nature culture\"].max()\n",
    "    , \"su terrain 2\": x[\"Surface terrain\"].sum()\n",
    "    , \"suterrainmax\": x[\"Surface terrain\"].max()\n",
    "  #  , \"su_bat\": x[\"Surface reelle bati\"]\n",
    "    ,\"nat_terrain_unique\": x[\"Nature culture\"].nunique()\n",
    "    , \"suterrain_count\" : x[\"Surface terrain\"].count()\n",
    "    ,\"suterrain_unique\": x[\"Surface terrain\"].nunique()\n",
    "    ,\"su_bat_unique\" : x[\"Surface reelle bati\"].nunique()\n",
    "    ,\"su_bat_count\" : x[\"Surface reelle bati\"].count()\n",
    "            \n",
    "})).tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "    \"surface_reelle_bati\" : (x[\"Surface reelle bati\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if (int(x[\"Nature culture\"].nunique() > 1)) else x[\"Surface reelle bati\"].sum())\n",
    " ,\"nb_pieces_principales\" : (x[\"Nombre pieces principales\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if int(x[\"Nature culture\"].nunique()) > 1 else x[\"Nombre pieces principales\"].sum())      \n",
    "    ,\"nb_piecemax\" : x[\"Nombre pieces principales\"].max()\n",
    "    ,\"Nature_culture\" : x[\"Nature culture\"].max()\n",
    "    , \"su bat 2\": x[\"Surface reelle bati\"].sum()\n",
    "    , \"sumax\": x[\"Surface reelle bati\"].max()\n",
    "  #  , \"su_bat\": x[\"Surface reelle bati\"]\n",
    "    , \"su_count\" : x[\"Surface reelle bati\"].count()\n",
    "    ,\"nat_cul_unique\": x[\"Nature culture\"].nunique()\n",
    "    ,\"subatiment_unique\": x[\"Surface reelle bati\"].nunique()\n",
    "            \n",
    "})).tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tot= GetData().read_csv([1,3])\n",
    "\n",
    "df_tot = Preprocessing(df_tot).clean_columns()\n",
    "\n",
    "df_tot= Preprocessing(df_tot).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cas ou type local identique mais nature culture différente: \n",
    "\n",
    "df_tot[(df_tot['parcelle_cadastrale']== '01289000AC0176') | (df_tot['parcelle_cadastrale']== '013500000C1248')| (df_tot['parcelle_cadastrale']== '01195000AD0050')]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# meme de surface reelle bati \n",
    "#pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "#meme nombre pieces principales \n",
    "# différente nature culture (variable texte )\n",
    "# différente surface terrain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cas ou type local identique mais nature culture différente: \n",
    "\n",
    "df[(df['parcelle_cadastrale']== '01289000AC0176') | (df['parcelle_cadastrale']== '013500000C1248')| (df['parcelle_cadastrale']== '01195000AD0050')]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# meme de surface reelle bati \n",
    "#pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "#meme nombre pieces principales \n",
    "# différente nature culture (variable texte )\n",
    "# différente surface terrain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['parcelle_cad_section']=='01001000ZH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# même maison surface reelle bati identique \n",
    "df[(df['parcelle_cadastrale'] == '013500000C1248')]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# meêm type local \n",
    "# même surface relle bati\n",
    " #pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "# même nombre de pieces principales\n",
    "\n",
    "# différence nature culture (variable texte )\n",
    "# surface terrain différente en fonction de la parcelle cadastrale \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cas ou 2 maisons , une dépendance et un terrain :\n",
    "# repérable par section et date commune \n",
    "\n",
    "df[df.index.isin([71,72,73,74])]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# code type lcoal différent pr dépendance absent pour terrain\n",
    "# différence de surface reelle bati  (0 dépendance et nan pour terrain)\n",
    "#pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "# différence sur nombre de pieces principales\n",
    "\n",
    "# différence nature culture (variable texte )\n",
    "# surface terrain différente en fonction de la parcelle cadastrale \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat= pd.read_csv('../data/base-ic-activite-residents-2018.CSV',sep=';', dtype= {\n",
    "\"IRIS\" : object\n",
    ",\"COM\": object \n",
    ",\"TYP_IRIS\":  object \n",
    ",\"MODIF_IRIS\": object \n",
    ",\"LAB_IRIS\": object\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat= pd.read_csv('../data/base-ic-logement-2018.CSV',sep=';', dtype= {\n",
    "\"IRIS\" : object\n",
    ",\"COM\": object \n",
    ",\"TYP_IRIS\":  object \n",
    ",\"MODIF_IRIS\": object \n",
    ",\"LAB_IRIS\": object\n",
    ",'P18_RP_ELEC': np.float32, 'P18_RP_EAUCH':np.float32, 'P18_RP_BDWC':np.float32, 'P18_RP_CHOS':np.float32,\n",
    "       'P18_RP_CLIM':np.float32, 'P18_RP_TTEGOU':np.float32, 'P18_RP_GARL':np.float32, 'P18_RP_VOIT1P':np.float32,\n",
    "       'P18_RP_VOIT1':np.float32, 'P18_RP_VOIT2P':np.float32, 'P18_RP_HABFOR':np.float32, 'P18_RP_CASE':np.float32,\n",
    "       'P18_RP_MIBOIS':np.float32,'P18_RP_MIDUR':np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat=df_stat[['IRIS', 'LAB_IRIS','P18_LOG', 'P18_RP', 'P18_RSECOCC', 'P18_LOGVAC', 'P18_MAISON', 'P18_APPART','P18_RP_1P', \n",
    "         'P18_RP_2P', 'P18_RP_3P', 'P18_RP_4P', 'P18_RP_5PP','P18_RP_M30M2', 'P18_RP_3040M2', 'P18_RP_4060M2',\n",
    "       'P18_RP_6080M2', 'P18_RP_80100M2', 'P18_RP_100120M2', 'P18_RP_120M2P','P18_RP_GARL','P18_RP_PROP', \n",
    "         'P18_RP_LOC', 'P18_RP_LOCHLMV','P18_RP_GRAT','P18_MEN_ANEM0002', 'P18_MEN_ANEM0204',\n",
    "       'P18_MEN_ANEM0509', 'P18_MEN_ANEM10P']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loading_data_in_db:\n",
    "    \"\"\" class helping to load data into a database\n",
    "    please enter db name as second argument \n",
    "    please enter table name as third argument\"\"\"\n",
    "\n",
    "    def __init__(self, df, db_name, table_name) :\n",
    "        self.df = df\n",
    "        self.db_name = db_name\n",
    "        self.table_name= table_name\n",
    "\n",
    "    def load_df_db (self) :\n",
    "        engine = create_engine(f'sqlite:///../data/{self.db_name}.sqlite',\n",
    "                                   echo=True)  # pass your db url\n",
    "        self.df.to_sql(name=self.table_name,con =engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loading_data_in_db(df_stat,'house_pred_database', 'logements_stats').load_df_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('aggregatedfile_houses_dep_WITH_IRIS_STATS.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_commune\": object\n",
    ",\"clean_code_departement\" : object\n",
    ",\"IRIS\": object\n",
    ",\"LAB_IRIS\" : object\n",
    ",\"Taux_RP\" : np.float32\n",
    ",\"parcelle_cad_section\" : object })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30,20))\n",
    "# Generate a mask to onlyshow the bottom triangle\n",
    "mask = np.triu(np.ones_like(df[[\"Valeur fonciere\",\n",
    "            \"surface_terrain\",\n",
    "            \"surface_reelle_bati\", \"nb_pieces_principales\",\n",
    "             \"Dependance\"]].corr(), dtype=bool))\n",
    "heatmap = sns.heatmap(df[[\"Valeur fonciere\",\n",
    "            \"surface_terrain\",\n",
    "            \"surface_reelle_bati\", \"nb_pieces_principales\",\n",
    "             \"Dependance\"]].corr().round(4), mask=mask, annot = True,  vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['LAB_IRIS', 'Taux_RP', 'Taux_LV', 'Taux_MAI',\n",
    "       'Taux_RP_1P', 'Taux_RP_2P', 'Taux_RP_3P', 'Taux_RP_4P', 'Taux_RP_5P',\n",
    "       'Taux_RP_30', 'Taux_RP_40', 'Taux_RP_60', 'Taux_RP_80', 'Taux_RP_100',\n",
    "       'Taux_RP_120', 'Taux_RP_P120', 'Taux_RP_GAR', 'Taux_RP_PROPRIO',\n",
    "       'Taux_RP_GRATUIT', 'Taux_RP_LOC', 'Taux_RP_HML', 'Taux_RP_AM02',\n",
    "       'Taux_RP_AM04', 'Taux_RP_AM09', 'Taux_RP_AM09P', 'Taux_1524',\n",
    "       'Taux_2554', 'Taux_5564', 'Taux_P_Act', 'Taux_P_ActOct', 'Taux_P_CHO',\n",
    "       'Taux_CS1', 'Taux_CS2', 'Taux_CS3', 'Taux_CS4', 'Taux_Travail_Commune',\n",
    "       'Taux_TT', 'Taux_Mar', 'Taux_Velo', 'Taux_2Roues', 'Taux_Voit',\n",
    "       'Taux_TCOM'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///../data/house_pred_database.sqlite',\n",
    "                                   echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS=tuple(df.IRIS.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-23 11:18:01,713 INFO sqlalchemy.engine.Engine [raw sql] ()\n"
     ]
    }
   ],
   "source": [
    "df_log= pd.read_sql_query(f\"SELECT * FROM logements_stats where IRIS in {IRIS}\", con = engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_keep = [\n",
    "            \"IRIS\", \"LAB_IRIS\", \"P18_LOG\", \"P18_RP\", \"P18_RSECOCC\", \"P18_LOGVAC\",\n",
    "            \"P18_MAISON\", \"P18_APPART\", \"P18_RP_1P\", \"P18_RP_2P\", \"P18_RP_3P\",\n",
    "            \"P18_RP_4P\", \"P18_RP_5PP\", \"P18_RP_M30M2\", \"P18_RP_3040M2\",\n",
    "            \"P18_RP_4060M2\", \"P18_RP_6080M2\", \"P18_RP_80100M2\", \"P18_RP_100120M2\",\n",
    "            \"P18_RP_120M2P\", \"P18_RP_GARL\", \"P18_RP_PROP\", \"P18_RP_LOC\",\n",
    "            \"P18_RP_LOCHLMV\", \"P18_RP_GRAT\", \"P18_MEN_ANEM0002\",\n",
    "            \"P18_MEN_ANEM0204\", \"P18_MEN_ANEM0509\", \"P18_MEN_ANEM10P\", \"P18_RP_ACHTOT\", \"P18_RP_ACH19\", \"P18_RP_ACH45\",\n",
    "    \"P18_RP_ACH70\",\"P18_RP_ACH90\", \"P18_RP_ACH05\", \"P18_RP_ACH15\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = df_log[variables_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_log, left_on='IRIS', right_on='IRIS',\n",
    "          suffixes=('_left', '_right'),  how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-23 11:18:28,419 INFO sqlalchemy.engine.Engine [raw sql] ()\n"
     ]
    }
   ],
   "source": [
    "df_log = pd.read_sql_query(\n",
    "            f'SELECT * FROM activites_stat WHERE IRIS in {IRIS}', con=engine)\n",
    "variables_to_keep = [\n",
    "            \"IRIS\", \"P18_POP1564\", \"P18_POP1524\", \"P18_POP2554\", \"P18_POP5564\",\n",
    "            \"P18_ACT1564\", \"P18_ACTOCC1564\", \"P18_CHOM1564\", \"C18_ACT1564\",\n",
    "            \"C18_ACT1564_CS1\", \"C18_ACT1564_CS3\", \"C18_ACT1564_CS2\",\n",
    "            \"C18_ACT1564_CS4\", \"C18_ACTOCC1564\", \"C18_ACTOCC1564_CS1\",\n",
    "            \"C18_ACTOCC1564_CS2\", \"C18_ACTOCC1564_CS3\", \"C18_ACTOCC1564_CS4\",\n",
    "            \"P18_ACTOCC15P_ILT1\", \"C18_ACTOCC15P\", \"C18_ACTOCC15P_PAS\",\n",
    "            \"C18_ACTOCC15P_MAR\", \"C18_ACTOCC15P_VELO\", \"C18_ACTOCC15P_2ROUESMOT\",\n",
    "            \"C18_ACTOCC15P_VOIT\", \"C18_ACTOCC15P_TCOM\"\n",
    "        ]\n",
    "df_log = df_log[variables_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns =['P18_LOG', 'P18_RP', 'P18_RSECOCC',\n",
    "       'P18_LOGVAC', 'P18_MAISON', 'P18_APPART', 'P18_RP_1P', 'P18_RP_2P',\n",
    "       'P18_RP_3P', 'P18_RP_4P', 'P18_RP_5PP', 'P18_RP_M30M2', 'P18_RP_3040M2',\n",
    "       'P18_RP_4060M2', 'P18_RP_6080M2', 'P18_RP_80100M2', 'P18_RP_100120M2',\n",
    "       'P18_RP_120M2P', 'P18_RP_GARL', 'P18_RP_PROP', 'P18_RP_LOC',\n",
    "       'P18_RP_LOCHLMV', 'P18_RP_GRAT', 'P18_MEN_ANEM0002', 'P18_MEN_ANEM0204',\n",
    "       'P18_MEN_ANEM0509', 'P18_MEN_ANEM10P', 'P18_RP_ACHTOT', 'P18_RP_ACH19',\n",
    "       'P18_RP_ACH45', 'P18_RP_ACH70', 'P18_RP_ACH90', 'P18_RP_ACH05',\n",
    "       'P18_RP_ACH15', 'P18_POP1564', 'P18_POP1524', 'P18_POP2554',\n",
    "       'P18_POP5564', 'P18_ACT1564', 'P18_ACTOCC1564', 'P18_CHOM1564',\n",
    "       'C18_ACT1564', 'C18_ACT1564_CS1', 'C18_ACT1564_CS3', 'C18_ACT1564_CS2',\n",
    "       'C18_ACT1564_CS4', 'C18_ACTOCC1564', 'C18_ACTOCC1564_CS1',\n",
    "       'C18_ACTOCC1564_CS2', 'C18_ACTOCC1564_CS3', 'C18_ACTOCC1564_CS4',\n",
    "       'P18_ACTOCC15P_ILT1', 'C18_ACTOCC15P', 'C18_ACTOCC15P_PAS',\n",
    "       'C18_ACTOCC15P_MAR', 'C18_ACTOCC15P_VELO', 'C18_ACTOCC15P_2ROUESMOT',\n",
    "       'C18_ACTOCC15P_VOIT', 'C18_ACTOCC15P_TCOM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_log, left_on='IRIS', right_on='IRIS',\n",
    "          suffixes=('_left', '_right'),  how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Valeur fonciere']<1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=15,ncols=4, figsize=(40,30))\n",
    "#fig.suptitle('Boxplot analysis', fontsize = 15)\n",
    "for column,axe in zip(columns,axes.flat) :\n",
    "    sns.scatterplot(data=df, x=column,y= df['Valeur fonciere'],ax = axe)\n",
    "    axe.set_title(f\"Scatterplot de {column} vs valeur fonciere\")\n",
    "    fig.tight_layout()\n",
    "#axes[4][1].remove()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_code_commune\"]=[c+a[1:] if c[:2]== '97' else (c+a) for c, a in zip(df[\"clean_code_departement\"], df[\"clean_code_commune\"])]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bord= df[df['clean_code_commune']=='33063']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=15,ncols=4, figsize=(40,30))\n",
    "#fig.suptitle('Boxplot analysis', fontsize = 15)\n",
    "for column,axe in zip(columns,axes.flat) :\n",
    "    sns.scatterplot(data=df_bord, x=column,y= df_bord['Valeur fonciere'],ax = axe, hue=df_bord['IRIS'])\n",
    "    axe.set_title(f\"Scatterplot de {column} vs valeur fonciere\")\n",
    "    fig.tight_layout()\n",
    "#axes[4][1].remove()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### opt binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_code_commune\"]=[c+a[1:] if c[:2]== '97' else (c+a) for c, a in zip(df[\"clean_code_departement\"], df[\"clean_code_commune\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optbinning import ContinuousOptimalBinning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['IRIS', 'parcelle_cad_section', 'Date mutation', 'Valeur fonciere',\n",
       "       'num_voie', 'B_T_Q', 'type_de_voie', 'voie', 'code_postal', 'commune',\n",
       "       'clean_code_departement', 'clean_code_commune', 'surface_carrez_lot_1',\n",
       "       'Nb_lots', 'surface_terrain', 'surface_reelle_bati',\n",
       "       'nb_pieces_principales', 'main_type_terrain', 'parcelle_cadastrale',\n",
       "       'Dependance', 'LAB_IRIS', 'P18_LOG', 'P18_RP', 'P18_RSECOCC',\n",
       "       'P18_LOGVAC', 'P18_MAISON', 'P18_APPART', 'P18_RP_1P', 'P18_RP_2P',\n",
       "       'P18_RP_3P', 'P18_RP_4P', 'P18_RP_5PP', 'P18_RP_M30M2', 'P18_RP_3040M2',\n",
       "       'P18_RP_4060M2', 'P18_RP_6080M2', 'P18_RP_80100M2', 'P18_RP_100120M2',\n",
       "       'P18_RP_120M2P', 'P18_RP_GARL', 'P18_RP_PROP', 'P18_RP_LOC',\n",
       "       'P18_RP_LOCHLMV', 'P18_RP_GRAT', 'P18_MEN_ANEM0002', 'P18_MEN_ANEM0204',\n",
       "       'P18_MEN_ANEM0509', 'P18_MEN_ANEM10P', 'P18_RP_ACHTOT', 'P18_RP_ACH19',\n",
       "       'P18_RP_ACH45', 'P18_RP_ACH70', 'P18_RP_ACH90', 'P18_RP_ACH05',\n",
       "       'P18_RP_ACH15', 'P18_POP1564', 'P18_POP1524', 'P18_POP2554',\n",
       "       'P18_POP5564', 'P18_ACT1564', 'P18_ACTOCC1564', 'P18_CHOM1564',\n",
       "       'C18_ACT1564', 'C18_ACT1564_CS1', 'C18_ACT1564_CS3', 'C18_ACT1564_CS2',\n",
       "       'C18_ACT1564_CS4', 'C18_ACTOCC1564', 'C18_ACTOCC1564_CS1',\n",
       "       'C18_ACTOCC1564_CS2', 'C18_ACTOCC1564_CS3', 'C18_ACTOCC1564_CS4',\n",
       "       'P18_ACTOCC15P_ILT1', 'C18_ACTOCC15P', 'C18_ACTOCC15P_PAS',\n",
       "       'C18_ACTOCC15P_MAR', 'C18_ACTOCC15P_VELO', 'C18_ACTOCC15P_2ROUESMOT',\n",
       "       'C18_ACTOCC15P_VOIT', 'C18_ACTOCC15P_TCOM'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = \"P18_MEN_ANEM0002\"\n",
    "x = df[variable].values\n",
    "y = df['Valeur fonciere']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OPTIMAL'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optb = ContinuousOptimalBinning(name=variable, dtype=\"numerical\")\n",
    "optb.fit(x, y)\n",
    "optb.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.13872862,  73.8159256 ,  85.88204956, 182.89972687,\n",
       "       336.75669861])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optb.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bin</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count (%)</th>\n",
       "      <th>Sum</th>\n",
       "      <th>Std</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Zeros count</th>\n",
       "      <th>WoE</th>\n",
       "      <th>IV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-inf, 10.14)</td>\n",
       "      <td>27199</td>\n",
       "      <td>0.062915</td>\n",
       "      <td>5.941139e+09</td>\n",
       "      <td>3584052.219341</td>\n",
       "      <td>218432.272504</td>\n",
       "      <td>1.0</td>\n",
       "      <td>220623264.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-110004.809123</td>\n",
       "      <td>6920.927572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[10.14, 73.82)</td>\n",
       "      <td>144418</td>\n",
       "      <td>0.334057</td>\n",
       "      <td>4.750623e+10</td>\n",
       "      <td>5004139.025505</td>\n",
       "      <td>328949.490864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>220623264.0</td>\n",
       "      <td>0</td>\n",
       "      <td>512.409237</td>\n",
       "      <td>171.174068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[73.82, 85.88)</td>\n",
       "      <td>21688</td>\n",
       "      <td>0.050167</td>\n",
       "      <td>7.217590e+09</td>\n",
       "      <td>3588153.829853</td>\n",
       "      <td>332791.876605</td>\n",
       "      <td>1.0</td>\n",
       "      <td>220623264.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4354.794979</td>\n",
       "      <td>218.467538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[85.88, 182.90)</td>\n",
       "      <td>140525</td>\n",
       "      <td>0.325052</td>\n",
       "      <td>4.709260e+10</td>\n",
       "      <td>3404994.108095</td>\n",
       "      <td>335119.043747</td>\n",
       "      <td>1.0</td>\n",
       "      <td>220623264.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6681.962120</td>\n",
       "      <td>2171.987386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[182.90, 336.76)</td>\n",
       "      <td>76834</td>\n",
       "      <td>0.177727</td>\n",
       "      <td>2.645849e+10</td>\n",
       "      <td>3460644.379853</td>\n",
       "      <td>344359.177138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>220623264.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15922.095511</td>\n",
       "      <td>2829.784501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[336.76, inf)</td>\n",
       "      <td>21634</td>\n",
       "      <td>0.050042</td>\n",
       "      <td>7.768365e+09</td>\n",
       "      <td>3033561.031994</td>\n",
       "      <td>359081.312080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>220623264.0</td>\n",
       "      <td>0</td>\n",
       "      <td>30644.230453</td>\n",
       "      <td>1533.505156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Special</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>-328437.081627</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Missing</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>3.858028e+06</td>\n",
       "      <td>70632.584205</td>\n",
       "      <td>226942.823529</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>442950.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-101494.258098</td>\n",
       "      <td>3.991077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Totals</th>\n",
       "      <td></td>\n",
       "      <td>432315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.419883e+11</td>\n",
       "      <td></td>\n",
       "      <td>328437.081627</td>\n",
       "      <td>1.0</td>\n",
       "      <td>220623264.0</td>\n",
       "      <td>0</td>\n",
       "      <td>598051.641147</td>\n",
       "      <td>13849.837298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Bin   Count  Count (%)           Sum             Std  \\\n",
       "0          (-inf, 10.14)   27199   0.062915  5.941139e+09  3584052.219341   \n",
       "1         [10.14, 73.82)  144418   0.334057  4.750623e+10  5004139.025505   \n",
       "2         [73.82, 85.88)   21688   0.050167  7.217590e+09  3588153.829853   \n",
       "3        [85.88, 182.90)  140525   0.325052  4.709260e+10  3404994.108095   \n",
       "4       [182.90, 336.76)   76834   0.177727  2.645849e+10  3460644.379853   \n",
       "5          [336.76, inf)   21634   0.050042  7.768365e+09  3033561.031994   \n",
       "6                Special       0   0.000000  0.000000e+00             NaN   \n",
       "7                Missing      17   0.000039  3.858028e+06    70632.584205   \n",
       "Totals                    432315   1.000000  1.419883e+11                   \n",
       "\n",
       "                 Mean       Min          Max  Zeros count            WoE  \\\n",
       "0       218432.272504       1.0  220623264.0            0 -110004.809123   \n",
       "1       328949.490864       1.0  220623264.0            0     512.409237   \n",
       "2       332791.876605       1.0  220623264.0            0    4354.794979   \n",
       "3       335119.043747       1.0  220623264.0            0    6681.962120   \n",
       "4       344359.177138       1.0  220623264.0            0   15922.095511   \n",
       "5       359081.312080       1.0  220623264.0            0   30644.230453   \n",
       "6            0.000000       NaN          NaN            0 -328437.081627   \n",
       "7       226942.823529  130000.0     442950.0            0 -101494.258098   \n",
       "Totals  328437.081627       1.0  220623264.0            0  598051.641147   \n",
       "\n",
       "                  IV  \n",
       "0        6920.927572  \n",
       "1         171.174068  \n",
       "2         218.467538  \n",
       "3        2171.987386  \n",
       "4        2829.784501  \n",
       "5        1533.505156  \n",
       "6           0.000000  \n",
       "7           3.991077  \n",
       "Totals  13849.837298  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optb.binning_table.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "OptimalBinning: Continuous Binning Table Analysis\n",
      "-------------------------------------------------\n",
      "\n",
      "  General metrics\n",
      "\n",
      "    IV                   13849.83729789\n",
      "    WoE                 598051.64114672\n",
      "    WoE (normalized)         1.82090170\n",
      "    HHI                      0.25781941\n",
      "    HHI (normalized)         0.15179361\n",
      "    Quality score            0.00062155\n",
      "\n",
      "  Monotonic trend             ascending\n",
      "\n",
      "  Significance tests\n",
      "\n",
      "    Bin A  Bin B  t-statistic  p-value\n",
      "        0      1    -4.349349 0.000014\n",
      "        1      2    -0.138737 0.889658\n",
      "        2      3    -0.089497 0.928688\n",
      "        3      4    -0.598479 0.549521\n",
      "        4      5    -0.610650 0.541435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optb.binning_table.analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "optb.binning_table.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_code_commune'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.at[18,'main_type_terrain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['main_type_terrain'].value_counts()df['main_type_terrain'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "        categorical_features = [\n",
    "            \"type_de_voie\", \"clean_code_departement\", \"clean_code_commune\",\n",
    "            \"code_postal\", \"main_type_terrain\", \"Dependance\", \"month\"\n",
    "        ]\n",
    "        numerical_features = [\n",
    "            \"surface_terrain\", \"surface_reelle_bati\", \"nb_pieces_principales\",'Taux_RP', 'Taux_LV', 'Taux_MAI',\n",
    "       'Taux_RP_1P', 'Taux_RP_2P', 'Taux_RP_3P', 'Taux_RP_4P', 'Taux_RP_5P',\n",
    "       'Taux_RP_30', 'Taux_RP_40', 'Taux_RP_60', 'Taux_RP_80', 'Taux_RP_100',\n",
    "       'Taux_RP_120', 'Taux_RP_P120', 'Taux_RP_GAR', 'Taux_RP_PROPRIO',\n",
    "       'Taux_RP_GRATUIT', 'Taux_RP_LOC', 'Taux_RP_HML', 'Taux_RP_AM02',\n",
    "       'Taux_RP_AM04', 'Taux_RP_AM09', 'Taux_RP_AM09P','Taux_1524',\n",
    "       'Taux_2554', 'Taux_5564', 'Taux_P_Act', 'Taux_P_ActOct', 'Taux_P_CHO',\n",
    "       'Taux_CS1', 'Taux_CS2', 'Taux_CS3', 'Taux_CS4', 'Taux_Travail_Commune',\n",
    "       'Taux_TT', 'Taux_Mar', 'Taux_Velo', 'Taux_2Roues', 'Taux_Voit',\n",
    "       'Taux_TCOM'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'house_dep_model_aggregations_logement_act.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result=loaded_model.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://api-adresse.data.gouv.fr/search/?q=42+rue=turenne&limit=15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'FeatureCollection',\n",
       " 'version': 'draft',\n",
       " 'features': [{'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [-0.582833, 44.845789]},\n",
       "   'properties': {'label': '42 Rue Turenne 33000 Bordeaux',\n",
       "    'score': 0.8946181818181818,\n",
       "    'housenumber': '42',\n",
       "    'id': '33063_9115_00042',\n",
       "    'name': '42 Rue Turenne',\n",
       "    'postcode': '33000',\n",
       "    'citycode': '33063',\n",
       "    'x': 416994.98,\n",
       "    'y': 6422712.79,\n",
       "    'city': 'Bordeaux',\n",
       "    'context': '33, Gironde, Nouvelle-Aquitaine',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.8408,\n",
       "    'street': 'Rue Turenne'}},\n",
       "  {'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [4.073029, 48.293806]},\n",
       "   'properties': {'label': '42 Rue Turenne 10000 Troyes',\n",
       "    'score': 0.8819545454545454,\n",
       "    'housenumber': '42',\n",
       "    'id': '10387_5180_00042',\n",
       "    'name': '42 Rue Turenne',\n",
       "    'postcode': '10000',\n",
       "    'citycode': '10387',\n",
       "    'x': 779579.91,\n",
       "    'y': 6799815.93,\n",
       "    'city': 'Troyes',\n",
       "    'context': '10, Aube, Grand Est',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.7015,\n",
       "    'street': 'Rue Turenne'}},\n",
       "  {'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [7.339576, 47.757776]},\n",
       "   'properties': {'label': '42 Rue Turenne 68100 Mulhouse',\n",
       "    'score': 0.8806918181818181,\n",
       "    'housenumber': '42',\n",
       "    'id': '68224_7410_00042',\n",
       "    'name': '42 Rue Turenne',\n",
       "    'postcode': '68100',\n",
       "    'citycode': '68224',\n",
       "    'x': 1024959.31,\n",
       "    'y': 6748640.93,\n",
       "    'city': 'Mulhouse',\n",
       "    'context': '68, Haut-Rhin, Grand Est',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.68761,\n",
       "    'street': 'Rue Turenne'}},\n",
       "  {'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [3.91062, 50.25449]},\n",
       "   'properties': {'label': '42 Rue Turenne 59330 Hautmont',\n",
       "    'score': 0.877770909090909,\n",
       "    'housenumber': '42',\n",
       "    'id': '59291_1250_00042',\n",
       "    'name': '42 Rue Turenne',\n",
       "    'postcode': '59330',\n",
       "    'citycode': '59291',\n",
       "    'x': 765020.28,\n",
       "    'y': 7017767.7,\n",
       "    'city': 'Hautmont',\n",
       "    'context': '59, Nord, Hauts-de-France',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.65548,\n",
       "    'street': 'Rue Turenne'}},\n",
       "  {'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [7.332922, 47.827986]},\n",
       "   'properties': {'label': '42 Rue Turenne 68270 Wittenheim',\n",
       "    'score': 0.8735963636363636,\n",
       "    'housenumber': '42',\n",
       "    'id': '68376_1270_00042',\n",
       "    'name': '42 Rue Turenne',\n",
       "    'postcode': '68270',\n",
       "    'citycode': '68376',\n",
       "    'x': 1024033.69,\n",
       "    'y': 6756402.75,\n",
       "    'city': 'Wittenheim',\n",
       "    'context': '68, Haut-Rhin, Grand Est',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.60956,\n",
       "    'street': 'Rue Turenne'}},\n",
       "  {'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [3.811474, 50.200735]},\n",
       "   'properties': {'label': '42 Rue Turenne 59145 Berlaimont',\n",
       "    'score': 0.8693445454545454,\n",
       "    'housenumber': '42',\n",
       "    'id': '59068_0820_00042',\n",
       "    'name': '42 Rue Turenne',\n",
       "    'postcode': '59145',\n",
       "    'citycode': '59068',\n",
       "    'x': 758002.82,\n",
       "    'y': 7011704.35,\n",
       "    'city': 'Berlaimont',\n",
       "    'context': '59, Nord, Hauts-de-France',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.56279,\n",
       "    'street': 'Rue Turenne'}},\n",
       "  {'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [3.607131, 43.500704]},\n",
       "   'properties': {'label': '42 Rue Turenne 34560 Villeveyrac',\n",
       "    'score': 0.8683254545454545,\n",
       "    'housenumber': '42',\n",
       "    'id': '34341_0112_00042',\n",
       "    'name': '42 Rue Turenne',\n",
       "    'postcode': '34560',\n",
       "    'citycode': '34341',\n",
       "    'x': 749121.81,\n",
       "    'y': 6267035.49,\n",
       "    'city': 'Villeveyrac',\n",
       "    'context': '34, Hérault, Occitanie',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.55158,\n",
       "    'street': 'Rue Turenne'}},\n",
       "  {'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [3.038015, 50.627914]},\n",
       "   'properties': {'label': '42 Rue de Turenne 59000 Lille',\n",
       "    'score': 0.7361781818181818,\n",
       "    'housenumber': '42',\n",
       "    'id': '59350_8789_00042',\n",
       "    'name': '42 Rue de Turenne',\n",
       "    'postcode': '59000',\n",
       "    'citycode': '59350',\n",
       "    'oldcitycode': '59350',\n",
       "    'x': 702694.39,\n",
       "    'y': 7058992.47,\n",
       "    'city': 'Lille',\n",
       "    'oldcity': 'Lille',\n",
       "    'context': '59, Nord, Hauts-de-France',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.78546,\n",
       "    'street': 'Rue de Turenne'}},\n",
       "  {'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [5.718857, 45.183062]},\n",
       "   'properties': {'label': '42 Rue de Turenne 38000 Grenoble',\n",
       "    'score': 0.7304845454545454,\n",
       "    'housenumber': '42',\n",
       "    'id': '38185_6690_00042',\n",
       "    'name': '42 Rue de Turenne',\n",
       "    'postcode': '38000',\n",
       "    'citycode': '38185',\n",
       "    'x': 913501.99,\n",
       "    'y': 6457426.3,\n",
       "    'city': 'Grenoble',\n",
       "    'context': '38, Isère, Auvergne-Rhône-Alpes',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.72283,\n",
       "    'street': 'Rue de Turenne'}},\n",
       "  {'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [2.364588, 48.85773]},\n",
       "   'properties': {'label': '42 Rue de Turenne 75003 Paris',\n",
       "    'score': 0.7288181818181818,\n",
       "    'housenumber': '42',\n",
       "    'id': '75103_9506_00042',\n",
       "    'name': '42 Rue de Turenne',\n",
       "    'postcode': '75003',\n",
       "    'citycode': '75103',\n",
       "    'x': 653378.96,\n",
       "    'y': 6862153.52,\n",
       "    'city': 'Paris',\n",
       "    'district': 'Paris 3e Arrondissement',\n",
       "    'context': '75, Paris, Île-de-France',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.7045,\n",
       "    'street': 'Rue de Turenne'}},\n",
       "  {'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [2.99956, 43.176782]},\n",
       "   'properties': {'label': '42 Rue de Turenne 11100 Narbonne',\n",
       "    'score': 0.7282681818181819,\n",
       "    'housenumber': '42',\n",
       "    'id': '11262_5040_00042',\n",
       "    'name': '42 Rue de Turenne',\n",
       "    'postcode': '11100',\n",
       "    'citycode': '11262',\n",
       "    'x': 699964.2,\n",
       "    'y': 6230838.98,\n",
       "    'city': 'Narbonne',\n",
       "    'context': '11, Aude, Occitanie',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.69845,\n",
       "    'street': 'Rue de Turenne'}},\n",
       "  {'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [1.843127, 50.94281]},\n",
       "   'properties': {'label': '42 Rue de Turenne 62100 Calais',\n",
       "    'score': 0.72815,\n",
       "    'housenumber': '42',\n",
       "    'id': '62193_4540_00042',\n",
       "    'name': '42 Rue de Turenne',\n",
       "    'postcode': '62100',\n",
       "    'citycode': '62193',\n",
       "    'x': 618521.47,\n",
       "    'y': 7094685.5,\n",
       "    'city': 'Calais',\n",
       "    'context': '62, Pas-de-Calais, Hauts-de-France',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.69715,\n",
       "    'street': 'Rue de Turenne'}},\n",
       "  {'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [3.076933, 50.656198]},\n",
       "   'properties': {'label': '42 Rue de Turenne 59110 La Madeleine',\n",
       "    'score': 0.72585,\n",
       "    'housenumber': '42',\n",
       "    'id': '59368_1610_00042',\n",
       "    'name': '42 Rue de Turenne',\n",
       "    'postcode': '59110',\n",
       "    'citycode': '59368',\n",
       "    'x': 705449.71,\n",
       "    'y': 7062146.19,\n",
       "    'city': 'La Madeleine',\n",
       "    'context': '59, Nord, Hauts-de-France',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.67185,\n",
       "    'street': 'Rue de Turenne'}},\n",
       "  {'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [1.530826, 45.15345]},\n",
       "   'properties': {'label': '42 Rue De Turenne 19100 Brive-la-Gaillarde',\n",
       "    'score': 0.7258145454545455,\n",
       "    'housenumber': '42',\n",
       "    'id': '19031_6450_00042',\n",
       "    'name': '42 Rue De Turenne',\n",
       "    'postcode': '19100',\n",
       "    'citycode': '19031',\n",
       "    'x': 584553.76,\n",
       "    'y': 6451535.57,\n",
       "    'city': 'Brive-la-Gaillarde',\n",
       "    'context': '19, Corrèze, Nouvelle-Aquitaine',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.67146,\n",
       "    'street': 'Rue De Turenne'}},\n",
       "  {'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [3.14961, 50.702854]},\n",
       "   'properties': {'label': '42 Rue de Turenne 59420 Mouvaux',\n",
       "    'score': 0.7239472727272728,\n",
       "    'housenumber': '42',\n",
       "    'id': '59421_0730_00042',\n",
       "    'name': '42 Rue de Turenne',\n",
       "    'postcode': '59420',\n",
       "    'citycode': '59421',\n",
       "    'x': 710588.07,\n",
       "    'y': 7067352.78,\n",
       "    'city': 'Mouvaux',\n",
       "    'context': '59, Nord, Hauts-de-France',\n",
       "    'type': 'housenumber',\n",
       "    'importance': 0.65092,\n",
       "    'street': 'Rue de Turenne'}}],\n",
       " 'attribution': 'BAN',\n",
       " 'licence': 'ETALAB-2.0',\n",
       " 'query': '42 rue=turenne',\n",
       " 'limit': 15}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "house_prediction",
   "language": "python",
   "name": "house_prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "323px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "7e331f8398536a24290f874ffd288d2ba0d3938ea5219b401a264de1a09e8807"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
