{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from locale import atof, setlocale, LC_NUMERIC, LC_ALL\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "#setlocale(LC_ALL, 'fr_FR.UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GetData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class GetData:\n",
    "    \"\"\" Read data from csv and load it in a dataframe\n",
    "    accepted arguments : path to file , separator, chunksize and filter\n",
    "    option to load csv by filtering on house type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,path =\"../data/valeursfoncieres-2021.txt\",sep = \"|\", chunksize = 100000):\n",
    "        self.path = path\n",
    "        self.sep = sep\n",
    "        self.chunksize = chunksize\n",
    "\n",
    "\n",
    "    def read_csv(self, filtering_column='Code type local', filter=[1]):\n",
    "        \"\"\" pass option on which column to filter and filter value\n",
    "        if several filter value, pass the as a list\"\"\"\n",
    "        iter_csv = pd.read_csv(self.path,\n",
    "                               sep=self.sep,\n",
    "                               iterator=True,\n",
    "                               chunksize=self.chunksize,\n",
    "                               low_memory=False)\n",
    "        self.df = pd.concat([\n",
    "            chunk[chunk[filtering_column].isin(filter)] for chunk in iter_csv\n",
    "        ])\n",
    "        return self.df\n",
    "\n",
    "    def enrichissement_coordinates(self,df):\n",
    "        pass\n",
    "\n",
    "    def enrichissement_insee(self,df):\n",
    "        pass\n",
    "\n",
    "    def loading_data_db (self,df):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout l'objectif de cette étape est de tirer le maximum du dataset, en reconstituant ce qui correspond réellement à une transaction. \n",
    "Nous avons réalisé cela en regroupant chaque transaction selon 3 clés : Parcelle / date / montant car ceux ci sont répété sur les lignes communes. \n",
    "Ensuite nous avons utilisé la methode apply qui pour chaque ligne du dataset aggrégé applique une fonction lambda dans laquelle on inséré une série dont les paramétres étaient la ligne aggrée. cela nous a permis de réaliser des fonctions plus complexes au niveau des aggrégats et de couvrir les différents cas ammenant à la mutltiplication des lignes \n",
    "Les cas plus complexes à retrouver sont par exemple la nature culture basée principale en se basant sur la superficie de celle ci (--- piste https://stackoverflow.com/questions/23394476/keep-other-columns-when-doing-groupby \n",
    "L'autre cas complexe était de compter le nombre de dépendances différentes ainsi que le nombre de maisons différentes au sein d'une même aggrégation -**ajouter cas ou on a plusieurs dépendances et plusieurs terrains -- trop complexe** : \n",
    "* #dépendance - count du terme dépendance valuecount sorted[0]  / nunique type local si nunique nat culture = 1 & len value count ==2 else count terme dependance valuecount sorted[0]   if len_value count ==2 else 0\n",
    "* #maisons - count terme maison value count sorted [1] / nunqiue type local si nunique nat culture = 1 & len value count ==2 else count terme maison value count if len value_count ==2 else 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from house_prediction_package.data import GetData\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from more_itertools import chunked\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#sans doute à supprimer au lancement final du modele\n",
    "#from locale import atof, setlocale, LC_NUMERIC, LC_ALL\n",
    "\n",
    "#setlocale(LC_ALL, 'fr_FR.UTF-8')\n",
    "\n",
    "class Preprocessing :\n",
    "\n",
    "    def __init__(self,df) :\n",
    "        # self.df = get_data().read_csv()\n",
    "        self.df = df\n",
    "\n",
    "    def clean_columns(self,\n",
    "                      columns=[\n",
    "                          'Code service CH', 'Reference document',\n",
    "                          '1 Articles CGI', '2 Articles CGI', '3 Articles CGI',\n",
    "                          '4 Articles CGI', '5 Articles CGI', 'No Volume',\n",
    "                          'Identifiant local'\n",
    "                      ]):\n",
    "        \"\"\" drop useless columns\n",
    "        Customisation of columns to drop must be entered as a list\n",
    "        \"\"\"\n",
    "        # suppression of 100% empty columns - these columns are officially not completed in this db\n",
    "        self.df = self.df.drop(columns,axis=1)\n",
    "        # suppression of columns poorly completed\n",
    "        columns_to_drop = [column for column in self.df.columns if ((self.df[column].isnull().value_counts().sort_index()[0]/self.df.shape[0])*100) < 2 ]\n",
    "        self.df= self.df.drop(columns_to_drop,axis=1)\n",
    "        # replacement of , by . in numerical variables & deletion of non numrical caracters in num columns : \n",
    "        columns_num = ['Valeur fonciere', 'Surface Carrez du 1er lot', 'Nombre de lots',\n",
    "        'Surface reelle bati', 'Nombre pieces principales', 'Surface terrain']\n",
    "        # transformation des , en . pour réaliser des opérations sur les nombres et suppressions des caracteres non numériques au sein de ces colonnes \n",
    "        for column in columns_num : \n",
    "            self.df[column]=self.df[column].apply(lambda s: s.replace(\",\",\".\") if isinstance(s,str) else s)\n",
    "            self.df[column] = pd.to_numeric(self.df[column], errors = 'coerce')\n",
    "        # suppression of nan value on target variable\n",
    "        self.df= self.df.dropna(subset=['Valeur fonciere'])\n",
    "        #self.df['Surface Carrez du 1er lot'] = self.df['Surface Carrez du 1er lot'].apply(\n",
    "        #    lambda x: atof(x))        \n",
    "        # pre processing avant groupby mais attention sortir valeures foncieres avant de mettre en POO\n",
    "        ob_columns= self.df.dtypes[self.df.dtypes == 'O'].index\n",
    "        num_columns = self.df.dtypes[(self.df.dtypes == 'int')\n",
    "                                     | (self.df.dtypes == 'float')].index\n",
    "        non_num_col = ['No disposition', 'No voie', 'Code postal', 'Code commune',\n",
    "       'Prefixe de section', 'No plan','Code type local']\n",
    "        num_columns = [value for value in num_columns if value not in non_num_col]\n",
    "        for column in ob_columns :\n",
    "            self.df[column]=self.df[column].replace(np.nan,'',regex=True)\n",
    "        #à adapter in v2\n",
    "        \n",
    "        self.df[num_columns] = self.df[num_columns].apply(pd.to_numeric,\n",
    "                                                              errors='coerce')\n",
    "        \n",
    "        #drop duplicates\n",
    "        self.df = self.df.drop_duplicates().reset_index(drop= True)\n",
    "        # by returning self, we can do method chaining like preprocessing(df).clean_columns().create_identifier()\n",
    "        return self.df\n",
    "\n",
    "    def create_identifier(self) :\n",
    "        \"\"\" Create a 'unique' identifier allowing us to group several lines corresponding to a unique transaction\n",
    "        \"\"\"\n",
    "        variables_to_clean = [\n",
    "            \"Code departement\", \"Code commune\", \"Prefixe de section\",\n",
    "            \"Section\", \"No plan\"\n",
    "            ]\n",
    "        size_variables= [2,3,3,2,4]\n",
    "        for i,j in zip(variables_to_clean,size_variables):\n",
    "            chunked_data = chunked(self.df[i], 10000, strict=False)\n",
    "            values = {\"Prefixe de section\": '000'}\n",
    "            self.df= self.df.fillna(value=values)\n",
    "            if i == \"Prefixe de section\" :\n",
    "                self.df[i] = self.df[i].apply(str).apply(lambda x: x[:3])\n",
    "            new_variable = [\n",
    "                str(value).replace(\".\",\"\").zfill(j) for sublist in list(chunked_data)\n",
    "                for value in sublist\n",
    "            ]\n",
    "            self.df[f\"clean_{i.replace(' ','_').lower()}\"] = new_variable\n",
    "            self.df= self.df.drop([i],axis=1)\n",
    "        self.df[\"parcelle_cadastrale\"] = self.df[[\n",
    "            \"clean_code_departement\", \"clean_code_commune\", \"clean_prefixe_de_section\",\n",
    "            \"clean_section\", \"clean_no_plan\"]].apply(lambda x: \"\".join(x), axis=1)\n",
    "        self.df[\"parcelle_cad_section\"]=self.df[\"parcelle_cadastrale\"].str[:10]\n",
    "        self.df = self.df.drop([\n",
    "            \"clean_prefixe_de_section\", \"clean_section\", \"clean_no_plan\"\n",
    "        ], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def aggregate_transactions(self):\n",
    "        self.df = self.df.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "            \"num_voie\" : x[\"No voie\"].max()\n",
    "            ,\"B_T_Q\" : x[\"B/T/Q\"].max()\n",
    "            ,\"type_de_voie\": x[\"Type de voie\"].max()\n",
    "            ,\"voie\": x[\"Voie\"].max()\n",
    "            ,\"code_postal\": x[\"Code postal\"].max()\n",
    "            ,\"commune\": max(x[\"Commune\"])\n",
    "            ,\"clean_code_departement\": x[\"clean_code_departement\"].max()\n",
    "            ,\"clean_code_commune\": max(x[\"clean_code_commune\"])\n",
    "            ,\"surface_carrez_lot_1\" :  x[\"Surface Carrez du 1er lot\"].sum()/((x[\"Surface reelle bati\"].count()/x[\"Nature culture\"].nunique()))\n",
    "            ,\"Nb_lots\": x[\"Nombre de lots\"].max()\n",
    "            ,\"surface_terrain\" : ((x[\"Surface terrain\"].sum()/x[\"Surface reelle bati\"].count()) if (int(x[\"Surface terrain\"].nunique()) ==1 and int(x[\"Nature culture\"].nunique()) == 1 )else x[\"Surface terrain\"].sum())\n",
    "            ,\"surface_reelle_bati\" : (x[\"Surface reelle bati\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if (int(x[\"Nature culture\"].nunique() > 1)) else x[\"Surface reelle bati\"].sum())\n",
    "            ,\"nb_pieces_principales\" : (x[\"Nombre pieces principales\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if int(x[\"Nature culture\"].nunique()) > 1 else x[\"Nombre pieces principales\"].sum())      \n",
    "            ,\"dependance\" : x[\"Type local\"].unique()\n",
    "            ,\"main_type_terrain\" : x[\"Nature culture\"].max()\n",
    "            ,\"parcelle_cadastrale\": x[\"parcelle_cadastrale\"].max()}))\n",
    "        self.df = self.df.replace(np.inf, np.nan)\n",
    "        #drop rows with only dependances transactions as we focus on houses\n",
    "        self.df = self.df[self.df.dependance.apply(\n",
    "            lambda x: x.all() != \"Dépendance\")].reset_index(drop=True)\n",
    "        self.df[\"dependance\"] = self.df.dependance.apply(sorted, 1)\n",
    "        self.df[[\"Dependance\",\n",
    "                 \"Maison\"]] = pd.DataFrame(self.df.dependance.tolist(),\n",
    "                                           index=self.df.index)\n",
    "        self.df[\"Dependance\"] = [1 if value ==\"Dépendance\"else 0 for value in self.df[\"Dependance\"]]\n",
    "        self.df= self.df.drop([\"dependance\",\"Maison\"],axis =1)\n",
    "        return self.df\n",
    "\n",
    "    # to do : function calling enrichissement from data\n",
    "\n",
    "\n",
    "    def feature_generation (self):\n",
    "        # convert the 'Date' column to datetime format\n",
    "        self.df[\"month\"] = pd.to_datetime(\n",
    "            self.df[\"Date mutation\"],format=\"%d/%m/%Y\").dt.month\n",
    "        self.df= self.df.drop([\"Date mutation\"], axis = 1)\n",
    "        ## attention à ne faire qu'après avoir enrichi avec variables insee\n",
    "        dict_type_voie = dict()\n",
    "        for value in self.df[\"type_de_voie\"].value_counts()[self.df[\"type_de_voie\"].value_counts()<300 ].index.values :\n",
    "            dict_type_voie[value] = \"Autres\"\n",
    "        self.df=self.df.replace({\"type_voie\" : dict_type_voie})\n",
    "        self.df[\"type_de_voie\"]= self.df[\"type_de_voie\"].replace(np.nan,'vide')\n",
    "        return self.df\n",
    "\n",
    "    def zscore (self) :\n",
    "        # Calculate the z-score from scratch\n",
    "        #self.df['Valeur fonciere']= df['Valeur fonciere'].apply(lambda x: atof(x))\n",
    "        standard_deviation = self.df[\"Valeur fonciere\"].std(ddof=0)\n",
    "        mean_value = self.df[\"Valeur fonciere\"].mean()\n",
    "        zscores = [(value - mean_value) / standard_deviation\n",
    "                for value in self.df[\"Valeur fonciere\"]]\n",
    "        self.df[\"zscores\"]= zscores\n",
    "        # absolute value of zscore and if sup x then 1  :\n",
    "        self.df[\"outlier\"] = [\n",
    "            1 if (abs(value) > 3) else 0 for value in self.df[\"zscores\"]\n",
    "        ]\n",
    "        self.df=self.df[self.df[\"outlier\"] == 0].reset_index(drop=True)\n",
    "        self.df = self.df.drop([\"zscores\",\"outlier\"], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def split_x_y (self):\n",
    "        columns_model = [\"type_de_voie\",\n",
    "            \"clean_code_departement\",\n",
    "            \"clean_code_commune\",\n",
    "            \"code_postal\",\n",
    "            \"surface_terrain\",\n",
    "            \"surface_reelle_bati\", \"nb_pieces_principales\",\n",
    "            \"main_type_terrain\",  \"Dependance\",\n",
    "            \"month\"]\n",
    "        # Séparation des variables catégorielles et numériques\n",
    "        categorical_features = [\n",
    "            \"type_de_voie\", \"clean_code_departement\", \"clean_code_commune\",\n",
    "            \"code_postal\", \"main_type_terrain\", \"Dependance\", \"month\"\n",
    "        ]\n",
    "        numerical_features = [\n",
    "            \"surface_terrain\", \"surface_reelle_bati\", \"nb_pieces_principales\"\n",
    "        ]\n",
    "        for column in categorical_features:\n",
    "            self.df[column] = self.df[column].replace(np.nan, \"\").apply(str)\n",
    "        X = self.df[columns_model]\n",
    "        y =self.df[\"Valeur fonciere\"]\n",
    "        # selection des variables\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            test_size=0.33,\n",
    "                                                            random_state=42)\n",
    "        return self.df,categorical_features, numerical_features, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "#from house_prediction_package.preprocessing import Preprocessing\n",
    "#from house_prediction_package.data import GetData\n",
    "\n",
    "\n",
    "class Pipeline :\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        # self.categorical_features = categorical_features\n",
    "        # self.numerical_features = numerical_features\n",
    "        # self.X_train = X_train\n",
    "        # self.y_train = y_train\n",
    "        # option 2\n",
    "        #appeler les méthodes\n",
    "        self.df, self.categorical_features, self.numerical_features, self.X_train, self.X_test, self.y_train, self.y_test = Preprocessing(\n",
    "            df).feature_generation().zscore().split_x_y()\n",
    "\n",
    "    def pipeline(self):\n",
    "        # création des pipelines de pré-processing pour les variables numériques et catégorielles\n",
    "        #ajout d'un parametre pour gerer les valeures non connues dans onehotencoder - il les passe à 0(autres options disponibles)\n",
    "        numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "        categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, self.numerical_features),\n",
    "            (categorical_pipeline, self.categorical_features))\n",
    "        model = make_pipeline(preprocessor, LinearRegression())\n",
    "        fitted_model = model.fit(self.X_train, self.y_train)\n",
    "        return fitted_model, self.X_train, self.y_train,self.X_test, self.y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zone de tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model sans aggrégation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = GetData().read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test = Preprocessing(df).split_x_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())\n",
    "fitted_model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_model_wo_aggregations.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model maison avec aggrégation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= GetData().read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).clean_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= Preprocessing(df).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).aggregate_transactions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"aggregatedfile_houses.csv\", sep='|', encoding=\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).feature_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).zscore() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test =  Preprocessing(df).split_x_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())\n",
    "fitted_model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[X_test.surface_terrain== np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "X_test = X_test.replace(np.inf, np.nan)\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R(MSE): %.2f\" % math.sqrt(np.mean((test_y_hat - y_test)**2)))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score r²:  0.4806680640761509      \n",
    "Mean absolute error: 83460.59     \n",
    "Residual  of squares (MSE): 46023819610.40     \n",
    "R(MSE): 214531.63    \n",
    "R2-score: -0.02    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_model_aggregations.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test model maison/dep with aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= GetData().read_csv(filter=[1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).clean_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= Preprocessing(df).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).aggregate_transactions()\n",
    "\n",
    "df.to_csv(\"aggregatedfile_houses_dep.csv\", sep='|', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = Preprocessing(df).feature_generation()\n",
    "\n",
    "df = Preprocessing(df).zscore() \n",
    "\n",
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test  = Preprocessing(df).split_x_y()\n",
    "\n",
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R(MSE): %.2f\" % math.sqrt(np.mean((test_y_hat - y_test)**2)))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))\n",
    "\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_dep_model_aggregations.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrichissement lat long iris insee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lit le nouveau csv aggrégé qui nous a fait réduire le nombre de lignes de 1 M à 430 K \n",
    "Ensuite, on utilise l'api ban \n",
    "enrichissement 20 h trop long\n",
    "test avec csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"aggregatedfile_houses_dep.csv\", sep = \"|\", index_col=0,dtype ={\"parcelle_cad_section\":  str  \n",
    ",\"Date mutation\": object \n",
    ",\"Valeur fonciere\"  : np.float32\n",
    ",\"num_voie\" : np.float32\n",
    ",\"B_T_Q\" : object \n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"code_postal\": np.float64\n",
    ",\"commune\": object \n",
    ",\"clean_code_departement\": object \n",
    ",\"clean_code_commune\": object  \n",
    ",\"surface_carrez_lot_1\": np.float32\n",
    ",\"Nb_lots\": np.int32  \n",
    ",\"surface_terrain\":  np.float32\n",
    ",\"surface_reelle_bati\":np.float32\n",
    ",\"nb_pieces_principales\":np.float32\n",
    ",\"main_type_terrain\":object \n",
    ",\"parcelle_cadastrale\": object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DataFrame.apply() and lambda function\\n\",\n",
    "# df_adresses['No voie']= df_adresses['No voie'].astype(int)\\n\",\n",
    "# on ne peut pas passer les num voies/code postaux  en int/string sans d'abord nettoyer les nan values \\n\",\n",
    "df[\"voie\"]=df[\"voie\"].replace(\" \",\"+\")\n",
    "df[\"adresse\"] = df[[\"num_voie\", \"type_de_voie\", \"voie\"]].apply(lambda x: \"+\".join(x.astype(str)), axis=1)\n",
    "#df['clean_code_commune'] = df[[\"clean_code_departement\",\"clean_code_commune\"]].apply(lambda x: \"\".join(x.astype(str)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_code_commun =[]\n",
    "for dep,comm in zip(df[\"clean_code_departement\"],df['clean_code_commune']):\n",
    "    if len(dep) == 3: \n",
    "        full_code_commun.append(dep + comm[1:3])\n",
    "    else : \n",
    "        full_code_commun.append(dep + comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_code_commune'] = full_code_commun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"adresse\"]= df[[\"adresse\",\"clean_code_commune\"]].apply(lambda x : \"&citycode=\".join(x.astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "# reassemblage by pd.concat possible mais on s'en fiche car on va fonctionner sur des'petits df' \n",
    "#pour enrichissement puis insertion en bdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df[0]['adresse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#long=[]\n",
    "#lat= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = 'ok'\n",
    "start_time = datetime.now()\n",
    "\n",
    "for j in range(2,len(list_df)):\n",
    "    if test == 'ok':\n",
    "        for value in list_df[j]['adresse']:\n",
    "            try : \n",
    "                long.append(requests.get(f'http://localhost:7878/search?q={value}').json()['features'][0]['geometry']['coordinates'][0])\n",
    "                lat.append(requests.get(f'http://localhost:7878/search/?q={value}').json()['features'][0]['geometry']['coordinates'][1])\n",
    "            except  : \n",
    "                lat.append('not found')\n",
    "                long.append('not found')            \n",
    "    test= input(f\"iteration {j}, pour passer à l'itération {j+1} taper ok  : \")\n",
    "    f=j\n",
    "end_time = datetime.now()\n",
    "print('Duration: {} et arret à la {}'.format(end_time - start_time, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iteration 2, pour passer à l'itération 3 taper ok  : ok    \n",
    "iteration 3, pour passer à l'itération 4 taper ok  : ok    \n",
    "iteration 4, pour passer à l'itération 5 taper ok  : ok    \n",
    "iteration 5, pour passer à l'itération 6 taper ok  : ok    \n",
    "iteration 6, pour passer à l'itération 7 taper ok  : ok    \n",
    "iteration 7, pour passer à l'itération 8 taper ok  : ok    \n",
    "iteration 8, pour passer à l'itération 9 taper ok  : ok    \n",
    "iteration 9, pour passer à l'itération 10 taper ok  : ok    \n",
    "iteration 10, pour passer à l'itération 11 taper ok  : ok    \n",
    "iteration 12, pour passer à l'itération 13 taper ok  : ok    \n",
    "iteration 13, pour passer à l'itération 14 taper ok  : ko    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'lat.sav'\n",
    "pickle.dump(lat, open(filename, 'wb'))\n",
    "filename2= 'long.sav'\n",
    "pickle.dump(lat, open(filename2, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "requests.get(\"http://localhost:7878/search/?q=27.0+RUE+DES PINS&citycode=97424\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération CSV enrichi en masse et récupération IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/geolocgeocoded.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_commune\": object, \"latitude\": np.float32, \"longitude\": np.float32, \"result_score\" : np.float32 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"num_voie\",\"type_de_voie\",\"voie\",\"commune\",\"clean_code_commune\",\"latitude\", \"longitude\",\"result_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pour lancer pyris : \n",
    "1. d'abord activer lancer postgres sql \n",
    " * sudo service postgresql start\n",
    "2.  puis depuis le dossier  house pred /api / pyris : \n",
    " * gunicorn -b 127.0.0.1:5555 pyris.api.run:app \n",
    "\n",
    "Qu'est ce qu'IRIS : \"Afin de préparer la diffusion du recensement de la population de 1999, l'INSEE avait développé un découpage du territoire en mailles de taille homogène appelées IRIS2000. Un sigle qui signifiait « Ilots Regroupés pour l'Information Statistique » et qui faisait référence à la taille visée de 2 000 habitants par maille élémentaire.\" source : https://www.insee.fr/fr/metadonnees/definition/c1523    \n",
    "API fonctionne avec Postgis ( extension de postgres pour les données géospatiales).    \n",
    "Pour qu'elle fonctionne, j'ai donc télécharger les fichiers suivants : \n",
    " * contours iris \n",
    " * références iris \n",
    " * divers statistiques INSEE à l'échelon IRIS \n",
    "     * statistiques activités -\n",
    "     * statistiques démographiques - ménages et evol pop \n",
    "     * statistiques scolaires\n",
    "     * statistiques logements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['coordinates'] = df[[\"latitude\",\"longitude\"]].apply(lambda x : \"&lon=\".join(x.astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api url \n",
    "url = \"http://127.0.0.1:5555/api/coords?geojson=false&lat=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50000  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "# reassemblage by pd.concat possible mais on s'en fiche car on va fonctionner sur des'petits df' \n",
    "#pour enrichissement puis insertion en bdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'ok'\n",
    "start_time = datetime.now()\n",
    "\n",
    "for j in range(0,len(list_df)):\n",
    "    if test == 'ok':\n",
    "        for value in list_df[j]['coordinates']:\n",
    "            try : \n",
    "                IRIS.append(requests.get(f'{url}{value}').json()['complete_code'])\n",
    "            except  : \n",
    "                IRIS.append('not found')\n",
    "    test= input(f\"iteration {j}, pour passer à l'itération {j+1} taper ok  : \")\n",
    "    f=j\n",
    "end_time = datetime.now()\n",
    "print('Duration: {} et arret à la {}'.format(end_time - start_time, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS= pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IRIS']= IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[df['IRIS'] =='not found']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recherche par ville sur les valeures non trouvées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in df[df['IRIS'] =='not found'].iterrows() : \n",
    "    print('######',index,'*****', value['clean_code_commune'])\n",
    "    try :\n",
    "        df.at[index,'IRIS'] = requests.get(f\"http://127.0.0.1:5555/api/city/code/{value['clean_code_commune']}\").json()[0]\n",
    "    except : \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche api open data soft \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les dernieres valeures non trouvées appartiennent aux dom tom Martinique Guadeloupe Guyanne et Réunion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://data.opendatasoft.com/api/records/1.0/search/?dataset=iris-millesime-france%40lareunion&q=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/api/records/1.0/search/?dataset=iris-millesime-france&q=97101&sort=year&facet=com_arm_name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appel d'un api externe pour la réunion : \n",
    "** ajouter un prefiltre dans le code **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://data.opendatasoft.com/explore/dataset/iris-millesime-france%40lareunion/api/?disjunctive.reg_name&disjunctive.dep_name&disjunctive.arrdep_name&disjunctive.ze2020_name&disjunctive.bv2012_name&disjunctive.epci_name&disjunctive.ept_name&disjunctive.com_name&disjunctive.com_arm_name&disjunctive.iris_name&sort=year&q=97424&geofilter.polygon=&geofilter.distance="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in df[df['IRIS'] =='not found'].iterrows() : \n",
    "    print('######',index,'*****', value['clean_code_commune'])\n",
    "    try :\n",
    "        df.at[index,'IRIS'] = requests.get(f\"{url}{value['clean_code_commune']}&sort=year&facet=com_arm_name\").json()['records'][0]['fields']['iris_code']\n",
    "    except : \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appel d'un api externe pour la guadeloupe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://regionguadeloupe.opendatasoft.com/explore/dataset/iris-millesime-france/information/?disjunctive.reg_name&disjunctive.dep_name&disjunctive.arrdep_name&disjunctive.ze2020_name&disjunctive.bv2012_name&disjunctive.epci_name&disjunctive.ept_name&disjunctive.com_name&disjunctive.com_arm_name&disjunctive.iris_name&sort=year&q=97101&dataChart=eyJxdWVyaWVzIjpbeyJjb25maWciOnsiZGF0YXNldCI6ImlyaXMtbWlsbGVzaW1lLWZyYW5jZSIsIm9wdGlvbnMiOnsiZGlzanVuY3RpdmUucmVnX25hbWUiOnRydWUsImRpc2p1bmN0aXZlLmRlcF9uYW1lIjp0cnVlLCJkaXNqdW5jdGl2ZS5hcnJkZXBfbmFtZSI6dHJ1ZSwiZGlzanVuY3RpdmUuemUyMDIwX25hbWUiOnRydWUsImRpc2p1bmN0aXZlLmJ2MjAxMl9uYW1lIjp0cnVlLCJkaXNqdW5jdGl2ZS5lcGNpX25hbWUiOnRydWUsImRpc2p1bmN0aXZlLmVwdF9uYW1lIjp0cnVlLCJkaXNqdW5jdGl2ZS5jb21fbmFtZSI6dHJ1ZSwiZGlzanVuY3RpdmUuY29tX2FybV9uYW1lIjp0cnVlLCJkaXNqdW5jdGl2ZS5pcmlzX25hbWUiOnRydWUsInNvcnQiOiJ5ZWFyIiwicSI6Im1hcmllIGdhbGFudGUgZ3JhbmQgYm91cmciLCJyZWZpbmUuemUyMDIwX25hbWUiOiJNYXJpZS1HYWxhbnRlIiwicmVmaW5lLmNvbV9uYW1lIjoiR3JhbmQtQm91cmcifX0sImNoYXJ0cyI6W3siYWxpZ25Nb250aCI6dHJ1ZSwidHlwZSI6ImxpbmUiLCJmdW5jIjoiQ09VTlQiLCJzY2llbnRpZmljRGlzcGxheSI6dHJ1ZSwiY29sb3IiOiIjRUQ5QTlBIn1dLCJ4QXhpcyI6InllYXIiLCJtYXhwb2ludHMiOiIiLCJ0aW1lc2NhbGUiOiJ5ZWFyIiwic29ydCI6IiJ9XSwiZGlzcGxheUxlZ2VuZCI6dHJ1ZSwiYWxpZ25Nb250aCI6dHJ1ZX0%3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://regionguadeloupe.opendatasoft.com/api/records/1.0/search/?dataset=iris-millesime-france&q=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, value in df[df['IRIS'] =='not found'].iterrows() : \n",
    "    print('######',index,'*****', value['clean_code_commune'])\n",
    "    if value['clean_code_commune'][0:3] == '971' :\n",
    "        try :\n",
    "            df.at[index,'IRIS'] = requests.get(f\"{url}{value['clean_code_commune']}&sort=year&facet=com_arm_name\").json()['records'][0]['fields']['iris_code']\n",
    "        except : \n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appel d'un autre api pour martinique et guyanne \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://public.opendatasoft.com/explore/dataset/georef-france-iris/api/?disjunctive.reg_name&disjunctive.dep_name&disjunctive.arrdep_name&disjunctive.ze2020_name&disjunctive.bv2012_name&disjunctive.epci_name&disjunctive.ept_name&disjunctive.com_name&disjunctive.com_arm_name&disjunctive.iris_name&sort=year&q=97201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://public.opendatasoft.com/api/records/1.0/search/?dataset=georef-france-iris&q=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, value in df[df['IRIS'] =='not found'].iterrows() : \n",
    "    print('######',index,'*****', value['clean_code_commune'])\n",
    "    try :\n",
    "        df.at[index,'IRIS'] = requests.get(f\"{url}{value['clean_code_commune']}&sort=year&facet=com_arm_name\").json()['records'][0]['fields']['iris_code']\n",
    "    except : \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reconstitution df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init = pd.read_csv('../data/geolocgeocoded.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_commune\": object, \"latitude\": np.float32, \"longitude\": np.float32, \"result_score\" : np.float32 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init= df_init[[\"num_voie\",\"type_de_voie\",\"voie\",\"commune\",\"clean_code_commune\",\"latitude\", \"longitude\",\"result_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour code final garder plutot l'id de résultat qui correspond à lat et long trouvé et dédoublonner selon cet id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init['coordinates'] = df_init[[\"latitude\",\"longitude\"]].apply(lambda x : \"&lon=\".join(x.astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init = df_init.merge(df[['coordinates','IRIS']], left_on='coordinates', right_on='coordinates',\n",
    "          suffixes=('_left', '_right'),  how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init.to_csv(\"aggregatedfile_houses_dep_WITH_IRIS.csv\", sep='|', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrichissement variables INSEE sur CODES IRIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('aggregatedfile_houses_dep_WITH_IRIS.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_commune\": object, \"latitude\": np.float32, \"longitude\": np.float32, \"result_score\" : np.float32, \n",
    "'IRIS': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"http://127.0.0.1:5555/api/insee/activite/distribution/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inititing lists : \n",
    "actif_15_24 = []\n",
    "actif_25_54 = []\n",
    "actif_55_64 = []\n",
    "chomage_15_24 = []\n",
    "chomage_25_54 = []\n",
    "chomage_55_64 = []\n",
    "taux_chomage_15_24 = []\n",
    "taux_chomage_25_54 = []\n",
    "taux_chomage_55_64 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in df.IRIS.unique() : \n",
    "    try:\n",
    "        result = requests.get(f'http://127.0.0.1:5555/api/insee/activite/distribution/{value}?by=age').json()['data']\n",
    "        for variable in result.keys():\n",
    "            try: \n",
    "                if variable == 'taux_chomage_15_24':\n",
    "                    taux_chomage_15_24.append(result[variable])\n",
    "                elif variable == 'taux_chomage_25_54':\n",
    "                    taux_chomage_25_54.append(result[variable])\n",
    "                elif variable == 'taux_chomage_55_64':\n",
    "                    taux_chomage_55_64.append(result[variable])\n",
    "            except : \n",
    "                if variable == 'taux_chomage_15_24':\n",
    "                    taux_chomage_15_24.append('not_found')\n",
    "                elif variable == 'taux_chomage_25_54':\n",
    "                    taux_chomage_25_54.append('not found')\n",
    "                elif variable == 'taux_chomage_55_64':\n",
    "                    taux_chomage_55_64.append('not_found')\n",
    "    except: \n",
    "        taux_chomage_15_24.append('not_found')\n",
    "        taux_chomage_25_54.append('not found')\n",
    "        taux_chomage_55_64.append('not_found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requests.get(f'http://127.0.0.1:5555/api/insee/activite/distribution/010010000?by=age').json()['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stat=pd.DataFrame({'IRIS': df.IRIS.unique(),'taux_chomage_15_24':taux_chomage_15_24,\"taux_chomage_25_54\":taux_chomage_25_54,\"taux_chomage_55_64\":taux_chomage_55_64 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IRIS</th>\n",
       "      <th>taux_chomage_15_24</th>\n",
       "      <th>taux_chomage_25_54</th>\n",
       "      <th>taux_chomage_55_64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010010000</td>\n",
       "      <td>30.781699</td>\n",
       "      <td>7.951426</td>\n",
       "      <td>3.349618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010020000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>2.083333</td>\n",
       "      <td>19.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010040101</td>\n",
       "      <td>15.546274</td>\n",
       "      <td>13.509962</td>\n",
       "      <td>8.700207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010040102</td>\n",
       "      <td>21.93555</td>\n",
       "      <td>19.040939</td>\n",
       "      <td>11.767701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010040201</td>\n",
       "      <td>31.624587</td>\n",
       "      <td>15.586252</td>\n",
       "      <td>7.263252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39229</th>\n",
       "      <td>974200601</td>\n",
       "      <td>57.169849</td>\n",
       "      <td>34.490463</td>\n",
       "      <td>37.954605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39230</th>\n",
       "      <td>974210301</td>\n",
       "      <td>57.009346</td>\n",
       "      <td>37.608319</td>\n",
       "      <td>33.858268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39231</th>\n",
       "      <td>974220901</td>\n",
       "      <td>63.18449</td>\n",
       "      <td>35.561451</td>\n",
       "      <td>33.546692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39232</th>\n",
       "      <td>974230103</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>25.080608</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39233</th>\n",
       "      <td>974240101</td>\n",
       "      <td>54.43038</td>\n",
       "      <td>37.544484</td>\n",
       "      <td>26.595745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39234 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            IRIS taux_chomage_15_24 taux_chomage_25_54 taux_chomage_55_64\n",
       "0      010010000          30.781699           7.951426           3.349618\n",
       "1      010020000          66.666667           2.083333          19.047619\n",
       "2      010040101          15.546274          13.509962           8.700207\n",
       "3      010040102           21.93555          19.040939          11.767701\n",
       "4      010040201          31.624587          15.586252           7.263252\n",
       "...          ...                ...                ...                ...\n",
       "39229  974200601          57.169849          34.490463          37.954605\n",
       "39230  974210301          57.009346          37.608319          33.858268\n",
       "39231  974220901           63.18449          35.561451          33.546692\n",
       "39232  974230103          66.666667          25.080608                0.0\n",
       "39233  974240101           54.43038          37.544484          26.595745\n",
       "\n",
       "[39234 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_residence_30m2= []\n",
    "main_residence_30_40m2= []\n",
    "main_residence_40_60m2= []\n",
    "main_residence_60_80m2= []\n",
    "main_residence_80_100m2= []\n",
    "main_residence_100_120m2= []\n",
    "main_residence_120m2= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in df_stat['IRIS'] : \n",
    "    try:\n",
    "        result= requests.get(f'http://127.0.0.1:5555/api/insee/logement/distribution/{value}?by=area').json()['data']\n",
    "        for variable in result.keys():\n",
    "            try: \n",
    "                if variable == 'main_residence_30m2':\n",
    "                    main_residence_30m2.append(result[variable])\n",
    "                elif variable == 'main_residence_30_40m2':\n",
    "                    main_residence_30_40m2.append(result[variable])\n",
    "                elif variable == 'main_residence_40_60m2':\n",
    "                    main_residence_40_60m2.append(result[variable])\n",
    "                elif variable == 'main_residence_60_80m2':\n",
    "                    main_residence_60_80m2.append(result[variable])\n",
    "                elif variable == 'main_residence_80_100m2':\n",
    "                    main_residence_80_100m2.append(result[variable])\n",
    "                elif variable == 'main_residence_100_120m2':\n",
    "                    main_residence_100_120m2.append(result[variable])\n",
    "                elif variable == 'main_residence_120m2':\n",
    "                    main_residence_120m2.append(result[variable])\n",
    "            except : \n",
    "                if variable == 'main_residence_30m2':\n",
    "                    main_residence_30m2.append('not_found')\n",
    "                elif variable == 'main_residence_30_40m2':\n",
    "                    main_residence_30_40m2.append('not found')\n",
    "                elif variable == 'main_residence_40_60m2':\n",
    "                    main_residence_40_60m2.append('not_found')\n",
    "                elif variable == 'main_residence_60_80m2':\n",
    "                    main_residence_60_80m2.append('not found')\n",
    "                elif variable == 'main_residence_80_100m2':\n",
    "                    main_residence_80_100m2.append('not_found')\n",
    "                elif variable == 'main_residence_30_40m2':\n",
    "                    main_residence_100_120m2.append('not found')\n",
    "                elif variable == 'main_residence_120m2':\n",
    "                    main_residence_120m2.append('not_found')\n",
    "    except: \n",
    "        main_residence_30m2.append('not_found')\n",
    "        main_residence_30_40m2.append('not found')\n",
    "        main_residence_40_60m2.append('not_found')\n",
    "        main_residence_60_80m2.append('not_found')\n",
    "        main_residence_80_100m2.append('not found')\n",
    "        main_residence_100_120m2.append('not_found')\n",
    "        main_residence_120m2.append('not_found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39234"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat['main_residence_30m2'] = main_residence_30m2\n",
    "df_stat['main_residence_30_40m2'] = main_residence_30_40m2\n",
    "df_stat['main_residence_40_60m2'] = main_residence_40_60m2\n",
    "df_stat['main_residence_60_80m2'] = main_residence_60_80m2\n",
    "df_stat['main_residence_80_100m2'] = main_residence_80_100m2\n",
    "df_stat['main_residence_100_120m2'] = main_residence_100_120m2\n",
    "df_stat['main_residence_120m2'] = main_residence_120m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat.to_csv(\"IRIS_STATS_INSEE.csv\", sep='|', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('IRIS_STATS_INSEE.csv', sep='|', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IRIS</th>\n",
       "      <th>taux_chomage_15_24</th>\n",
       "      <th>taux_chomage_25_54</th>\n",
       "      <th>taux_chomage_55_64</th>\n",
       "      <th>main_residence_30m2</th>\n",
       "      <th>main_residence_30_40m2</th>\n",
       "      <th>main_residence_40_60m2</th>\n",
       "      <th>main_residence_60_80m2</th>\n",
       "      <th>main_residence_80_100m2</th>\n",
       "      <th>main_residence_100_120m2</th>\n",
       "      <th>main_residence_120m2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010010000</td>\n",
       "      <td>30.781699051803873</td>\n",
       "      <td>7.951425511099434</td>\n",
       "      <td>3.349618266127431</td>\n",
       "      <td>1.02007247489045</td>\n",
       "      <td>2.05513214994842</td>\n",
       "      <td>4.10277069981308</td>\n",
       "      <td>40.7963874586391</td>\n",
       "      <td>82.40487863695</td>\n",
       "      <td>75.1828077367137</td>\n",
       "      <td>105.437950843046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010020000</td>\n",
       "      <td>66.66666666666666</td>\n",
       "      <td>2.083333333333334</td>\n",
       "      <td>19.047619047619083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.02057613168724</td>\n",
       "      <td>6.12345679012344</td>\n",
       "      <td>8.16460905349792</td>\n",
       "      <td>25.514403292181</td>\n",
       "      <td>20.4115226337448</td>\n",
       "      <td>41.8436213991768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010040101</td>\n",
       "      <td>15.546273592258824</td>\n",
       "      <td>13.509962289770527</td>\n",
       "      <td>8.700207036189525</td>\n",
       "      <td>124.799171710648</td>\n",
       "      <td>6.33147370971934</td>\n",
       "      <td>148.583062229812</td>\n",
       "      <td>222.922083457259</td>\n",
       "      <td>188.383238324276</td>\n",
       "      <td>74.6817491206856</td>\n",
       "      <td>48.2199720459297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010040102</td>\n",
       "      <td>21.93554992744209</td>\n",
       "      <td>19.040939419400164</td>\n",
       "      <td>11.767700885207525</td>\n",
       "      <td>42.3121687277957</td>\n",
       "      <td>129.171132275325</td>\n",
       "      <td>424.571110077357</td>\n",
       "      <td>551.162472853823</td>\n",
       "      <td>324.676306386888</td>\n",
       "      <td>131.723968000821</td>\n",
       "      <td>113.187770351294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010040201</td>\n",
       "      <td>31.624586878064985</td>\n",
       "      <td>15.586252108322359</td>\n",
       "      <td>7.263251703606728</td>\n",
       "      <td>7.66407096898548</td>\n",
       "      <td>87.1173128733012</td>\n",
       "      <td>334.270897358721</td>\n",
       "      <td>614.685637147138</td>\n",
       "      <td>505.869952728022</td>\n",
       "      <td>225.475226068816</td>\n",
       "      <td>161.070692431894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39229</th>\n",
       "      <td>974200601</td>\n",
       "      <td>57.16984861345612</td>\n",
       "      <td>34.49046270253541</td>\n",
       "      <td>37.954605146315664</td>\n",
       "      <td>10.2287040248056</td>\n",
       "      <td>10.1553538408883</td>\n",
       "      <td>10.3159153513555</td>\n",
       "      <td>56.3360670142889</td>\n",
       "      <td>86.9350394386021</td>\n",
       "      <td>43.1854582990727</td>\n",
       "      <td>22.9316626348459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39230</th>\n",
       "      <td>974210301</td>\n",
       "      <td>57.009345794392466</td>\n",
       "      <td>37.608318890814495</td>\n",
       "      <td>33.85826771653545</td>\n",
       "      <td>18.4441329034067</td>\n",
       "      <td>32.7895696060563</td>\n",
       "      <td>64.5544651619234</td>\n",
       "      <td>221.32959484088</td>\n",
       "      <td>298.180148605075</td>\n",
       "      <td>111.689471470629</td>\n",
       "      <td>50.2090284592737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39231</th>\n",
       "      <td>974220901</td>\n",
       "      <td>63.18448958064734</td>\n",
       "      <td>35.561451466448</td>\n",
       "      <td>33.54669191100054</td>\n",
       "      <td>12.8485249019488</td>\n",
       "      <td>39.7416934267193</td>\n",
       "      <td>165.483240641782</td>\n",
       "      <td>354.406422872675</td>\n",
       "      <td>383.773584321716</td>\n",
       "      <td>239.939976565863</td>\n",
       "      <td>114.554222584866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39232</th>\n",
       "      <td>974230103</td>\n",
       "      <td>66.66666666666679</td>\n",
       "      <td>25.08060771565701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.967281765289865</td>\n",
       "      <td>1.98785681067681</td>\n",
       "      <td>1.04831286871385</td>\n",
       "      <td>0.967281765289865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39233</th>\n",
       "      <td>974240101</td>\n",
       "      <td>54.43037974683544</td>\n",
       "      <td>37.544483985765126</td>\n",
       "      <td>26.595744680851062</td>\n",
       "      <td>18.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39234 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            IRIS  taux_chomage_15_24  taux_chomage_25_54  taux_chomage_55_64  \\\n",
       "0      010010000  30.781699051803873   7.951425511099434   3.349618266127431   \n",
       "1      010020000   66.66666666666666   2.083333333333334  19.047619047619083   \n",
       "2      010040101  15.546273592258824  13.509962289770527   8.700207036189525   \n",
       "3      010040102   21.93554992744209  19.040939419400164  11.767700885207525   \n",
       "4      010040201  31.624586878064985  15.586252108322359   7.263251703606728   \n",
       "...          ...                 ...                 ...                 ...   \n",
       "39229  974200601   57.16984861345612   34.49046270253541  37.954605146315664   \n",
       "39230  974210301  57.009345794392466  37.608318890814495   33.85826771653545   \n",
       "39231  974220901   63.18448958064734     35.561451466448   33.54669191100054   \n",
       "39232  974230103   66.66666666666679   25.08060771565701                 0.0   \n",
       "39233  974240101   54.43037974683544  37.544483985765126  26.595744680851062   \n",
       "\n",
       "      main_residence_30m2 main_residence_30_40m2 main_residence_40_60m2  \\\n",
       "0        1.02007247489045       2.05513214994842       4.10277069981308   \n",
       "1                     0.0       1.02057613168724       6.12345679012344   \n",
       "2        124.799171710648       6.33147370971934       148.583062229812   \n",
       "3        42.3121687277957       129.171132275325       424.571110077357   \n",
       "4        7.66407096898548       87.1173128733012       334.270897358721   \n",
       "...                   ...                    ...                    ...   \n",
       "39229    10.2287040248056       10.1553538408883       10.3159153513555   \n",
       "39230    18.4441329034067       32.7895696060563       64.5544651619234   \n",
       "39231    12.8485249019488       39.7416934267193       165.483240641782   \n",
       "39232                 0.0                    0.0                    0.0   \n",
       "39233                18.0                   40.0                   80.0   \n",
       "\n",
       "      main_residence_60_80m2 main_residence_80_100m2 main_residence_100_120m2  \\\n",
       "0           40.7963874586391          82.40487863695         75.1828077367137   \n",
       "1           8.16460905349792         25.514403292181         20.4115226337448   \n",
       "2           222.922083457259        188.383238324276         74.6817491206856   \n",
       "3           551.162472853823        324.676306386888         131.723968000821   \n",
       "4           614.685637147138        505.869952728022         225.475226068816   \n",
       "...                      ...                     ...                      ...   \n",
       "39229       56.3360670142889        86.9350394386021         43.1854582990727   \n",
       "39230        221.32959484088        298.180148605075         111.689471470629   \n",
       "39231       354.406422872675        383.773584321716         239.939976565863   \n",
       "39232      0.967281765289865        1.98785681067681         1.04831286871385   \n",
       "39233                  154.0                   243.0                    102.0   \n",
       "\n",
       "      main_residence_120m2  \n",
       "0         105.437950843046  \n",
       "1         41.8436213991768  \n",
       "2         48.2199720459297  \n",
       "3         113.187770351294  \n",
       "4         161.070692431894  \n",
       "...                    ...  \n",
       "39229     22.9316626348459  \n",
       "39230     50.2090284592737  \n",
       "39231     114.554222584866  \n",
       "39232    0.967281765289865  \n",
       "39233                 84.0  \n",
       "\n",
       "[39234 rows x 11 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification regroupement modalité code commune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(f'http://127.0.0.1:5555/api/insee/logement/distribution/010040201?by=area').json()['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cas de tests sur aggregations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= GetData().read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).clean_columns()\n",
    "\n",
    "df= Preprocessing(df).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "\"surface_terrain\" : ((x[\"Surface terrain\"].sum()/x[\"Surface reelle bati\"].count()) if (int(x[\"Surface terrain\"].nunique()) ==1 and int(x[\"Nature culture\"].nunique()) == 1 )else x[\"Surface terrain\"].sum())\n",
    "    ,\"Nature_culture\" : x[\"Nature culture\"].max()\n",
    "    , \"su terrain 2\": x[\"Surface terrain\"].sum()\n",
    "    , \"suterrainmax\": x[\"Surface terrain\"].max()\n",
    "  #  , \"su_bat\": x[\"Surface reelle bati\"]\n",
    "    ,\"nat_terrain_unique\": x[\"Nature culture\"].nunique()\n",
    "    , \"suterrain_count\" : x[\"Surface terrain\"].count()\n",
    "    ,\"suterrain_unique\": x[\"Surface terrain\"].nunique()\n",
    "    ,\"su_bat_unique\" : x[\"Surface reelle bati\"].nunique()\n",
    "    ,\"su_bat_count\" : x[\"Surface reelle bati\"].count()\n",
    "            \n",
    "})).tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "    \"surface_reelle_bati\" : (x[\"Surface reelle bati\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if (int(x[\"Nature culture\"].nunique() > 1)) else x[\"Surface reelle bati\"].sum())\n",
    " ,\"nb_pieces_principales\" : (x[\"Nombre pieces principales\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if int(x[\"Nature culture\"].nunique()) > 1 else x[\"Nombre pieces principales\"].sum())      \n",
    "    ,\"nb_piecemax\" : x[\"Nombre pieces principales\"].max()\n",
    "    ,\"Nature_culture\" : x[\"Nature culture\"].max()\n",
    "    , \"su bat 2\": x[\"Surface reelle bati\"].sum()\n",
    "    , \"sumax\": x[\"Surface reelle bati\"].max()\n",
    "  #  , \"su_bat\": x[\"Surface reelle bati\"]\n",
    "    , \"su_count\" : x[\"Surface reelle bati\"].count()\n",
    "    ,\"nat_cul_unique\": x[\"Nature culture\"].nunique()\n",
    "    ,\"subatiment_unique\": x[\"Surface reelle bati\"].nunique()\n",
    "            \n",
    "})).tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tot= GetData().read_csv([1,3])\n",
    "\n",
    "df_tot = Preprocessing(df_tot).clean_columns()\n",
    "\n",
    "df_tot= Preprocessing(df_tot).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cas ou type local identique mais nature culture différente: \n",
    "\n",
    "df_tot[(df_tot['parcelle_cadastrale']== '01289000AC0176') | (df_tot['parcelle_cadastrale']== '013500000C1248')| (df_tot['parcelle_cadastrale']== '01195000AD0050')]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# meme de surface reelle bati \n",
    "#pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "#meme nombre pieces principales \n",
    "# différente nature culture (variable texte )\n",
    "# différente surface terrain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cas ou type local identique mais nature culture différente: \n",
    "\n",
    "df[(df['parcelle_cadastrale']== '01289000AC0176') | (df['parcelle_cadastrale']== '013500000C1248')| (df['parcelle_cadastrale']== '01195000AD0050')]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# meme de surface reelle bati \n",
    "#pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "#meme nombre pieces principales \n",
    "# différente nature culture (variable texte )\n",
    "# différente surface terrain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['parcelle_cad_section']=='01001000ZH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# même maison surface reelle bati identique \n",
    "df[(df['parcelle_cadastrale'] == '013500000C1248')]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# meêm type local \n",
    "# même surface relle bati\n",
    " #pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "# même nombre de pieces principales\n",
    "\n",
    "# différence nature culture (variable texte )\n",
    "# surface terrain différente en fonction de la parcelle cadastrale \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cas ou 2 maisons , une dépendance et un terrain :\n",
    "# repérable par section et date commune \n",
    "\n",
    "df[df.index.isin([71,72,73,74])]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# code type lcoal différent pr dépendance absent pour terrain\n",
    "# différence de surface reelle bati  (0 dépendance et nan pour terrain)\n",
    "#pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "# différence sur nombre de pieces principales\n",
    "\n",
    "# différence nature culture (variable texte )\n",
    "# surface terrain différente en fonction de la parcelle cadastrale \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "house_prediction",
   "language": "python",
   "name": "house_prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "7e331f8398536a24290f874ffd288d2ba0d3938ea5219b401a264de1a09e8807"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
