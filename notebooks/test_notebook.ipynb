{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from locale import atof, setlocale, LC_NUMERIC, LC_ALL\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "#setlocale(LC_ALL, 'fr_FR.UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GetData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class GetData:\n",
    "    \"\"\" Read data from csv and load it in a dataframe\n",
    "    accepted arguments : path to file , separator, chunksize and filter\n",
    "    option to load csv by filtering on house type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,path =\"../data/valeursfoncieres-2021.txt\",sep = \"|\", chunksize = 100000):\n",
    "        self.path = path\n",
    "        self.sep = sep\n",
    "        self.chunksize = chunksize\n",
    "\n",
    "\n",
    "    def read_csv(self, filtering_column='Code type local', filter=[1]):\n",
    "        \"\"\" pass option on which column to filter and filter value\n",
    "        if several filter value, pass the as a list\"\"\"\n",
    "        iter_csv = pd.read_csv(self.path,\n",
    "                               sep=self.sep,\n",
    "                               iterator=True,\n",
    "                               chunksize=self.chunksize,\n",
    "                               low_memory=False)\n",
    "        self.df = pd.concat([\n",
    "            chunk[chunk[filtering_column].isin(filter)] for chunk in iter_csv\n",
    "        ])\n",
    "        return self.df\n",
    "\n",
    "    def enrichissement_coordinates(self,df):\n",
    "        pass\n",
    "\n",
    "    def enrichissement_insee(self,df):\n",
    "        pass\n",
    "\n",
    "    def loading_data_db (self,df):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout l'objectif de cette étape est de tirer le maximum du dataset, en reconstituant ce qui correspond réellement à une transaction. \n",
    "Nous avons réalisé cela en regroupant chaque transaction selon 3 clés : Parcelle / date / montant car ceux ci sont répété sur les lignes communes. \n",
    "Ensuite nous avons utilisé la methode apply qui pour chaque ligne du dataset aggrégé applique une fonction lambda dans laquelle on inséré une série dont les paramétres étaient la ligne aggrée. cela nous a permis de réaliser des fonctions plus complexes au niveau des aggrégats et de couvrir les différents cas ammenant à la mutltiplication des lignes \n",
    "Les cas plus complexes à retrouver sont par exemple la nature culture basée principale en se basant sur la superficie de celle ci (--- piste https://stackoverflow.com/questions/23394476/keep-other-columns-when-doing-groupby \n",
    "L'autre cas complexe était de compter le nombre de dépendances différentes ainsi que le nombre de maisons différentes au sein d'une même aggrégation -**ajouter cas ou on a plusieurs dépendances et plusieurs terrains -- trop complexe** : \n",
    "* #dépendance - count du terme dépendance valuecount sorted[0]  / nunique type local si nunique nat culture = 1 & len value count ==2 else count terme dependance valuecount sorted[0]   if len_value count ==2 else 0\n",
    "* #maisons - count terme maison value count sorted [1] / nunqiue type local si nunique nat culture = 1 & len value count ==2 else count terme maison value count if len value_count ==2 else 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from house_prediction_package.data import GetData\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from more_itertools import chunked\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#sans doute à supprimer au lancement final du modele\n",
    "#from locale import atof, setlocale, LC_NUMERIC, LC_ALL\n",
    "\n",
    "#setlocale(LC_ALL, 'fr_FR.UTF-8')\n",
    "\n",
    "class Preprocessing :\n",
    "\n",
    "    def __init__(self,df) :\n",
    "        # self.df = get_data().read_csv()\n",
    "        self.df = df\n",
    "\n",
    "    def clean_columns(self,\n",
    "                      columns=[\n",
    "                          'Code service CH', 'Reference document',\n",
    "                          '1 Articles CGI', '2 Articles CGI', '3 Articles CGI',\n",
    "                          '4 Articles CGI', '5 Articles CGI', 'No Volume',\n",
    "                          'Identifiant local'\n",
    "                      ]):\n",
    "        \"\"\" drop useless columns\n",
    "        Customisation of columns to drop must be entered as a list\n",
    "        \"\"\"\n",
    "        # suppression of 100% empty columns - these columns are officially not completed in this db\n",
    "        self.df = self.df.drop(columns,axis=1)\n",
    "        # suppression of columns poorly completed\n",
    "        columns_to_drop = [column for column in self.df.columns if ((self.df[column].isnull().value_counts().sort_index()[0]/self.df.shape[0])*100) < 2 ]\n",
    "        self.df= self.df.drop(columns_to_drop,axis=1)\n",
    "        # replacement of , by . in numerical variables & deletion of non numrical caracters in num columns : \n",
    "        columns_num = ['Valeur fonciere', 'Surface Carrez du 1er lot', 'Nombre de lots',\n",
    "        'Surface reelle bati', 'Nombre pieces principales', 'Surface terrain']\n",
    "        # transformation des , en . pour réaliser des opérations sur les nombres et suppressions des caracteres non numériques au sein de ces colonnes \n",
    "        for column in columns_num : \n",
    "            self.df[column]=self.df[column].apply(lambda s: s.replace(\",\",\".\") if isinstance(s,str) else s)\n",
    "            self.df[column] = pd.to_numeric(self.df[column], errors = 'coerce')\n",
    "        # suppression of nan value on target variable\n",
    "        self.df= self.df.dropna(subset=['Valeur fonciere'])\n",
    "        #self.df['Surface Carrez du 1er lot'] = self.df['Surface Carrez du 1er lot'].apply(\n",
    "        #    lambda x: atof(x))        \n",
    "        # pre processing avant groupby mais attention sortir valeures foncieres avant de mettre en POO\n",
    "        ob_columns= self.df.dtypes[self.df.dtypes == 'O'].index\n",
    "        num_columns = self.df.dtypes[(self.df.dtypes == 'int')\n",
    "                                     | (self.df.dtypes == 'float')].index\n",
    "        non_num_col = ['No disposition', 'No voie', 'Code postal', 'Code commune',\n",
    "       'Prefixe de section', 'No plan','Code type local']\n",
    "        num_columns = [value for value in num_columns if value not in non_num_col]\n",
    "        for column in ob_columns :\n",
    "            self.df[column]=self.df[column].replace(np.nan,'',regex=True)\n",
    "        #à adapter in v2\n",
    "        \n",
    "        self.df[num_columns] = self.df[num_columns].apply(pd.to_numeric,\n",
    "                                                              errors='coerce')\n",
    "        \n",
    "        #drop duplicates\n",
    "        self.df = self.df.drop_duplicates().reset_index(drop= True)\n",
    "        # by returning self, we can do method chaining like preprocessing(df).clean_columns().create_identifier()\n",
    "        return self.df\n",
    "\n",
    "    def create_identifier(self) :\n",
    "        \"\"\" Create a 'unique' identifier allowing us to group several lines corresponding to a unique transaction\n",
    "        \"\"\"\n",
    "        variables_to_clean = [\n",
    "            \"Code departement\", \"Code commune\", \"Prefixe de section\",\n",
    "            \"Section\", \"No plan\"\n",
    "            ]\n",
    "        size_variables= [2,3,3,2,4]\n",
    "        for i,j in zip(variables_to_clean,size_variables):\n",
    "            chunked_data = chunked(self.df[i], 10000, strict=False)\n",
    "            values = {\"Prefixe de section\": '000'}\n",
    "            self.df= self.df.fillna(value=values)\n",
    "            if i == \"Prefixe de section\" :\n",
    "                self.df[i] = self.df[i].apply(str).apply(lambda x: x[:3])\n",
    "            new_variable = [\n",
    "                str(value).replace(\".\",\"\").zfill(j) for sublist in list(chunked_data)\n",
    "                for value in sublist\n",
    "            ]\n",
    "            self.df[f\"clean_{i.replace(' ','_').lower()}\"] = new_variable\n",
    "            self.df= self.df.drop([i],axis=1)\n",
    "        self.df[\"parcelle_cadastrale\"] = self.df[[\n",
    "            \"clean_code_departement\", \"clean_code_commune\", \"clean_prefixe_de_section\",\n",
    "            \"clean_section\", \"clean_no_plan\"]].apply(lambda x: \"\".join(x), axis=1)\n",
    "        self.df[\"parcelle_cad_section\"]=self.df[\"parcelle_cadastrale\"].str[:10]\n",
    "        self.df = self.df.drop([\n",
    "            \"clean_prefixe_de_section\", \"clean_section\", \"clean_no_plan\"\n",
    "        ], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def aggregate_transactions(self):\n",
    "        self.df = self.df.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "            \"num_voie\" : x[\"No voie\"].max()\n",
    "            ,\"B_T_Q\" : x[\"B/T/Q\"].max()\n",
    "            ,\"type_de_voie\": x[\"Type de voie\"].max()\n",
    "            ,\"voie\": x[\"Voie\"].max()\n",
    "            ,\"code_postal\": x[\"Code postal\"].max()\n",
    "            ,\"commune\": max(x[\"Commune\"])\n",
    "            ,\"clean_code_departement\": x[\"clean_code_departement\"].max()\n",
    "            ,\"clean_code_commune\": max(x[\"clean_code_commune\"])\n",
    "            ,\"surface_carrez_lot_1\" :  x[\"Surface Carrez du 1er lot\"].sum()/((x[\"Surface reelle bati\"].count()/x[\"Nature culture\"].nunique()))\n",
    "            ,\"Nb_lots\": x[\"Nombre de lots\"].max()\n",
    "            ,\"surface_terrain\" : ((x[\"Surface terrain\"].sum()/x[\"Surface reelle bati\"].count()) if (int(x[\"Surface terrain\"].nunique()) ==1 and int(x[\"Nature culture\"].nunique()) == 1 )else x[\"Surface terrain\"].sum())\n",
    "            ,\"surface_reelle_bati\" : (x[\"Surface reelle bati\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if (int(x[\"Nature culture\"].nunique() > 1)) else x[\"Surface reelle bati\"].sum())\n",
    "            ,\"nb_pieces_principales\" : (x[\"Nombre pieces principales\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if int(x[\"Nature culture\"].nunique()) > 1 else x[\"Nombre pieces principales\"].sum())      \n",
    "            ,\"dependance\" : x[\"Type local\"].unique()\n",
    "            ,\"main_type_terrain\" : x[\"Nature culture\"].max()\n",
    "            ,\"parcelle_cadastrale\": x[\"parcelle_cadastrale\"].max()}))\n",
    "        self.df = self.df.replace(np.inf, np.nan)\n",
    "        #drop rows with only dependances transactions as we focus on houses\n",
    "        self.df = self.df[self.df.dependance.apply(\n",
    "            lambda x: x.all() != \"Dépendance\")].reset_index(drop=True)\n",
    "        self.df[\"dependance\"] = self.df.dependance.apply(sorted, 1)\n",
    "        self.df[[\"Dependance\",\n",
    "                 \"Maison\"]] = pd.DataFrame(self.df.dependance.tolist(),\n",
    "                                           index=self.df.index)\n",
    "        self.df[\"Dependance\"] = [1 if value ==\"Dépendance\"else 0 for value in self.df[\"Dependance\"]]\n",
    "        self.df= self.df.drop([\"dependance\",\"Maison\"],axis =1)\n",
    "        return self.df\n",
    "\n",
    "    # to do : function calling enrichissement from data\n",
    "\n",
    "\n",
    "    def feature_generation (self):\n",
    "        # convert the 'Date' column to datetime format\n",
    "        self.df[\"month\"] = pd.to_datetime(\n",
    "            self.df[\"Date mutation\"],format=\"%d/%m/%Y\").dt.month\n",
    "        self.df= self.df.drop([\"Date mutation\"], axis = 1)\n",
    "        ## attention à ne faire qu'après avoir enrichi avec variables insee\n",
    "        dict_type_voie = dict()\n",
    "        for value in self.df[\"type_de_voie\"].value_counts()[self.df[\"type_de_voie\"].value_counts()<300 ].index.values :\n",
    "            dict_type_voie[value] = \"Autres\"\n",
    "        self.df=self.df.replace({\"type_voie\" : dict_type_voie})\n",
    "        self.df[\"type_de_voie\"]= self.df[\"type_de_voie\"].replace(np.nan,'vide')\n",
    "        return self.df\n",
    "\n",
    "    def zscore (self) :\n",
    "        # Calculate the z-score from scratch\n",
    "        #self.df['Valeur fonciere']= df['Valeur fonciere'].apply(lambda x: atof(x))\n",
    "        standard_deviation = self.df[\"Valeur fonciere\"].std(ddof=0)\n",
    "        mean_value = self.df[\"Valeur fonciere\"].mean()\n",
    "        zscores = [(value - mean_value) / standard_deviation\n",
    "                for value in self.df[\"Valeur fonciere\"]]\n",
    "        self.df[\"zscores\"]= zscores\n",
    "        # absolute value of zscore and if sup x then 1  :\n",
    "        self.df[\"outlier\"] = [\n",
    "            1 if (abs(value) > 3) else 0 for value in self.df[\"zscores\"]\n",
    "        ]\n",
    "        self.df=self.df[self.df[\"outlier\"] == 0].reset_index(drop=True)\n",
    "        self.df = self.df.drop([\"zscores\",\"outlier\"], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def split_x_y (self):\n",
    "        columns_model = [\"type_de_voie\",\n",
    "            \"clean_code_departement\",\n",
    "            \"clean_code_commune\",\n",
    "            \"code_postal\",\n",
    "            \"surface_terrain\",\n",
    "            \"surface_reelle_bati\", \"nb_pieces_principales\",\n",
    "            \"main_type_terrain\",  \"Dependance\",\n",
    "            \"month\"]\n",
    "        # Séparation des variables catégorielles et numériques\n",
    "        categorical_features = [\n",
    "            \"type_de_voie\", \"clean_code_departement\", \"clean_code_commune\",\n",
    "            \"code_postal\", \"main_type_terrain\", \"Dependance\", \"month\"\n",
    "        ]\n",
    "        numerical_features = [\n",
    "            \"surface_terrain\", \"surface_reelle_bati\", \"nb_pieces_principales\"\n",
    "        ]\n",
    "        for column in categorical_features:\n",
    "            self.df[column] = self.df[column].replace(np.nan, \"\").apply(str)\n",
    "        X = self.df[columns_model]\n",
    "        y =self.df[\"Valeur fonciere\"]\n",
    "        # selection des variables\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            test_size=0.33,\n",
    "                                                            random_state=42)\n",
    "        return self.df,categorical_features, numerical_features, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "#from house_prediction_package.preprocessing import Preprocessing\n",
    "#from house_prediction_package.data import GetData\n",
    "\n",
    "\n",
    "class Pipeline :\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        # self.categorical_features = categorical_features\n",
    "        # self.numerical_features = numerical_features\n",
    "        # self.X_train = X_train\n",
    "        # self.y_train = y_train\n",
    "        # option 2\n",
    "        #appeler les méthodes\n",
    "        self.df, self.categorical_features, self.numerical_features, self.X_train, self.X_test, self.y_train, self.y_test = Preprocessing(\n",
    "            df).feature_generation().zscore().split_x_y()\n",
    "\n",
    "    def pipeline(self):\n",
    "        # création des pipelines de pré-processing pour les variables numériques et catégorielles\n",
    "        #ajout d'un parametre pour gerer les valeures non connues dans onehotencoder - il les passe à 0(autres options disponibles)\n",
    "        numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "        categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, self.numerical_features),\n",
    "            (categorical_pipeline, self.categorical_features))\n",
    "        model = make_pipeline(preprocessor, LinearRegression())\n",
    "        fitted_model = model.fit(self.X_train, self.y_train)\n",
    "        return fitted_model, self.X_train, self.y_train,self.X_test, self.y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zone de tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model sans aggrégation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = GetData().read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test = Preprocessing(df).split_x_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())\n",
    "fitted_model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_model_wo_aggregations.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model maison avec aggrégation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= GetData().read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).clean_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= Preprocessing(df).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).aggregate_transactions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"aggregatedfile_houses.csv\", sep='|', encoding=\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).feature_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).zscore() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test =  Preprocessing(df).split_x_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())\n",
    "fitted_model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[X_test.surface_terrain== np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "X_test = X_test.replace(np.inf, np.nan)\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R(MSE): %.2f\" % math.sqrt(np.mean((test_y_hat - y_test)**2)))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score r²:  0.4806680640761509      \n",
    "Mean absolute error: 83460.59     \n",
    "Residual  of squares (MSE): 46023819610.40     \n",
    "R(MSE): 214531.63    \n",
    "R2-score: -0.02    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_model_aggregations.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test model maison/dep with aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= GetData().read_csv(filter=[1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).clean_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= Preprocessing(df).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).aggregate_transactions()\n",
    "\n",
    "df.to_csv(\"aggregatedfile_houses_dep.csv\", sep='|', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = Preprocessing(df).feature_generation()\n",
    "\n",
    "df = Preprocessing(df).zscore() \n",
    "\n",
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test  = Preprocessing(df).split_x_y()\n",
    "\n",
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R(MSE): %.2f\" % math.sqrt(np.mean((test_y_hat - y_test)**2)))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))\n",
    "\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_dep_model_aggregations.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrichissement lat long iris insee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lit le nouveau csv aggrégé qui nous a fait réduire le nombre de lignes de 1 M à 430 K \n",
    "Ensuite, on utilise l'api ban \n",
    "enrichissement 20 h trop long\n",
    "test avec csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"aggregatedfile_houses_dep.csv\", sep = \"|\", index_col=0,dtype ={\"parcelle_cad_section\":  str  \n",
    ",\"Date mutation\": object \n",
    ",\"Valeur fonciere\"  : np.float32\n",
    ",\"num_voie\" : np.float32\n",
    ",\"B_T_Q\" : object \n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"code_postal\": np.float64\n",
    ",\"commune\": object \n",
    ",\"clean_code_departement\": object \n",
    ",\"clean_code_commune\": object  \n",
    ",\"surface_carrez_lot_1\": np.float32\n",
    ",\"Nb_lots\": np.int32  \n",
    ",\"surface_terrain\":  np.float32\n",
    ",\"surface_reelle_bati\":np.float32\n",
    ",\"nb_pieces_principales\":np.float32\n",
    ",\"main_type_terrain\":object \n",
    ",\"parcelle_cadastrale\": object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DataFrame.apply() and lambda function\\n\",\n",
    "# df_adresses['No voie']= df_adresses['No voie'].astype(int)\\n\",\n",
    "# on ne peut pas passer les num voies/code postaux  en int/string sans d'abord nettoyer les nan values \\n\",\n",
    "df[\"voie\"]=df[\"voie\"].replace(\" \",\"+\")\n",
    "df[\"adresse\"] = df[[\"num_voie\", \"type_de_voie\", \"voie\"]].apply(lambda x: \"+\".join(x.astype(str)), axis=1)\n",
    "#df['clean_code_commune'] = df[[\"clean_code_departement\",\"clean_code_commune\"]].apply(lambda x: \"\".join(x.astype(str)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_code_commun =[]\n",
    "for dep,comm in zip(df[\"clean_code_departement\"],df['clean_code_commune']):\n",
    "    if len(dep) == 3: \n",
    "        full_code_commun.append(dep + comm[1:3])\n",
    "    else : \n",
    "        full_code_commun.append(dep + comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_code_commune'] = full_code_commun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"adresse\"]= df[[\"adresse\",\"clean_code_commune\"]].apply(lambda x : \"&citycode=\".join(x.astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "# reassemblage by pd.concat possible mais on s'en fiche car on va fonctionner sur des'petits df' \n",
    "#pour enrichissement puis insertion en bdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df[0]['adresse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#long=[]\n",
    "#lat= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = 'ok'\n",
    "start_time = datetime.now()\n",
    "\n",
    "for j in range(2,len(list_df)):\n",
    "    if test == 'ok':\n",
    "        for value in list_df[j]['adresse']:\n",
    "            try : \n",
    "                long.append(requests.get(f'http://localhost:7878/search?q={value}').json()['features'][0]['geometry']['coordinates'][0])\n",
    "                lat.append(requests.get(f'http://localhost:7878/search/?q={value}').json()['features'][0]['geometry']['coordinates'][1])\n",
    "            except  : \n",
    "                lat.append('not found')\n",
    "                long.append('not found')            \n",
    "    test= input(f\"iteration {j}, pour passer à l'itération {j+1} taper ok  : \")\n",
    "    f=j\n",
    "end_time = datetime.now()\n",
    "print('Duration: {} et arret à la {}'.format(end_time - start_time, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iteration 2, pour passer à l'itération 3 taper ok  : ok    \n",
    "iteration 3, pour passer à l'itération 4 taper ok  : ok    \n",
    "iteration 4, pour passer à l'itération 5 taper ok  : ok    \n",
    "iteration 5, pour passer à l'itération 6 taper ok  : ok    \n",
    "iteration 6, pour passer à l'itération 7 taper ok  : ok    \n",
    "iteration 7, pour passer à l'itération 8 taper ok  : ok    \n",
    "iteration 8, pour passer à l'itération 9 taper ok  : ok    \n",
    "iteration 9, pour passer à l'itération 10 taper ok  : ok    \n",
    "iteration 10, pour passer à l'itération 11 taper ok  : ok    \n",
    "iteration 12, pour passer à l'itération 13 taper ok  : ok    \n",
    "iteration 13, pour passer à l'itération 14 taper ok  : ko    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'lat.sav'\n",
    "pickle.dump(lat, open(filename, 'wb'))\n",
    "filename2= 'long.sav'\n",
    "pickle.dump(lat, open(filename2, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "requests.get(\"http://localhost:7878/search/?q=27.0+RUE+DES PINS&citycode=97424\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération CSV enrichi en masse et récupération IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/geolocgeocoded.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_commune\": object, \"latitude\": np.float32, \"longitude\": np.float32, \"result_score\" : np.float32 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"num_voie\",\"type_de_voie\",\"voie\",\"commune\",\"clean_code_commune\",\"latitude\", \"longitude\",\"result_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pour lancer pyris : \n",
    "1. d'abord activer lancer postgres sql \n",
    " * sudo service postgresql start\n",
    "2.  puis depuis le dossier  house pred /api / pyris : \n",
    " * gunicorn -b 127.0.0.1:5555 pyris.api.run:app \n",
    "\n",
    "Qu'est ce qu'IRIS : \"Afin de préparer la diffusion du recensement de la population de 1999, l'INSEE avait développé un découpage du territoire en mailles de taille homogène appelées IRIS2000. Un sigle qui signifiait « Ilots Regroupés pour l'Information Statistique » et qui faisait référence à la taille visée de 2 000 habitants par maille élémentaire.\" source : https://www.insee.fr/fr/metadonnees/definition/c1523    \n",
    "API fonctionne avec Postgis ( extension de postgres pour les données géospatiales).    \n",
    "Pour qu'elle fonctionne, j'ai donc télécharger les fichiers suivants : \n",
    " * contours iris \n",
    " * références iris \n",
    " * divers statistiques INSEE à l'échelon IRIS \n",
    "     * statistiques activités -\n",
    "     * statistiques démographiques - ménages et evol pop \n",
    "     * statistiques scolaires\n",
    "     * statistiques logements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['coordinates'] = df[[\"latitude\",\"longitude\"]].apply(lambda x : \"&lon=\".join(x.astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api url \n",
    "url = \"http://127.0.0.1:5555/api/coords?geojson=false&lat=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50000  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "# reassemblage by pd.concat possible mais on s'en fiche car on va fonctionner sur des'petits df' \n",
    "#pour enrichissement puis insertion en bdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'ok'\n",
    "start_time = datetime.now()\n",
    "\n",
    "for j in range(0,len(list_df)):\n",
    "    if test == 'ok':\n",
    "        for value in list_df[j]['coordinates']:\n",
    "            try : \n",
    "                IRIS.append(requests.get(f'{url}{value}').json()['complete_code'])\n",
    "            except  : \n",
    "                IRIS.append('not found')\n",
    "    test= input(f\"iteration {j}, pour passer à l'itération {j+1} taper ok  : \")\n",
    "    f=j\n",
    "end_time = datetime.now()\n",
    "print('Duration: {} et arret à la {}'.format(end_time - start_time, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS= pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IRIS']= IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[df['IRIS'] =='not found']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recherche par ville sur les valeures non trouvées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in df[df['IRIS'] =='not found'].iterrows() : \n",
    "    print('######',index,'*****', value['clean_code_commune'])\n",
    "    try :\n",
    "        df.at[index,'IRIS'] = requests.get(f\"http://127.0.0.1:5555/api/city/code/{value['clean_code_commune']}\").json()[0]\n",
    "    except : \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche api open data soft \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les dernieres valeures non trouvées appartiennent aux dom tom Martinique Guadeloupe Guyanne et Réunion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://data.opendatasoft.com/api/records/1.0/search/?dataset=iris-millesime-france%40lareunion&q=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/api/records/1.0/search/?dataset=iris-millesime-france&q=97101&sort=year&facet=com_arm_name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appel d'un api externe pour la réunion : \n",
    "** ajouter un prefiltre dans le code **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://data.opendatasoft.com/explore/dataset/iris-millesime-france%40lareunion/api/?disjunctive.reg_name&disjunctive.dep_name&disjunctive.arrdep_name&disjunctive.ze2020_name&disjunctive.bv2012_name&disjunctive.epci_name&disjunctive.ept_name&disjunctive.com_name&disjunctive.com_arm_name&disjunctive.iris_name&sort=year&q=97424&geofilter.polygon=&geofilter.distance="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in df[df['IRIS'] =='not found'].iterrows() : \n",
    "    print('######',index,'*****', value['clean_code_commune'])\n",
    "    try :\n",
    "        df.at[index,'IRIS'] = requests.get(f\"{url}{value['clean_code_commune']}&sort=year&facet=com_arm_name\").json()['records'][0]['fields']['iris_code']\n",
    "    except : \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appel d'un api externe pour la guadeloupe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://regionguadeloupe.opendatasoft.com/explore/dataset/iris-millesime-france/information/?disjunctive.reg_name&disjunctive.dep_name&disjunctive.arrdep_name&disjunctive.ze2020_name&disjunctive.bv2012_name&disjunctive.epci_name&disjunctive.ept_name&disjunctive.com_name&disjunctive.com_arm_name&disjunctive.iris_name&sort=year&q=97101&dataChart=eyJxdWVyaWVzIjpbeyJjb25maWciOnsiZGF0YXNldCI6ImlyaXMtbWlsbGVzaW1lLWZyYW5jZSIsIm9wdGlvbnMiOnsiZGlzanVuY3RpdmUucmVnX25hbWUiOnRydWUsImRpc2p1bmN0aXZlLmRlcF9uYW1lIjp0cnVlLCJkaXNqdW5jdGl2ZS5hcnJkZXBfbmFtZSI6dHJ1ZSwiZGlzanVuY3RpdmUuemUyMDIwX25hbWUiOnRydWUsImRpc2p1bmN0aXZlLmJ2MjAxMl9uYW1lIjp0cnVlLCJkaXNqdW5jdGl2ZS5lcGNpX25hbWUiOnRydWUsImRpc2p1bmN0aXZlLmVwdF9uYW1lIjp0cnVlLCJkaXNqdW5jdGl2ZS5jb21fbmFtZSI6dHJ1ZSwiZGlzanVuY3RpdmUuY29tX2FybV9uYW1lIjp0cnVlLCJkaXNqdW5jdGl2ZS5pcmlzX25hbWUiOnRydWUsInNvcnQiOiJ5ZWFyIiwicSI6Im1hcmllIGdhbGFudGUgZ3JhbmQgYm91cmciLCJyZWZpbmUuemUyMDIwX25hbWUiOiJNYXJpZS1HYWxhbnRlIiwicmVmaW5lLmNvbV9uYW1lIjoiR3JhbmQtQm91cmcifX0sImNoYXJ0cyI6W3siYWxpZ25Nb250aCI6dHJ1ZSwidHlwZSI6ImxpbmUiLCJmdW5jIjoiQ09VTlQiLCJzY2llbnRpZmljRGlzcGxheSI6dHJ1ZSwiY29sb3IiOiIjRUQ5QTlBIn1dLCJ4QXhpcyI6InllYXIiLCJtYXhwb2ludHMiOiIiLCJ0aW1lc2NhbGUiOiJ5ZWFyIiwic29ydCI6IiJ9XSwiZGlzcGxheUxlZ2VuZCI6dHJ1ZSwiYWxpZ25Nb250aCI6dHJ1ZX0%3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://regionguadeloupe.opendatasoft.com/api/records/1.0/search/?dataset=iris-millesime-france&q=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, value in df[df['IRIS'] =='not found'].iterrows() : \n",
    "    print('######',index,'*****', value['clean_code_commune'])\n",
    "    if value['clean_code_commune'][0:3] == '971' :\n",
    "        try :\n",
    "            df.at[index,'IRIS'] = requests.get(f\"{url}{value['clean_code_commune']}&sort=year&facet=com_arm_name\").json()['records'][0]['fields']['iris_code']\n",
    "        except : \n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appel d'un autre api pour martinique et guyanne \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://public.opendatasoft.com/explore/dataset/georef-france-iris/api/?disjunctive.reg_name&disjunctive.dep_name&disjunctive.arrdep_name&disjunctive.ze2020_name&disjunctive.bv2012_name&disjunctive.epci_name&disjunctive.ept_name&disjunctive.com_name&disjunctive.com_arm_name&disjunctive.iris_name&sort=year&q=97201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://public.opendatasoft.com/api/records/1.0/search/?dataset=georef-france-iris&q=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, value in df[df['IRIS'] =='not found'].iterrows() : \n",
    "    print('######',index,'*****', value['clean_code_commune'])\n",
    "    try :\n",
    "        df.at[index,'IRIS'] = requests.get(f\"{url}{value['clean_code_commune']}&sort=year&facet=com_arm_name\").json()['records'][0]['fields']['iris_code']\n",
    "    except : \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reconstitution df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init = pd.read_csv('../data/geolocgeocoded.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_commune\": object, \"latitude\": np.float32, \"longitude\": np.float32, \"result_score\" : np.float32 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init= df_init[[\"num_voie\",\"type_de_voie\",\"voie\",\"commune\",\"clean_code_commune\",\"latitude\", \"longitude\",\"result_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour code final garder plutot l'id de résultat qui correspond à lat et long trouvé et dédoublonner selon cet id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init['coordinates'] = df_init[[\"latitude\",\"longitude\"]].apply(lambda x : \"&lon=\".join(x.astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init = df_init.merge(df[['coordinates','IRIS']], left_on='coordinates', right_on='coordinates',\n",
    "          suffixes=('_left', '_right'),  how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init.to_csv(\"aggregatedfile_houses_dep_WITH_IRIS.csv\", sep='|', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrichissement variables INSEE sur CODES IRIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('aggregatedfile_houses_dep_WITH_IRIS.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_commune\": object, \"latitude\": np.float32, \"longitude\": np.float32, \"result_score\" : np.float32, \n",
    "'IRIS': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"http://127.0.0.1:5555/api/insee/activite/distribution/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inititing lists : \n",
    "actif_15_24 = []\n",
    "actif_25_54 = []\n",
    "actif_55_64 = []\n",
    "chomage_15_24 = []\n",
    "chomage_25_54 = []\n",
    "chomage_55_64 = []\n",
    "taux_chomage_15_24 = []\n",
    "taux_chomage_25_54 = []\n",
    "taux_chomage_55_64 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in df.IRIS.unique() : \n",
    "    try:\n",
    "        result = requests.get(f'http://127.0.0.1:5555/api/insee/activite/distribution/{value}?by=age').json()['data']\n",
    "        for variable in result.keys():\n",
    "            try: \n",
    "                if variable == 'taux_chomage_15_24':\n",
    "                    taux_chomage_15_24.append(result[variable])\n",
    "                elif variable == 'taux_chomage_25_54':\n",
    "                    taux_chomage_25_54.append(result[variable])\n",
    "                elif variable == 'taux_chomage_55_64':\n",
    "                    taux_chomage_55_64.append(result[variable])\n",
    "            except : \n",
    "                if variable == 'taux_chomage_15_24':\n",
    "                    taux_chomage_15_24.append('not_found')\n",
    "                elif variable == 'taux_chomage_25_54':\n",
    "                    taux_chomage_25_54.append('not found')\n",
    "                elif variable == 'taux_chomage_55_64':\n",
    "                    taux_chomage_55_64.append('not_found')\n",
    "    except: \n",
    "        taux_chomage_15_24.append('not_found')\n",
    "        taux_chomage_25_54.append('not found')\n",
    "        taux_chomage_55_64.append('not_found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requests.get(f'http://127.0.0.1:5555/api/insee/activite/distribution/010010000?by=age').json()['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stat=pd.DataFrame({'IRIS': df.IRIS.unique(),'taux_chomage_15_24':taux_chomage_15_24,\"taux_chomage_25_54\":taux_chomage_25_54,\"taux_chomage_55_64\":taux_chomage_55_64 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_residence_30m2= []\n",
    "main_residence_30_40m2= []\n",
    "main_residence_40_60m2= []\n",
    "main_residence_60_80m2= []\n",
    "main_residence_80_100m2= []\n",
    "main_residence_100_120m2= []\n",
    "main_residence_120m2= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in df_stat['IRIS'] : \n",
    "    try:\n",
    "        result= requests.get(f'http://127.0.0.1:5555/api/insee/logement/distribution/{value}?by=area').json()['data']\n",
    "        for variable in result.keys():\n",
    "            try: \n",
    "                if variable == 'main_residence_30m2':\n",
    "                    main_residence_30m2.append(result[variable])\n",
    "                elif variable == 'main_residence_30_40m2':\n",
    "                    main_residence_30_40m2.append(result[variable])\n",
    "                elif variable == 'main_residence_40_60m2':\n",
    "                    main_residence_40_60m2.append(result[variable])\n",
    "                elif variable == 'main_residence_60_80m2':\n",
    "                    main_residence_60_80m2.append(result[variable])\n",
    "                elif variable == 'main_residence_80_100m2':\n",
    "                    main_residence_80_100m2.append(result[variable])\n",
    "                elif variable == 'main_residence_100_120m2':\n",
    "                    main_residence_100_120m2.append(result[variable])\n",
    "                elif variable == 'main_residence_120m2':\n",
    "                    main_residence_120m2.append(result[variable])\n",
    "            except : \n",
    "                if variable == 'main_residence_30m2':\n",
    "                    main_residence_30m2.append('not_found')\n",
    "                elif variable == 'main_residence_30_40m2':\n",
    "                    main_residence_30_40m2.append('not found')\n",
    "                elif variable == 'main_residence_40_60m2':\n",
    "                    main_residence_40_60m2.append('not_found')\n",
    "                elif variable == 'main_residence_60_80m2':\n",
    "                    main_residence_60_80m2.append('not found')\n",
    "                elif variable == 'main_residence_80_100m2':\n",
    "                    main_residence_80_100m2.append('not_found')\n",
    "                elif variable == 'main_residence_30_40m2':\n",
    "                    main_residence_100_120m2.append('not found')\n",
    "                elif variable == 'main_residence_120m2':\n",
    "                    main_residence_120m2.append('not_found')\n",
    "    except: \n",
    "        main_residence_30m2.append('not_found')\n",
    "        main_residence_30_40m2.append('not found')\n",
    "        main_residence_40_60m2.append('not_found')\n",
    "        main_residence_60_80m2.append('not_found')\n",
    "        main_residence_80_100m2.append('not found')\n",
    "        main_residence_100_120m2.append('not_found')\n",
    "        main_residence_120m2.append('not_found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat['main_residence_30m2'] = main_residence_30m2\n",
    "df_stat['main_residence_30_40m2'] = main_residence_30_40m2\n",
    "df_stat['main_residence_40_60m2'] = main_residence_40_60m2\n",
    "df_stat['main_residence_60_80m2'] = main_residence_60_80m2\n",
    "df_stat['main_residence_80_100m2'] = main_residence_80_100m2\n",
    "df_stat['main_residence_100_120m2'] = main_residence_100_120m2\n",
    "df_stat['main_residence_120m2'] = main_residence_120m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat.to_csv(\"IRIS_STATS_INSEE.csv\", sep='|', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat= pd.read_csv('IRIS_STATS_INSEE.csv', sep='|', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrichissement logement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logement= []\n",
    "main_residence= []\n",
    "second_residence= []\n",
    "unoccupied= []\n",
    "house= []\n",
    "appartment= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(f'http://127.0.0.1:5555/api/insee/logement/010040201').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in df_stat['IRIS']: \n",
    "    try:\n",
    "        result= requests.get(f'http://127.0.0.1:5555/api/insee/logement/{value}').json()\n",
    "        for variable in result.keys():\n",
    "            try: \n",
    "                if variable == 'logement':\n",
    "                    logement.append(result[variable])\n",
    "                elif variable == 'main_residence':\n",
    "                    main_residence.append(result[variable])\n",
    "                elif variable == 'second_residence':\n",
    "                    second_residence.append(result[variable])\n",
    "                elif variable == 'unoccupied':\n",
    "                    unoccupied.append(result[variable])\n",
    "                elif variable == 'house':\n",
    "                    house.append(result[variable])\n",
    "                elif variable == 'appartment':\n",
    "                    appartment.append(result[variable])\n",
    "            except : \n",
    "                if variable == 'logement':\n",
    "                    logement.append('not_found')\n",
    "                elif variable == 'main_residence':\n",
    "                    main_residence.append('not found')\n",
    "                elif variable == 'second_residence':\n",
    "                    second_residence.append('not_found')\n",
    "                elif variable == 'unoccupied':\n",
    "                    unoccupied.append('not found')\n",
    "                elif variable == 'house':\n",
    "                    house.append('not_found')\n",
    "                elif variable == 'appartment':\n",
    "                    appartment.append('not found')\n",
    "    except: \n",
    "        logement.append('not_found')\n",
    "        main_residence.append('not found')\n",
    "        second_residence.append('not_found')\n",
    "        unoccupied.append('not_found')\n",
    "        house.append('not found')\n",
    "        appartment.append('not_found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IRIS'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[~df['IRIS'].isin(df_stat['IRIS'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liste des bases de données niveau IRIS \n",
    "https://www.insee.fr/fr/information/2383389 :\n",
    "\n",
    "\n",
    "* spécial Filosofi : https://www.insee.fr/fr/statistiques/6049648#dictionnaire\n",
    "\n",
    "* https://www.insee.fr/fr/statistiques/5650714#consulter\n",
    "\n",
    "* https://www.insee.fr/fr/statistiques/5650712#dictionnaire\n",
    "\n",
    "* https://www.insee.fr/fr/statistiques/5650708#dictionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyse data logement - variables à garder :\n",
    "https://www.insee.fr/fr/statistiques/5650749#dictionnaire\n",
    " 92 variables - variables suivantes gardées : \n",
    " - variables inter logements - nbre de logements,logements vacants nbre type apparte, nbre hlm, nbre maisons, nbre residences princ (je n'ai pas pris le nombre de résidence principale de type maison ou appartements redondant avec données déjà prise : nbre maison, nbre appartements)\n",
    " - variables intra logements - nbre pieces, superficies, garage....\n",
    " - variables ménages - residence par propio /locataires , emménagement 2 ans , 4 ans e\n",
    " \n",
    "choix de ne pas prendre les variables sur période de fabrication des logements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat= pd.read_csv('../data/base-ic-logement-2018.CSV',sep=';', dtype= {\n",
    "\"IRIS\" : object\n",
    ",\"COM\": object \n",
    ",\"TYP_IRIS\":  object \n",
    ",\"MODIF_IRIS\": object \n",
    ",\"LAB_IRIS\": object\n",
    ",'P18_RP_ELEC': np.float32, 'P18_RP_EAUCH':np.float32, 'P18_RP_BDWC':np.float32, 'P18_RP_CHOS':np.float32,\n",
    "       'P18_RP_CLIM':np.float32, 'P18_RP_TTEGOU':np.float32, 'P18_RP_GARL':np.float32, 'P18_RP_VOIT1P':np.float32,\n",
    "       'P18_RP_VOIT1':np.float32, 'P18_RP_VOIT2P':np.float32, 'P18_RP_HABFOR':np.float32, 'P18_RP_CASE':np.float32,\n",
    "       'P18_RP_MIBOIS':np.float32,'P18_RP_MIDUR':np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stat[['IRIS']].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat=df_stat[['IRIS', 'LAB_IRIS','P18_LOG', 'P18_RP', 'P18_RSECOCC', 'P18_LOGVAC', 'P18_MAISON', 'P18_APPART','P18_RP_1P', \n",
    "         'P18_RP_2P', 'P18_RP_3P', 'P18_RP_4P', 'P18_RP_5PP','P18_RP_M30M2', 'P18_RP_3040M2', 'P18_RP_4060M2',\n",
    "       'P18_RP_6080M2', 'P18_RP_80100M2', 'P18_RP_100120M2', 'P18_RP_120M2P','P18_RP_GARL','P18_RP_PROP', \n",
    "         'P18_RP_LOC', 'P18_RP_LOCHLMV','P18_RP_GRAT','P18_MEN_ANEM0002', 'P18_MEN_ANEM0204',\n",
    "       'P18_MEN_ANEM0509', 'P18_MEN_ANEM10P']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat[['P18_LOG','P18_RP','P18_RP_GARL','P18_RP_PROP', 'P18_RP_GRAT',\n",
    "         'P18_RP_LOC', 'P18_RP_LOCHLMV','P18_MEN_ANEM0002', 'P18_MEN_ANEM0204',\n",
    "       'P18_MEN_ANEM0509', 'P18_MEN_ANEM10P']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_featuring (df) : \n",
    "    df[\"Taux_RP\"] = df[\"P18_RP\"]/df[\"P18_LOG\"]\n",
    "    df[\"Taux_LV\"] = df[\"P18_LOGVAC\"]/df[\"P18_LOG\"]\n",
    "    df[\"Taux_MAI\"] = df[\"P18_MAISON\"]/df[\"P18_LOG\"]\n",
    "    # APPARTEMENT PAS UTILE CAR SOIT MAISON SOIT APPARTEMENT\n",
    "    #nb pieces\n",
    "    df[\"Taux_RP_1P\"] = df[\"P18_RP_1P\"]/df[\"P18_RP\"]\n",
    "    df[\"Taux_RP_2P\"] = df[\"P18_RP_2P\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_3P\"] = df[\"P18_RP_3P\"]/df[\"P18_RP\"]    \n",
    "    df[\"Taux_RP_4P\"] = df[\"P18_RP_4P\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_5P\"] = df[\"P18_RP_5PP\"]/df[\"P18_RP\"] \n",
    "    #superficie\n",
    "    df[\"Taux_RP_30\"] = df[\"P18_RP_M30M2\"]/df[\"P18_RP\"]\n",
    "    df[\"Taux_RP_40\"] = df[\"P18_RP_3040M2\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_60\"] = df[\"P18_RP_4060M2\"]/df[\"P18_RP\"]    \n",
    "    df[\"Taux_RP_80\"] = df[\"P18_RP_6080M2\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_100\"] = df[\"P18_RP_80100M2\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_120\"] = df[\"P18_RP_100120M2\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_P120\"] = df[\"P18_RP_120M2P\"]/df[\"P18_RP\"] \n",
    "    #occupation\n",
    "    df[\"Taux_RP_GAR\"] = df[\"P18_RP_GARL\"]/df[\"P18_RP\"]\n",
    "    df[\"Taux_RP_PROPRIO\"] = df[\"P18_RP_PROP\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_GRATUIT\"] = df[\"P18_RP_GRAT\"]/df[\"P18_RP\"]    \n",
    "    df[\"Taux_RP_LOC\"] = df[\"P18_RP_LOC\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_HML\"] = df[\"P18_RP_LOCHLMV\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_AM02\"] = df[\"P18_MEN_ANEM0002\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_AM04\"] = df[\"P18_MEN_ANEM0204\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_AM09\"] = df[\"P18_MEN_ANEM0509\"]/df[\"P18_RP\"] \n",
    "    df[\"Taux_RP_AM09P\"] = df[\"P18_MEN_ANEM10P\"]/df[\"P18_RP\"]  \n",
    "    df =df.drop(['P18_LOG', 'P18_RP', 'P18_RSECOCC', 'P18_LOGVAC', 'P18_MAISON', 'P18_APPART','P18_RP_1P', \n",
    "         'P18_RP_2P', 'P18_RP_3P', 'P18_RP_4P', 'P18_RP_5PP','P18_RP_M30M2', 'P18_RP_3040M2', 'P18_RP_4060M2',\n",
    "       'P18_RP_6080M2', 'P18_RP_80100M2', 'P18_RP_100120M2', 'P18_RP_120M2P','P18_RP_GARL','P18_RP_PROP', \n",
    "         'P18_RP_LOC', 'P18_RP_LOCHLMV','P18_RP_GRAT','P18_MEN_ANEM0002', 'P18_MEN_ANEM0204',\n",
    "       'P18_MEN_ANEM0509', 'P18_MEN_ANEM10P'], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dans mon code générique pour l'API ne pas supprimer les lignes mais tout mettre en bdd \n",
    "\n",
    "la suppression des lignes est pour l'apprentissage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = columns_featuring(df_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = df_stat[df_stat['IRIS'].isin(df['IRIS'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df avec stat sur logement mais pas iris\n",
    "df_ini = pd.read_csv('aggregatedfile_houses_dep.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_departement\" : object\n",
    ",\"clean_code_commune\": object, \"latitude\": np.float32, \"longitude\": np.float32, \"result_score\" : np.float32 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df avec iris mais pas de stat sur les logements\n",
    "df= pd.read_csv('aggregatedfile_houses_dep_WITH_IRIS.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_commune\": object, \"latitude\": np.float32, \"longitude\": np.float32, \"result_score\" : np.float32, \n",
    "'IRIS': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ini.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['IRIS']].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test avant concat pour voir si ordre lignes préservé\n",
    "df['Diff'] = np.where( df['voie_a'] == df['voie'] , '1', '0')\n",
    "#df['Diff'].value_counts()\n",
    "df[df['Diff'] == '0'][['voie','voie_a']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df[['IRIS']],df_ini],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enrichissement df avec infos logements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_stat, left_on='IRIS', right_on='IRIS',\n",
    "          suffixes=('_left', '_right'),  how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test model avec enrichissement logements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from house_prediction_package.data import GetData\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from more_itertools import chunked\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#sans doute à supprimer au lancement final du modele\n",
    "#from locale import atof, setlocale, LC_NUMERIC, LC_ALL\n",
    "\n",
    "#setlocale(LC_ALL, 'fr_FR.UTF-8')\n",
    "\n",
    "class Preprocessing :\n",
    "\n",
    "    def __init__(self,df) :\n",
    "        # self.df = get_data().read_csv()\n",
    "        self.df = df\n",
    "\n",
    "    def clean_columns(self,\n",
    "                      columns=[\n",
    "                          'Code service CH', 'Reference document',\n",
    "                          '1 Articles CGI', '2 Articles CGI', '3 Articles CGI',\n",
    "                          '4 Articles CGI', '5 Articles CGI', 'No Volume',\n",
    "                          'Identifiant local'\n",
    "                      ]):\n",
    "        \"\"\" drop useless columns\n",
    "        Customisation of columns to drop must be entered as a list\n",
    "        \"\"\"\n",
    "        # suppression of 100% empty columns - these columns are officially not completed in this db\n",
    "        self.df = self.df.drop(columns,axis=1)\n",
    "        # suppression of columns poorly completed\n",
    "        columns_to_drop = [column for column in self.df.columns if ((self.df[column].isnull().value_counts().sort_index()[0]/self.df.shape[0])*100) < 2 ]\n",
    "        self.df= self.df.drop(columns_to_drop,axis=1)\n",
    "        # replacement of , by . in numerical variables & deletion of non numrical caracters in num columns : \n",
    "        columns_num = ['Valeur fonciere', 'Surface Carrez du 1er lot', 'Nombre de lots',\n",
    "        'Surface reelle bati', 'Nombre pieces principales', 'Surface terrain']\n",
    "        # transformation des , en . pour réaliser des opérations sur les nombres et suppressions des caracteres non numériques au sein de ces colonnes \n",
    "        for column in columns_num : \n",
    "            self.df[column]=self.df[column].apply(lambda s: s.replace(\",\",\".\") if isinstance(s,str) else s)\n",
    "            self.df[column] = pd.to_numeric(self.df[column], errors = 'coerce')\n",
    "        # suppression of nan value on target variable\n",
    "        self.df= self.df.dropna(subset=['Valeur fonciere'])\n",
    "        #self.df['Surface Carrez du 1er lot'] = self.df['Surface Carrez du 1er lot'].apply(\n",
    "        #    lambda x: atof(x))        \n",
    "        # pre processing avant groupby mais attention sortir valeures foncieres avant de mettre en POO\n",
    "        ob_columns= self.df.dtypes[self.df.dtypes == 'O'].index\n",
    "        num_columns = self.df.dtypes[(self.df.dtypes == 'int')\n",
    "                                     | (self.df.dtypes == 'float')].index\n",
    "        non_num_col = ['No disposition', 'No voie', 'Code postal', 'Code commune',\n",
    "       'Prefixe de section', 'No plan','Code type local']\n",
    "        num_columns = [value for value in num_columns if value not in non_num_col]\n",
    "        for column in ob_columns :\n",
    "            self.df[column]=self.df[column].replace(np.nan,'',regex=True)\n",
    "        #à adapter in v2\n",
    "        \n",
    "        self.df[num_columns] = self.df[num_columns].apply(pd.to_numeric,\n",
    "                                                              errors='coerce')\n",
    "        \n",
    "        #drop duplicates\n",
    "        self.df = self.df.drop_duplicates().reset_index(drop= True)\n",
    "        # by returning self, we can do method chaining like preprocessing(df).clean_columns().create_identifier()\n",
    "        return self.df\n",
    "\n",
    "    def create_identifier(self) :\n",
    "        \"\"\" Create a 'unique' identifier allowing us to group several lines corresponding to a unique transaction\n",
    "        \"\"\"\n",
    "        variables_to_clean = [\n",
    "            \"Code departement\", \"Code commune\", \"Prefixe de section\",\n",
    "            \"Section\", \"No plan\"\n",
    "            ]\n",
    "        size_variables= [2,3,3,2,4]\n",
    "        for i,j in zip(variables_to_clean,size_variables):\n",
    "            chunked_data = chunked(self.df[i], 10000, strict=False)\n",
    "            values = {\"Prefixe de section\": '000'}\n",
    "            self.df= self.df.fillna(value=values)\n",
    "            if i == \"Prefixe de section\" :\n",
    "                self.df[i] = self.df[i].apply(str).apply(lambda x: x[:3])\n",
    "            new_variable = [\n",
    "                str(value).replace(\".\",\"\").zfill(j) for sublist in list(chunked_data)\n",
    "                for value in sublist\n",
    "            ]\n",
    "            self.df[f\"clean_{i.replace(' ','_').lower()}\"] = new_variable\n",
    "            self.df= self.df.drop([i],axis=1)\n",
    "        self.df[\"parcelle_cadastrale\"] = self.df[[\n",
    "            \"clean_code_departement\", \"clean_code_commune\", \"clean_prefixe_de_section\",\n",
    "            \"clean_section\", \"clean_no_plan\"]].apply(lambda x: \"\".join(x), axis=1)\n",
    "        self.df[\"parcelle_cad_section\"]=self.df[\"parcelle_cadastrale\"].str[:10]\n",
    "        self.df = self.df.drop([\n",
    "            \"clean_prefixe_de_section\", \"clean_section\", \"clean_no_plan\"\n",
    "        ], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def aggregate_transactions(self):\n",
    "        self.df = self.df.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "            \"num_voie\" : x[\"No voie\"].max()\n",
    "            ,\"B_T_Q\" : x[\"B/T/Q\"].max()\n",
    "            ,\"type_de_voie\": x[\"Type de voie\"].max()\n",
    "            ,\"voie\": x[\"Voie\"].max()\n",
    "            ,\"code_postal\": x[\"Code postal\"].max()\n",
    "            ,\"commune\": max(x[\"Commune\"])\n",
    "            ,\"clean_code_departement\": x[\"clean_code_departement\"].max()\n",
    "            ,\"clean_code_commune\": max(x[\"clean_code_commune\"])\n",
    "            ,\"surface_carrez_lot_1\" :  x[\"Surface Carrez du 1er lot\"].sum()/((x[\"Surface reelle bati\"].count()/x[\"Nature culture\"].nunique()))\n",
    "            ,\"Nb_lots\": x[\"Nombre de lots\"].max()\n",
    "            ,\"surface_terrain\" : ((x[\"Surface terrain\"].sum()/x[\"Surface reelle bati\"].count()) if (int(x[\"Surface terrain\"].nunique()) ==1 and int(x[\"Nature culture\"].nunique()) == 1 )else x[\"Surface terrain\"].sum())\n",
    "            ,\"surface_reelle_bati\" : (x[\"Surface reelle bati\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if (int(x[\"Nature culture\"].nunique() > 1)) else x[\"Surface reelle bati\"].sum())\n",
    "            ,\"nb_pieces_principales\" : (x[\"Nombre pieces principales\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if int(x[\"Nature culture\"].nunique()) > 1 else x[\"Nombre pieces principales\"].sum())      \n",
    "            ,\"dependance\" : x[\"Type local\"].unique()\n",
    "            ,\"main_type_terrain\" : x[\"Nature culture\"].max()\n",
    "            ,\"parcelle_cadastrale\": x[\"parcelle_cadastrale\"].max()}))\n",
    "        self.df = self.df.replace(np.inf, np.nan)\n",
    "        #drop rows with only dependances transactions as we focus on houses\n",
    "        self.df = self.df[self.df.dependance.apply(\n",
    "            lambda x: x.all() != \"Dépendance\")].reset_index(drop=True)\n",
    "        self.df[\"dependance\"] = self.df.dependance.apply(sorted, 1)\n",
    "        self.df[[\"Dependance\",\n",
    "                 \"Maison\"]] = pd.DataFrame(self.df.dependance.tolist(),\n",
    "                                           index=self.df.index)\n",
    "        self.df[\"Dependance\"] = [1 if value ==\"Dépendance\"else 0 for value in self.df[\"Dependance\"]]\n",
    "        self.df= self.df.drop([\"dependance\",\"Maison\"],axis =1)\n",
    "        return self.df\n",
    "\n",
    "    # to do : function calling enrichissement from data\n",
    "\n",
    "\n",
    "    def feature_generation (self):\n",
    "        # convert the 'Date' column to datetime format\n",
    "        self.df[\"month\"] = pd.to_datetime(\n",
    "            self.df[\"Date mutation\"],format=\"%d/%m/%Y\").dt.month\n",
    "        self.df= self.df.drop([\"Date mutation\"], axis = 1)\n",
    "        ## attention à ne faire qu'après avoir enrichi avec variables insee\n",
    "        dict_type_voie = dict()\n",
    "        for value in self.df[\"type_de_voie\"].value_counts()[self.df[\"type_de_voie\"].value_counts()<300 ].index.values :\n",
    "            dict_type_voie[value] = \"Autres\"\n",
    "        self.df=self.df.replace({\"type_voie\" : dict_type_voie})\n",
    "        self.df[\"type_de_voie\"]= self.df[\"type_de_voie\"].replace(np.nan,'vide')\n",
    "        return self.df\n",
    "\n",
    "    def zscore (self) :\n",
    "        # Calculate the z-score from scratch\n",
    "        #self.df['Valeur fonciere']= df['Valeur fonciere'].apply(lambda x: atof(x))\n",
    "        standard_deviation = self.df[\"Valeur fonciere\"].std(ddof=0)\n",
    "        mean_value = self.df[\"Valeur fonciere\"].mean()\n",
    "        zscores = [(value - mean_value) / standard_deviation\n",
    "                for value in self.df[\"Valeur fonciere\"]]\n",
    "        self.df[\"zscores\"]= zscores\n",
    "        # absolute value of zscore and if sup x then 1  :\n",
    "        self.df[\"outlier\"] = [\n",
    "            1 if (abs(value) > 0.25) else 0 for value in self.df[\"zscores\"]\n",
    "        ]\n",
    "        self.df=self.df[self.df[\"outlier\"] == 0].reset_index(drop=True)\n",
    "        self.df = self.df.drop([\"zscores\",\"outlier\"], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def split_x_y (self):\n",
    "        columns_model = [\"type_de_voie\",\n",
    "            \"clean_code_departement\",\n",
    "            \"clean_code_commune\",\n",
    "            \"code_postal\",\n",
    "            \"surface_terrain\",\n",
    "            \"surface_reelle_bati\", \"nb_pieces_principales\",\n",
    "            \"main_type_terrain\",  \"Dependance\",'Taux_RP', 'Taux_LV', 'Taux_MAI',\n",
    "       'Taux_RP_1P', 'Taux_RP_2P', 'Taux_RP_3P', 'Taux_RP_4P', 'Taux_RP_5P',\n",
    "       'Taux_RP_30', 'Taux_RP_40', 'Taux_RP_60', 'Taux_RP_80', 'Taux_RP_100',\n",
    "       'Taux_RP_120', 'Taux_RP_P120', 'Taux_RP_GAR', 'Taux_RP_PROPRIO',\n",
    "       'Taux_RP_GRATUIT', 'Taux_RP_LOC', 'Taux_RP_HML', 'Taux_RP_AM02',\n",
    "       'Taux_RP_AM04', 'Taux_RP_AM09', 'Taux_RP_AM09P',\n",
    "            \"month\"]\n",
    "        # Séparation des variables catégorielles et numériques\n",
    "        categorical_features = [\n",
    "            \"type_de_voie\", \"clean_code_departement\", \"clean_code_commune\",\n",
    "            \"code_postal\", \"main_type_terrain\", \"Dependance\", \"month\"\n",
    "        ]\n",
    "        numerical_features = [\n",
    "            \"surface_terrain\", \"surface_reelle_bati\", \"nb_pieces_principales\",'Taux_RP', 'Taux_LV', 'Taux_MAI',\n",
    "       'Taux_RP_1P', 'Taux_RP_2P', 'Taux_RP_3P', 'Taux_RP_4P', 'Taux_RP_5P',\n",
    "       'Taux_RP_30', 'Taux_RP_40', 'Taux_RP_60', 'Taux_RP_80', 'Taux_RP_100',\n",
    "       'Taux_RP_120', 'Taux_RP_P120', 'Taux_RP_GAR', 'Taux_RP_PROPRIO',\n",
    "       'Taux_RP_GRATUIT', 'Taux_RP_LOC', 'Taux_RP_HML', 'Taux_RP_AM02',\n",
    "       'Taux_RP_AM04', 'Taux_RP_AM09', 'Taux_RP_AM09P'\n",
    "        ]\n",
    "        for column in categorical_features:\n",
    "            self.df[column] = self.df[column].replace(np.nan, \"\").apply(str)\n",
    "        X = self.df[columns_model]\n",
    "        y =self.df[\"Valeur fonciere\"]\n",
    "        # selection des variables\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            test_size=0.33,\n",
    "                                                            random_state=42)\n",
    "        return self.df,categorical_features, numerical_features, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).feature_generation()\n",
    "\n",
    "df = Preprocessing(df).zscore() \n",
    "#df = Preprocessing(df).zscore() \n",
    "\n",
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test  = Preprocessing(df).split_x_y()\n",
    "\n",
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R(MSE): %.2f\" % math.sqrt(np.mean((test_y_hat - y_test)**2)))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))\n",
    "\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_dep_model_aggregations_logement.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "double application du zscore pr les deux idées :      \n",
    "score avec les colonnes génériques sans calcul des taux : \n",
    "\n",
    "* Score r²:  0.6286664529682898     \n",
    "* Mean absolute error: 56530.92   \n",
    "* Residual  of squares (MSE): 6222012343.59    \n",
    "* R(MSE): 78879.73    \n",
    "* R2-score: 0.43    \n",
    "\n",
    "score avec  colonnes featuring et calcul des taux : \n",
    "\n",
    "* Score r²:  0.6653430369461752\n",
    "* Mean absolute error: 64268.84\n",
    "* Residual  of squares (MSE): 9148083298.02\n",
    "* R(MSE): 95645.61\n",
    "* R2-score: 0.51\n",
    "\n",
    "application d'un zscore de 1 et calcul des taux : \n",
    "* Score r²:  0.6039277500731881\n",
    "* Mean absolute error: 75588.18\n",
    "* Residual  of squares (MSE): 21660448082.09\n",
    "* R(MSE): 147174.89\n",
    "* R2-score: 0.35\n",
    "\n",
    "application d'un zscore de 0.5 et calcul des taux : \n",
    "* Score r²:  0.6423394140464491\n",
    "* Mean absolute error: 71624.55\n",
    "* Residual  of squares (MSE): 14824202415.15\n",
    "* R(MSE): 121754.68\n",
    "* R2-score: 0.48\n",
    "\n",
    "\n",
    "application d'un zscore de 0.25 et calcul des taux (VF max : 1.3 M€): \n",
    "* Score r²:  0.6597730148379957\n",
    "* Mean absolute error: 66801.81\n",
    "* Residual  of squares (MSE): 10643126458.30\n",
    "* R(MSE): 103165.53\n",
    "* R2-score: 0.51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrichissemnt activites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat= pd.read_csv('../data/base-ic-activite-residents-2018.CSV',sep=';', dtype= {\n",
    "\"IRIS\" : object\n",
    ",\"COM\": object \n",
    ",\"TYP_IRIS\":  object \n",
    ",\"MODIF_IRIS\": object \n",
    ",\"LAB_IRIS\": object\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat=df_stat[[\"IRIS\",\"P18_POP1564\", \"P18_POP1524\",\"P18_POP2554\",\"P18_POP5564\", \"P18_ACT1564\",\"P18_ACTOCC1564\",\"P18_CHOM1564\",\n",
    "         \"C18_ACT1564\", \"C18_ACT1564_CS1\",\"C18_ACT1564_CS3\",\"C18_ACT1564_CS2\",\"C18_ACT1564_CS4\",\"C18_ACTOCC1564\",\n",
    "        \"C18_ACTOCC1564_CS1\" ,\"C18_ACTOCC1564_CS2\",\"C18_ACTOCC1564_CS3\", \"C18_ACTOCC1564_CS4\",\"P18_ACTOCC15P_ILT1\",\"C18_ACTOCC15P\",\n",
    "         \"C18_ACTOCC15P_PAS\" ,\"C18_ACTOCC15P_MAR\" ,\"C18_ACTOCC15P_VELO\" ,\"C18_ACTOCC15P_2ROUESMOT\" ,\"C18_ACTOCC15P_VOIT\" ,\n",
    "         \"C18_ACTOCC15P_TCOM\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_featuring_act (df) : \n",
    "    # repartition population \n",
    "    df[\"Taux_1524\"] = df[\"P18_POP1524\"]/df[\"P18_POP1564\"]\n",
    "    df[\"Taux_2554\"] = df[\"P18_POP2554\"]/df[\"P18_POP1564\"]\n",
    "    df[\"Taux_5564\"] = df[\"P18_POP5564\"]/df[\"P18_POP1564\"]\n",
    "    # statistiques économiques\n",
    "    df[\"Taux_P_Act\"] = df[\"P18_ACT1564\"]/df[\"P18_POP1564\"]\n",
    "    df[\"Taux_P_ActOct\"] = df[\"P18_ACTOCC1564\"]/df[\"P18_ACT1564\"] \n",
    "    df[\"Taux_P_CHO\"] = df[\"P18_CHOM1564\"]/df[\"P18_ACT1564\"]\n",
    "    \n",
    "    df[\"Taux_CS1\"] = df[\"C18_ACT1564_CS1\"]/df[\"C18_ACT1564\"] \n",
    "    df[\"Taux_CS2\"] = df[\"C18_ACT1564_CS2\"]/df[\"C18_ACT1564\"] \n",
    "    df[\"Taux_CS3\"] = df[\"C18_ACT1564_CS3\"]/df[\"C18_ACT1564\"] \n",
    "    df[\"Taux_CS4\"] = df[\"C18_ACT1564_CS4\"]/df[\"C18_ACT1564\"]  \n",
    "    \n",
    "    # Statistiques sur transport travail\n",
    "    df[\"Taux_Travail_Commune\"] = df[\"P18_ACTOCC15P_ILT1\"]/df[\"P18_ACTOCC1564\"]  \n",
    "    df[\"Taux_TT\"] = df[\"C18_ACTOCC15P_PAS\"]/df[\"C18_ACTOCC15P\"]\n",
    "    df[\"Taux_Mar\"] = df[\"C18_ACTOCC15P_MAR\"]/df[\"C18_ACTOCC15P\"] \n",
    "    df[\"Taux_Velo\"] = df[\"C18_ACTOCC15P_VELO\"]/df[\"C18_ACTOCC15P\"]\n",
    "    df[\"Taux_2Roues\"] = df[\"C18_ACTOCC15P_2ROUESMOT\"]/df[\"C18_ACTOCC15P\"] \n",
    "    df[\"Taux_Voit\"] = df[\"C18_ACTOCC15P_VOIT\"]/df[\"C18_ACTOCC15P\"] \n",
    "    df[\"Taux_TCOM\"] = df[\"C18_ACTOCC15P_TCOM\"]/df[\"C18_ACTOCC15P\"] \n",
    "   \n",
    "    \n",
    "    df =df.drop([\"P18_POP1564\", \"P18_POP1524\",\"P18_POP2554\",\"P18_POP5564\", \"P18_ACT1564\",\"P18_ACTOCC1564\",\"P18_CHOM1564\",\n",
    "         \"C18_ACT1564\", \"C18_ACT1564_CS1\",\"C18_ACT1564_CS3\",\"C18_ACT1564_CS2\",\"C18_ACT1564_CS4\",\"C18_ACTOCC1564\",\n",
    "        \"C18_ACTOCC1564_CS1\" ,\"C18_ACTOCC1564_CS2\",\"C18_ACTOCC1564_CS3\", \"C18_ACTOCC1564_CS4\",\"P18_ACTOCC15P_ILT1\",\"C18_ACTOCC15P\",\n",
    "         \"C18_ACTOCC15P_PAS\" ,\"C18_ACTOCC15P_MAR\" ,\"C18_ACTOCC15P_VELO\" ,\"C18_ACTOCC15P_2ROUESMOT\" ,\"C18_ACTOCC15P_VOIT\" ,\n",
    "         \"C18_ACTOCC15P_TCOM\"], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = columns_featuring_act(df_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = df_stat[df_stat['IRIS'].isin(df['IRIS'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_stat, left_on='IRIS', right_on='IRIS',\n",
    "          suffixes=('_left', '_right'),  how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"aggregatedfile_houses_dep_WITH_IRIS_STATS.csv\", sep='|', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test model enrichissement logement et activité \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('aggregatedfile_houses_dep_WITH_IRIS_STATS.csv', sep='|', index_col=0, dtype= {\n",
    "\"num_voie\" : np.float32\n",
    ",\"type_de_voie\": object \n",
    ",\"voie\":  object \n",
    ",\"commune\": object \n",
    ",\"clean_code_commune\": object\n",
    ",\"clean_code_departement\" : object\n",
    ",\"IRIS\": object\n",
    ",\"LAB_IRIS\" : object\n",
    ",\"Taux_RP\" : np.float32\n",
    ",\"parcelle_cad_section\" : object })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IRIS</th>\n",
       "      <th>parcelle_cad_section</th>\n",
       "      <th>Date mutation</th>\n",
       "      <th>Valeur fonciere</th>\n",
       "      <th>num_voie</th>\n",
       "      <th>B_T_Q</th>\n",
       "      <th>type_de_voie</th>\n",
       "      <th>voie</th>\n",
       "      <th>code_postal</th>\n",
       "      <th>commune</th>\n",
       "      <th>...</th>\n",
       "      <th>Taux_CS2</th>\n",
       "      <th>Taux_CS3</th>\n",
       "      <th>Taux_CS4</th>\n",
       "      <th>Taux_Travail_Commune</th>\n",
       "      <th>Taux_TT</th>\n",
       "      <th>Taux_Mar</th>\n",
       "      <th>Taux_Velo</th>\n",
       "      <th>Taux_2Roues</th>\n",
       "      <th>Taux_Voit</th>\n",
       "      <th>Taux_TCOM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010010000</td>\n",
       "      <td>01001000ZE</td>\n",
       "      <td>15/07/2021</td>\n",
       "      <td>127000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RTE</td>\n",
       "      <td>DU MONT</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>L'ABERGEMENT-CLEMENCIAT</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045751</td>\n",
       "      <td>0.174245</td>\n",
       "      <td>0.223526</td>\n",
       "      <td>0.160549</td>\n",
       "      <td>0.081668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.878921</td>\n",
       "      <td>0.039411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010010000</td>\n",
       "      <td>01001000ZH</td>\n",
       "      <td>02/02/2021</td>\n",
       "      <td>465225.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RTE</td>\n",
       "      <td>DE LA FONTAINE</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>L'ABERGEMENT-CLEMENCIAT</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045751</td>\n",
       "      <td>0.174245</td>\n",
       "      <td>0.223526</td>\n",
       "      <td>0.160549</td>\n",
       "      <td>0.081668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.878921</td>\n",
       "      <td>0.039411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010010000</td>\n",
       "      <td>01001000ZH</td>\n",
       "      <td>05/07/2021</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RUE</td>\n",
       "      <td>DE MUNETVILLE</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>L'ABERGEMENT-CLEMENCIAT</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045751</td>\n",
       "      <td>0.174245</td>\n",
       "      <td>0.223526</td>\n",
       "      <td>0.160549</td>\n",
       "      <td>0.081668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.878921</td>\n",
       "      <td>0.039411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010010000</td>\n",
       "      <td>01001000ZH</td>\n",
       "      <td>19/04/2021</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RUE</td>\n",
       "      <td>DU STADE</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>L'ABERGEMENT-CLEMENCIAT</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045751</td>\n",
       "      <td>0.174245</td>\n",
       "      <td>0.223526</td>\n",
       "      <td>0.160549</td>\n",
       "      <td>0.081668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.878921</td>\n",
       "      <td>0.039411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010010000</td>\n",
       "      <td>01001000ZH</td>\n",
       "      <td>23/06/2021</td>\n",
       "      <td>198000.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RUE</td>\n",
       "      <td>DES MURIERS</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>L'ABERGEMENT-CLEMENCIAT</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045751</td>\n",
       "      <td>0.174245</td>\n",
       "      <td>0.223526</td>\n",
       "      <td>0.160549</td>\n",
       "      <td>0.081668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.878921</td>\n",
       "      <td>0.039411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432310</th>\n",
       "      <td>974240101</td>\n",
       "      <td>974024000A</td>\n",
       "      <td>18/10/2021</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IMP</td>\n",
       "      <td>ETHEVE</td>\n",
       "      <td>97413.0</td>\n",
       "      <td>CILAOS</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039520</td>\n",
       "      <td>0.059659</td>\n",
       "      <td>0.179055</td>\n",
       "      <td>0.919202</td>\n",
       "      <td>0.076252</td>\n",
       "      <td>0.410453</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.028765</td>\n",
       "      <td>0.456028</td>\n",
       "      <td>0.018727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432311</th>\n",
       "      <td>974240101</td>\n",
       "      <td>974024000A</td>\n",
       "      <td>27/04/2021</td>\n",
       "      <td>220000.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHE</td>\n",
       "      <td>TERRE BLANCHE</td>\n",
       "      <td>97413.0</td>\n",
       "      <td>CILAOS</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039520</td>\n",
       "      <td>0.059659</td>\n",
       "      <td>0.179055</td>\n",
       "      <td>0.919202</td>\n",
       "      <td>0.076252</td>\n",
       "      <td>0.410453</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.028765</td>\n",
       "      <td>0.456028</td>\n",
       "      <td>0.018727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432312</th>\n",
       "      <td>974240101</td>\n",
       "      <td>974024000A</td>\n",
       "      <td>27/05/2021</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PAS</td>\n",
       "      <td>DES MARCHES</td>\n",
       "      <td>97413.0</td>\n",
       "      <td>CILAOS</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039520</td>\n",
       "      <td>0.059659</td>\n",
       "      <td>0.179055</td>\n",
       "      <td>0.919202</td>\n",
       "      <td>0.076252</td>\n",
       "      <td>0.410453</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.028765</td>\n",
       "      <td>0.456028</td>\n",
       "      <td>0.018727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432313</th>\n",
       "      <td>974240101</td>\n",
       "      <td>974024000A</td>\n",
       "      <td>28/05/2021</td>\n",
       "      <td>148850.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RUE</td>\n",
       "      <td>DES GLYCINES</td>\n",
       "      <td>97413.0</td>\n",
       "      <td>CILAOS</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039520</td>\n",
       "      <td>0.059659</td>\n",
       "      <td>0.179055</td>\n",
       "      <td>0.919202</td>\n",
       "      <td>0.076252</td>\n",
       "      <td>0.410453</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.028765</td>\n",
       "      <td>0.456028</td>\n",
       "      <td>0.018727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432314</th>\n",
       "      <td>974240101</td>\n",
       "      <td>974024000A</td>\n",
       "      <td>30/03/2021</td>\n",
       "      <td>175000.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>B</td>\n",
       "      <td>RUE</td>\n",
       "      <td>DES PINS</td>\n",
       "      <td>97413.0</td>\n",
       "      <td>CILAOS</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039520</td>\n",
       "      <td>0.059659</td>\n",
       "      <td>0.179055</td>\n",
       "      <td>0.919202</td>\n",
       "      <td>0.076252</td>\n",
       "      <td>0.410453</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.028765</td>\n",
       "      <td>0.456028</td>\n",
       "      <td>0.018727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>432315 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             IRIS parcelle_cad_section Date mutation  Valeur fonciere  \\\n",
       "0       010010000           01001000ZE    15/07/2021         127000.0   \n",
       "1       010010000           01001000ZH    02/02/2021         465225.0   \n",
       "2       010010000           01001000ZH    05/07/2021         120000.0   \n",
       "3       010010000           01001000ZH    19/04/2021         329000.0   \n",
       "4       010010000           01001000ZH    23/06/2021         198000.0   \n",
       "...           ...                  ...           ...              ...   \n",
       "432310  974240101           974024000A    18/10/2021         180000.0   \n",
       "432311  974240101           974024000A    27/04/2021         220000.0   \n",
       "432312  974240101           974024000A    27/05/2021          61000.0   \n",
       "432313  974240101           974024000A    28/05/2021         148850.0   \n",
       "432314  974240101           974024000A    30/03/2021         175000.0   \n",
       "\n",
       "        num_voie B_T_Q type_de_voie            voie  code_postal  \\\n",
       "0            1.0   NaN          RTE         DU MONT       1400.0   \n",
       "1          495.0   NaN          RTE  DE LA FONTAINE       1400.0   \n",
       "2          307.0   NaN          RUE   DE MUNETVILLE       1400.0   \n",
       "3          260.0   NaN          RUE        DU STADE       1400.0   \n",
       "4           43.0   NaN          RUE     DES MURIERS       1400.0   \n",
       "...          ...   ...          ...             ...          ...   \n",
       "432310      50.0   NaN          IMP          ETHEVE      97413.0   \n",
       "432311     185.0   NaN          CHE   TERRE BLANCHE      97413.0   \n",
       "432312       1.0   NaN          PAS     DES MARCHES      97413.0   \n",
       "432313      20.0   NaN          RUE    DES GLYCINES      97413.0   \n",
       "432314      27.0     B          RUE        DES PINS      97413.0   \n",
       "\n",
       "                        commune  ...  Taux_CS2  Taux_CS3  Taux_CS4  \\\n",
       "0       L'ABERGEMENT-CLEMENCIAT  ...  0.045751  0.174245  0.223526   \n",
       "1       L'ABERGEMENT-CLEMENCIAT  ...  0.045751  0.174245  0.223526   \n",
       "2       L'ABERGEMENT-CLEMENCIAT  ...  0.045751  0.174245  0.223526   \n",
       "3       L'ABERGEMENT-CLEMENCIAT  ...  0.045751  0.174245  0.223526   \n",
       "4       L'ABERGEMENT-CLEMENCIAT  ...  0.045751  0.174245  0.223526   \n",
       "...                         ...  ...       ...       ...       ...   \n",
       "432310                   CILAOS  ...  0.039520  0.059659  0.179055   \n",
       "432311                   CILAOS  ...  0.039520  0.059659  0.179055   \n",
       "432312                   CILAOS  ...  0.039520  0.059659  0.179055   \n",
       "432313                   CILAOS  ...  0.039520  0.059659  0.179055   \n",
       "432314                   CILAOS  ...  0.039520  0.059659  0.179055   \n",
       "\n",
       "        Taux_Travail_Commune   Taux_TT  Taux_Mar  Taux_Velo Taux_2Roues  \\\n",
       "0                   0.160549  0.081668  0.000000   0.000000    0.000000   \n",
       "1                   0.160549  0.081668  0.000000   0.000000    0.000000   \n",
       "2                   0.160549  0.081668  0.000000   0.000000    0.000000   \n",
       "3                   0.160549  0.081668  0.000000   0.000000    0.000000   \n",
       "4                   0.160549  0.081668  0.000000   0.000000    0.000000   \n",
       "...                      ...       ...       ...        ...         ...   \n",
       "432310              0.919202  0.076252  0.410453   0.009774    0.028765   \n",
       "432311              0.919202  0.076252  0.410453   0.009774    0.028765   \n",
       "432312              0.919202  0.076252  0.410453   0.009774    0.028765   \n",
       "432313              0.919202  0.076252  0.410453   0.009774    0.028765   \n",
       "432314              0.919202  0.076252  0.410453   0.009774    0.028765   \n",
       "\n",
       "       Taux_Voit  Taux_TCOM  \n",
       "0       0.878921   0.039411  \n",
       "1       0.878921   0.039411  \n",
       "2       0.878921   0.039411  \n",
       "3       0.878921   0.039411  \n",
       "4       0.878921   0.039411  \n",
       "...          ...        ...  \n",
       "432310  0.456028   0.018727  \n",
       "432311  0.456028   0.018727  \n",
       "432312  0.456028   0.018727  \n",
       "432313  0.456028   0.018727  \n",
       "432314  0.456028   0.018727  \n",
       "\n",
       "[432315 rows x 62 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from house_prediction_package.data import GetData\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from more_itertools import chunked\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#sans doute à supprimer au lancement final du modele\n",
    "#from locale import atof, setlocale, LC_NUMERIC, LC_ALL\n",
    "\n",
    "#setlocale(LC_ALL, 'fr_FR.UTF-8')\n",
    "\n",
    "class Preprocessing :\n",
    "\n",
    "    def __init__(self,df) :\n",
    "        # self.df = get_data().read_csv()\n",
    "        self.df = df\n",
    "\n",
    "    def clean_columns(self,\n",
    "                      columns=[\n",
    "                          'Code service CH', 'Reference document',\n",
    "                          '1 Articles CGI', '2 Articles CGI', '3 Articles CGI',\n",
    "                          '4 Articles CGI', '5 Articles CGI', 'No Volume',\n",
    "                          'Identifiant local'\n",
    "                      ]):\n",
    "        \"\"\" drop useless columns\n",
    "        Customisation of columns to drop must be entered as a list\n",
    "        \"\"\"\n",
    "        # suppression of 100% empty columns - these columns are officially not completed in this db\n",
    "        self.df = self.df.drop(columns,axis=1)\n",
    "        # suppression of columns poorly completed\n",
    "        columns_to_drop = [column for column in self.df.columns if ((self.df[column].isnull().value_counts().sort_index()[0]/self.df.shape[0])*100) < 2 ]\n",
    "        self.df= self.df.drop(columns_to_drop,axis=1)\n",
    "        # replacement of , by . in numerical variables & deletion of non numrical caracters in num columns : \n",
    "        columns_num = ['Valeur fonciere', 'Surface Carrez du 1er lot', 'Nombre de lots',\n",
    "        'Surface reelle bati', 'Nombre pieces principales', 'Surface terrain']\n",
    "        # transformation des , en . pour réaliser des opérations sur les nombres et suppressions des caracteres non numériques au sein de ces colonnes \n",
    "        for column in columns_num : \n",
    "            self.df[column]=self.df[column].apply(lambda s: s.replace(\",\",\".\") if isinstance(s,str) else s)\n",
    "            self.df[column] = pd.to_numeric(self.df[column], errors = 'coerce')\n",
    "        # suppression of nan value on target variable\n",
    "        self.df= self.df.dropna(subset=['Valeur fonciere'])\n",
    "        #self.df['Surface Carrez du 1er lot'] = self.df['Surface Carrez du 1er lot'].apply(\n",
    "        #    lambda x: atof(x))        \n",
    "        # pre processing avant groupby mais attention sortir valeures foncieres avant de mettre en POO\n",
    "        ob_columns= self.df.dtypes[self.df.dtypes == 'O'].index\n",
    "        num_columns = self.df.dtypes[(self.df.dtypes == 'int')\n",
    "                                     | (self.df.dtypes == 'float')].index\n",
    "        non_num_col = ['No disposition', 'No voie', 'Code postal', 'Code commune',\n",
    "       'Prefixe de section', 'No plan','Code type local']\n",
    "        num_columns = [value for value in num_columns if value not in non_num_col]\n",
    "        for column in ob_columns :\n",
    "            self.df[column]=self.df[column].replace(np.nan,'',regex=True)\n",
    "        #à adapter in v2\n",
    "        \n",
    "        self.df[num_columns] = self.df[num_columns].apply(pd.to_numeric,\n",
    "                                                              errors='coerce')\n",
    "        \n",
    "        #drop duplicates\n",
    "        self.df = self.df.drop_duplicates().reset_index(drop= True)\n",
    "        # by returning self, we can do method chaining like preprocessing(df).clean_columns().create_identifier()\n",
    "        return self.df\n",
    "\n",
    "    def create_identifier(self) :\n",
    "        \"\"\" Create a 'unique' identifier allowing us to group several lines corresponding to a unique transaction\n",
    "        \"\"\"\n",
    "        variables_to_clean = [\n",
    "            \"Code departement\", \"Code commune\", \"Prefixe de section\",\n",
    "            \"Section\", \"No plan\"\n",
    "            ]\n",
    "        size_variables= [2,3,3,2,4]\n",
    "        for i,j in zip(variables_to_clean,size_variables):\n",
    "            chunked_data = chunked(self.df[i], 10000, strict=False)\n",
    "            values = {\"Prefixe de section\": '000'}\n",
    "            self.df= self.df.fillna(value=values)\n",
    "            if i == \"Prefixe de section\" :\n",
    "                self.df[i] = self.df[i].apply(str).apply(lambda x: x[:3])\n",
    "            new_variable = [\n",
    "                str(value).replace(\".\",\"\").zfill(j) for sublist in list(chunked_data)\n",
    "                for value in sublist\n",
    "            ]\n",
    "            self.df[f\"clean_{i.replace(' ','_').lower()}\"] = new_variable\n",
    "            self.df= self.df.drop([i],axis=1)\n",
    "        self.df[\"parcelle_cadastrale\"] = self.df[[\n",
    "            \"clean_code_departement\", \"clean_code_commune\", \"clean_prefixe_de_section\",\n",
    "            \"clean_section\", \"clean_no_plan\"]].apply(lambda x: \"\".join(x), axis=1)\n",
    "        self.df[\"parcelle_cad_section\"]=self.df[\"parcelle_cadastrale\"].str[:10]\n",
    "        self.df = self.df.drop([\n",
    "            \"clean_prefixe_de_section\", \"clean_section\", \"clean_no_plan\"\n",
    "        ], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def aggregate_transactions(self):\n",
    "        self.df = self.df.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "            \"num_voie\" : x[\"No voie\"].max()\n",
    "            ,\"B_T_Q\" : x[\"B/T/Q\"].max()\n",
    "            ,\"type_de_voie\": x[\"Type de voie\"].max()\n",
    "            ,\"voie\": x[\"Voie\"].max()\n",
    "            ,\"code_postal\": x[\"Code postal\"].max()\n",
    "            ,\"commune\": max(x[\"Commune\"])\n",
    "            ,\"clean_code_departement\": x[\"clean_code_departement\"].max()\n",
    "            ,\"clean_code_commune\": max(x[\"clean_code_commune\"])\n",
    "            ,\"surface_carrez_lot_1\" :  x[\"Surface Carrez du 1er lot\"].sum()/((x[\"Surface reelle bati\"].count()/x[\"Nature culture\"].nunique()))\n",
    "            ,\"Nb_lots\": x[\"Nombre de lots\"].max()\n",
    "            ,\"surface_terrain\" : ((x[\"Surface terrain\"].sum()/x[\"Surface reelle bati\"].count()) if (int(x[\"Surface terrain\"].nunique()) ==1 and int(x[\"Nature culture\"].nunique()) == 1 )else x[\"Surface terrain\"].sum())\n",
    "            ,\"surface_reelle_bati\" : (x[\"Surface reelle bati\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if (int(x[\"Nature culture\"].nunique() > 1)) else x[\"Surface reelle bati\"].sum())\n",
    "            ,\"nb_pieces_principales\" : (x[\"Nombre pieces principales\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if int(x[\"Nature culture\"].nunique()) > 1 else x[\"Nombre pieces principales\"].sum())      \n",
    "            ,\"dependance\" : x[\"Type local\"].unique()\n",
    "            ,\"main_type_terrain\" : x[\"Nature culture\"].max()\n",
    "            ,\"parcelle_cadastrale\": x[\"parcelle_cadastrale\"].max()}))\n",
    "        self.df = self.df.replace(np.inf, np.nan)\n",
    "        #drop rows with only dependances transactions as we focus on houses\n",
    "        self.df = self.df[self.df.dependance.apply(\n",
    "            lambda x: x.all() != \"Dépendance\")].reset_index(drop=True)\n",
    "        self.df[\"dependance\"] = self.df.dependance.apply(sorted, 1)\n",
    "        self.df[[\"Dependance\",\n",
    "                 \"Maison\"]] = pd.DataFrame(self.df.dependance.tolist(),\n",
    "                                           index=self.df.index)\n",
    "        self.df[\"Dependance\"] = [1 if value ==\"Dépendance\"else 0 for value in self.df[\"Dependance\"]]\n",
    "        self.df= self.df.drop([\"dependance\",\"Maison\"],axis =1)\n",
    "        return self.df\n",
    "\n",
    "    # to do : function calling enrichissement from data\n",
    "\n",
    "\n",
    "    def feature_generation (self):\n",
    "        # convert the 'Date' column to datetime format\n",
    "        self.df[\"month\"] = pd.to_datetime(\n",
    "            self.df[\"Date mutation\"],format=\"%d/%m/%Y\").dt.month\n",
    "        self.df= self.df.drop([\"Date mutation\"], axis = 1)\n",
    "        ## attention à ne faire qu'après avoir enrichi avec variables insee\n",
    "        dict_type_voie = dict()\n",
    "        for value in self.df[\"type_de_voie\"].value_counts()[self.df[\"type_de_voie\"].value_counts()<300 ].index.values :\n",
    "            dict_type_voie[value] = \"Autres\"\n",
    "        self.df=self.df.replace({\"type_voie\" : dict_type_voie})\n",
    "        self.df[\"type_de_voie\"]= self.df[\"type_de_voie\"].replace(np.nan,'vide')\n",
    "        return self.df\n",
    "\n",
    "    def zscore (self) :\n",
    "        # Calculate the z-score from scratch\n",
    "        #self.df['Valeur fonciere']= df['Valeur fonciere'].apply(lambda x: atof(x))\n",
    "        standard_deviation = self.df[\"Valeur fonciere\"].std(ddof=0)\n",
    "        mean_value = self.df[\"Valeur fonciere\"].mean()\n",
    "        zscores = [(value - mean_value) / standard_deviation\n",
    "                for value in self.df[\"Valeur fonciere\"]]\n",
    "        self.df[\"zscores\"]= zscores\n",
    "        # absolute value of zscore and if sup x then 1  :\n",
    "        self.df[\"outlier\"] = [\n",
    "            1 if (abs(value) > 0.2) else 0 for value in self.df[\"zscores\"]\n",
    "        ]\n",
    "        self.df=self.df[self.df[\"outlier\"] == 0].reset_index(drop=True)\n",
    "        self.df = self.df.drop([\"zscores\",\"outlier\"], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def split_x_y (self):\n",
    "        columns_model = [\"type_de_voie\",\n",
    "            \"clean_code_departement\",\n",
    "            \"clean_code_commune\",\n",
    "            \"code_postal\",\n",
    "            \"surface_terrain\",\n",
    "            \"surface_reelle_bati\", \"nb_pieces_principales\",\n",
    "            \"main_type_terrain\",  \"Dependance\",'Taux_RP', 'Taux_LV', 'Taux_MAI',\n",
    "       'Taux_RP_1P', 'Taux_RP_2P', 'Taux_RP_3P', 'Taux_RP_4P', 'Taux_RP_5P',\n",
    "       'Taux_RP_30', 'Taux_RP_40', 'Taux_RP_60', 'Taux_RP_80', 'Taux_RP_100',\n",
    "       'Taux_RP_120', 'Taux_RP_P120', 'Taux_RP_GAR', 'Taux_RP_PROPRIO',\n",
    "       'Taux_RP_GRATUIT', 'Taux_RP_LOC', 'Taux_RP_HML', 'Taux_RP_AM02',\n",
    "       'Taux_RP_AM04', 'Taux_RP_AM09', 'Taux_RP_AM09P','Taux_1524',\n",
    "       'Taux_2554', 'Taux_5564', 'Taux_P_Act', 'Taux_P_ActOct', 'Taux_P_CHO',\n",
    "       'Taux_CS1', 'Taux_CS2', 'Taux_CS3', 'Taux_CS4', 'Taux_Travail_Commune',\n",
    "       'Taux_TT', 'Taux_Mar', 'Taux_Velo', 'Taux_2Roues', 'Taux_Voit',\n",
    "       'Taux_TCOM',\n",
    "            \"month\"]\n",
    "        # Séparation des variables catégorielles et numériques\n",
    "        categorical_features = [\n",
    "            \"type_de_voie\", \"clean_code_departement\", \"clean_code_commune\",\n",
    "            \"code_postal\", \"main_type_terrain\", \"Dependance\", \"month\"\n",
    "        ]\n",
    "        numerical_features = [\n",
    "            \"surface_terrain\", \"surface_reelle_bati\", \"nb_pieces_principales\",'Taux_RP', 'Taux_LV', 'Taux_MAI',\n",
    "       'Taux_RP_1P', 'Taux_RP_2P', 'Taux_RP_3P', 'Taux_RP_4P', 'Taux_RP_5P',\n",
    "       'Taux_RP_30', 'Taux_RP_40', 'Taux_RP_60', 'Taux_RP_80', 'Taux_RP_100',\n",
    "       'Taux_RP_120', 'Taux_RP_P120', 'Taux_RP_GAR', 'Taux_RP_PROPRIO',\n",
    "       'Taux_RP_GRATUIT', 'Taux_RP_LOC', 'Taux_RP_HML', 'Taux_RP_AM02',\n",
    "       'Taux_RP_AM04', 'Taux_RP_AM09', 'Taux_RP_AM09P','Taux_1524',\n",
    "       'Taux_2554', 'Taux_5564', 'Taux_P_Act', 'Taux_P_ActOct', 'Taux_P_CHO',\n",
    "       'Taux_CS1', 'Taux_CS2', 'Taux_CS3', 'Taux_CS4', 'Taux_Travail_Commune',\n",
    "       'Taux_TT', 'Taux_Mar', 'Taux_Velo', 'Taux_2Roues', 'Taux_Voit',\n",
    "       'Taux_TCOM'\n",
    "        ]\n",
    "        for column in categorical_features:\n",
    "            self.df[column] = self.df[column].replace(np.nan, \"\").apply(str)\n",
    "        X = self.df[columns_model]\n",
    "        y =self.df[\"Valeur fonciere\"]\n",
    "        # selection des variables\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            test_size=0.33,\n",
    "                                                            random_state=42)\n",
    "        return self.df,categorical_features, numerical_features, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "#from house_prediction_package.preprocessing import Preprocessing\n",
    "#from house_prediction_package.data import GetData\n",
    "\n",
    "\n",
    "class Pipeline :\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        # self.categorical_features = categorical_features\n",
    "        # self.numerical_features = numerical_features\n",
    "        # self.X_train = X_train\n",
    "        # self.y_train = y_train\n",
    "        # option 2\n",
    "        #appeler les méthodes\n",
    "        self.df, self.categorical_features, self.numerical_features, self.X_train, self.X_test, self.y_train, self.y_test = Preprocessing(\n",
    "            df).feature_generation().zscore().split_x_y()\n",
    "\n",
    "    def pipeline(self):\n",
    "        # création des pipelines de pré-processing pour les variables numériques et catégorielles\n",
    "        #ajout d'un parametre pour gerer les valeures non connues dans onehotencoder - il les passe à 0(autres options disponibles)\n",
    "        numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "        categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, self.numerical_features),\n",
    "            (categorical_pipeline, self.categorical_features))\n",
    "        model = make_pipeline(preprocessor, LinearRegression())\n",
    "        fitted_model = model.fit(self.X_train, self.y_train)\n",
    "        return fitted_model, self.X_train, self.y_train,self.X_test, self.y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).feature_generation()\n",
    "\n",
    "df = Preprocessing(df).zscore() \n",
    "#df = Preprocessing(df).zscore() \n",
    "\n",
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test  = Preprocessing(df).split_x_y()\n",
    "\n",
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R(MSE): %.2f\" % math.sqrt(np.mean((test_y_hat - y_test)**2)))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))\n",
    "\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_dep_model_aggregations_logement_act.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result with zscore à 0.2 et ensemble des stats : \n",
    "* Score r²:  0.6722149896989466\n",
    "* Mean absolute error: 64178.07\n",
    "* Residual  of squares (MSE): 9194771971.12\n",
    "* R(MSE): 95889.37\n",
    "* R2-score: 0.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IRIS</th>\n",
       "      <th>parcelle_cad_section</th>\n",
       "      <th>Valeur fonciere</th>\n",
       "      <th>num_voie</th>\n",
       "      <th>B_T_Q</th>\n",
       "      <th>type_de_voie</th>\n",
       "      <th>voie</th>\n",
       "      <th>code_postal</th>\n",
       "      <th>commune</th>\n",
       "      <th>clean_code_departement</th>\n",
       "      <th>...</th>\n",
       "      <th>Taux_CS3</th>\n",
       "      <th>Taux_CS4</th>\n",
       "      <th>Taux_Travail_Commune</th>\n",
       "      <th>Taux_TT</th>\n",
       "      <th>Taux_Mar</th>\n",
       "      <th>Taux_Velo</th>\n",
       "      <th>Taux_2Roues</th>\n",
       "      <th>Taux_Voit</th>\n",
       "      <th>Taux_TCOM</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010010000</td>\n",
       "      <td>01001000ZE</td>\n",
       "      <td>127000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RTE</td>\n",
       "      <td>DU MONT</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>L'ABERGEMENT-CLEMENCIAT</td>\n",
       "      <td>01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174245</td>\n",
       "      <td>0.223526</td>\n",
       "      <td>0.160549</td>\n",
       "      <td>0.081668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.878921</td>\n",
       "      <td>0.039411</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010010000</td>\n",
       "      <td>01001000ZH</td>\n",
       "      <td>465225.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RTE</td>\n",
       "      <td>DE LA FONTAINE</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>L'ABERGEMENT-CLEMENCIAT</td>\n",
       "      <td>01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174245</td>\n",
       "      <td>0.223526</td>\n",
       "      <td>0.160549</td>\n",
       "      <td>0.081668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.878921</td>\n",
       "      <td>0.039411</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010010000</td>\n",
       "      <td>01001000ZH</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RUE</td>\n",
       "      <td>DE MUNETVILLE</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>L'ABERGEMENT-CLEMENCIAT</td>\n",
       "      <td>01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174245</td>\n",
       "      <td>0.223526</td>\n",
       "      <td>0.160549</td>\n",
       "      <td>0.081668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.878921</td>\n",
       "      <td>0.039411</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010010000</td>\n",
       "      <td>01001000ZH</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RUE</td>\n",
       "      <td>DU STADE</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>L'ABERGEMENT-CLEMENCIAT</td>\n",
       "      <td>01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174245</td>\n",
       "      <td>0.223526</td>\n",
       "      <td>0.160549</td>\n",
       "      <td>0.081668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.878921</td>\n",
       "      <td>0.039411</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010010000</td>\n",
       "      <td>01001000ZH</td>\n",
       "      <td>198000.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RUE</td>\n",
       "      <td>DES MURIERS</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>L'ABERGEMENT-CLEMENCIAT</td>\n",
       "      <td>01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174245</td>\n",
       "      <td>0.223526</td>\n",
       "      <td>0.160549</td>\n",
       "      <td>0.081668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.878921</td>\n",
       "      <td>0.039411</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427478</th>\n",
       "      <td>974240101</td>\n",
       "      <td>974024000A</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IMP</td>\n",
       "      <td>ETHEVE</td>\n",
       "      <td>97413.0</td>\n",
       "      <td>CILAOS</td>\n",
       "      <td>974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059659</td>\n",
       "      <td>0.179055</td>\n",
       "      <td>0.919202</td>\n",
       "      <td>0.076252</td>\n",
       "      <td>0.410453</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.028765</td>\n",
       "      <td>0.456028</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427479</th>\n",
       "      <td>974240101</td>\n",
       "      <td>974024000A</td>\n",
       "      <td>220000.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHE</td>\n",
       "      <td>TERRE BLANCHE</td>\n",
       "      <td>97413.0</td>\n",
       "      <td>CILAOS</td>\n",
       "      <td>974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059659</td>\n",
       "      <td>0.179055</td>\n",
       "      <td>0.919202</td>\n",
       "      <td>0.076252</td>\n",
       "      <td>0.410453</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.028765</td>\n",
       "      <td>0.456028</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427480</th>\n",
       "      <td>974240101</td>\n",
       "      <td>974024000A</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PAS</td>\n",
       "      <td>DES MARCHES</td>\n",
       "      <td>97413.0</td>\n",
       "      <td>CILAOS</td>\n",
       "      <td>974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059659</td>\n",
       "      <td>0.179055</td>\n",
       "      <td>0.919202</td>\n",
       "      <td>0.076252</td>\n",
       "      <td>0.410453</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.028765</td>\n",
       "      <td>0.456028</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427481</th>\n",
       "      <td>974240101</td>\n",
       "      <td>974024000A</td>\n",
       "      <td>148850.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RUE</td>\n",
       "      <td>DES GLYCINES</td>\n",
       "      <td>97413.0</td>\n",
       "      <td>CILAOS</td>\n",
       "      <td>974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059659</td>\n",
       "      <td>0.179055</td>\n",
       "      <td>0.919202</td>\n",
       "      <td>0.076252</td>\n",
       "      <td>0.410453</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.028765</td>\n",
       "      <td>0.456028</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427482</th>\n",
       "      <td>974240101</td>\n",
       "      <td>974024000A</td>\n",
       "      <td>175000.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>B</td>\n",
       "      <td>RUE</td>\n",
       "      <td>DES PINS</td>\n",
       "      <td>97413.0</td>\n",
       "      <td>CILAOS</td>\n",
       "      <td>974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059659</td>\n",
       "      <td>0.179055</td>\n",
       "      <td>0.919202</td>\n",
       "      <td>0.076252</td>\n",
       "      <td>0.410453</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.028765</td>\n",
       "      <td>0.456028</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>427483 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             IRIS parcelle_cad_section  Valeur fonciere  num_voie B_T_Q  \\\n",
       "0       010010000           01001000ZE         127000.0       1.0   NaN   \n",
       "1       010010000           01001000ZH         465225.0     495.0   NaN   \n",
       "2       010010000           01001000ZH         120000.0     307.0   NaN   \n",
       "3       010010000           01001000ZH         329000.0     260.0   NaN   \n",
       "4       010010000           01001000ZH         198000.0      43.0   NaN   \n",
       "...           ...                  ...              ...       ...   ...   \n",
       "427478  974240101           974024000A         180000.0      50.0   NaN   \n",
       "427479  974240101           974024000A         220000.0     185.0   NaN   \n",
       "427480  974240101           974024000A          61000.0       1.0   NaN   \n",
       "427481  974240101           974024000A         148850.0      20.0   NaN   \n",
       "427482  974240101           974024000A         175000.0      27.0     B   \n",
       "\n",
       "       type_de_voie            voie code_postal                  commune  \\\n",
       "0               RTE         DU MONT      1400.0  L'ABERGEMENT-CLEMENCIAT   \n",
       "1               RTE  DE LA FONTAINE      1400.0  L'ABERGEMENT-CLEMENCIAT   \n",
       "2               RUE   DE MUNETVILLE      1400.0  L'ABERGEMENT-CLEMENCIAT   \n",
       "3               RUE        DU STADE      1400.0  L'ABERGEMENT-CLEMENCIAT   \n",
       "4               RUE     DES MURIERS      1400.0  L'ABERGEMENT-CLEMENCIAT   \n",
       "...             ...             ...         ...                      ...   \n",
       "427478          IMP          ETHEVE     97413.0                   CILAOS   \n",
       "427479          CHE   TERRE BLANCHE     97413.0                   CILAOS   \n",
       "427480          PAS     DES MARCHES     97413.0                   CILAOS   \n",
       "427481          RUE    DES GLYCINES     97413.0                   CILAOS   \n",
       "427482          RUE        DES PINS     97413.0                   CILAOS   \n",
       "\n",
       "       clean_code_departement  ...  Taux_CS3  Taux_CS4  Taux_Travail_Commune  \\\n",
       "0                          01  ...  0.174245  0.223526              0.160549   \n",
       "1                          01  ...  0.174245  0.223526              0.160549   \n",
       "2                          01  ...  0.174245  0.223526              0.160549   \n",
       "3                          01  ...  0.174245  0.223526              0.160549   \n",
       "4                          01  ...  0.174245  0.223526              0.160549   \n",
       "...                       ...  ...       ...       ...                   ...   \n",
       "427478                    974  ...  0.059659  0.179055              0.919202   \n",
       "427479                    974  ...  0.059659  0.179055              0.919202   \n",
       "427480                    974  ...  0.059659  0.179055              0.919202   \n",
       "427481                    974  ...  0.059659  0.179055              0.919202   \n",
       "427482                    974  ...  0.059659  0.179055              0.919202   \n",
       "\n",
       "         Taux_TT  Taux_Mar  Taux_Velo Taux_2Roues Taux_Voit Taux_TCOM month  \n",
       "0       0.081668  0.000000   0.000000    0.000000  0.878921  0.039411     7  \n",
       "1       0.081668  0.000000   0.000000    0.000000  0.878921  0.039411     2  \n",
       "2       0.081668  0.000000   0.000000    0.000000  0.878921  0.039411     7  \n",
       "3       0.081668  0.000000   0.000000    0.000000  0.878921  0.039411     4  \n",
       "4       0.081668  0.000000   0.000000    0.000000  0.878921  0.039411     6  \n",
       "...          ...       ...        ...         ...       ...       ...   ...  \n",
       "427478  0.076252  0.410453   0.009774    0.028765  0.456028  0.018727    10  \n",
       "427479  0.076252  0.410453   0.009774    0.028765  0.456028  0.018727     4  \n",
       "427480  0.076252  0.410453   0.009774    0.028765  0.456028  0.018727     5  \n",
       "427481  0.076252  0.410453   0.009774    0.028765  0.456028  0.018727     5  \n",
       "427482  0.076252  0.410453   0.009774    0.028765  0.456028  0.018727     3  \n",
       "\n",
       "[427483 rows x 62 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(preprocessor, RandomForestRegressor())\n",
    "\n",
    "fitted_model = model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R(MSE): %.2f\" % math.sqrt(np.mean((test_y_hat - y_test)**2)))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))\n",
    "\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_dep_model_aggregations_logement_act_forest.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifiers\n",
    "reg1 = GradientBoostingRegressor(random_state=1)\n",
    "reg2 = RandomForestRegressor(random_state=1)\n",
    "reg3 = LinearRegression()\n",
    "ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "ereg = ereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification regroupement modalité code commune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(f'http://127.0.0.1:5555/api/insee/logement/distribution/010040201?by=area').json()['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cas de tests sur aggregations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= GetData().read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).clean_columns()\n",
    "\n",
    "df= Preprocessing(df).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "\"surface_terrain\" : ((x[\"Surface terrain\"].sum()/x[\"Surface reelle bati\"].count()) if (int(x[\"Surface terrain\"].nunique()) ==1 and int(x[\"Nature culture\"].nunique()) == 1 )else x[\"Surface terrain\"].sum())\n",
    "    ,\"Nature_culture\" : x[\"Nature culture\"].max()\n",
    "    , \"su terrain 2\": x[\"Surface terrain\"].sum()\n",
    "    , \"suterrainmax\": x[\"Surface terrain\"].max()\n",
    "  #  , \"su_bat\": x[\"Surface reelle bati\"]\n",
    "    ,\"nat_terrain_unique\": x[\"Nature culture\"].nunique()\n",
    "    , \"suterrain_count\" : x[\"Surface terrain\"].count()\n",
    "    ,\"suterrain_unique\": x[\"Surface terrain\"].nunique()\n",
    "    ,\"su_bat_unique\" : x[\"Surface reelle bati\"].nunique()\n",
    "    ,\"su_bat_count\" : x[\"Surface reelle bati\"].count()\n",
    "            \n",
    "})).tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "    \"surface_reelle_bati\" : (x[\"Surface reelle bati\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if (int(x[\"Nature culture\"].nunique() > 1)) else x[\"Surface reelle bati\"].sum())\n",
    " ,\"nb_pieces_principales\" : (x[\"Nombre pieces principales\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if int(x[\"Nature culture\"].nunique()) > 1 else x[\"Nombre pieces principales\"].sum())      \n",
    "    ,\"nb_piecemax\" : x[\"Nombre pieces principales\"].max()\n",
    "    ,\"Nature_culture\" : x[\"Nature culture\"].max()\n",
    "    , \"su bat 2\": x[\"Surface reelle bati\"].sum()\n",
    "    , \"sumax\": x[\"Surface reelle bati\"].max()\n",
    "  #  , \"su_bat\": x[\"Surface reelle bati\"]\n",
    "    , \"su_count\" : x[\"Surface reelle bati\"].count()\n",
    "    ,\"nat_cul_unique\": x[\"Nature culture\"].nunique()\n",
    "    ,\"subatiment_unique\": x[\"Surface reelle bati\"].nunique()\n",
    "            \n",
    "})).tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tot= GetData().read_csv([1,3])\n",
    "\n",
    "df_tot = Preprocessing(df_tot).clean_columns()\n",
    "\n",
    "df_tot= Preprocessing(df_tot).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cas ou type local identique mais nature culture différente: \n",
    "\n",
    "df_tot[(df_tot['parcelle_cadastrale']== '01289000AC0176') | (df_tot['parcelle_cadastrale']== '013500000C1248')| (df_tot['parcelle_cadastrale']== '01195000AD0050')]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# meme de surface reelle bati \n",
    "#pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "#meme nombre pieces principales \n",
    "# différente nature culture (variable texte )\n",
    "# différente surface terrain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cas ou type local identique mais nature culture différente: \n",
    "\n",
    "df[(df['parcelle_cadastrale']== '01289000AC0176') | (df['parcelle_cadastrale']== '013500000C1248')| (df['parcelle_cadastrale']== '01195000AD0050')]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# meme de surface reelle bati \n",
    "#pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "#meme nombre pieces principales \n",
    "# différente nature culture (variable texte )\n",
    "# différente surface terrain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['parcelle_cad_section']=='01001000ZH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# même maison surface reelle bati identique \n",
    "df[(df['parcelle_cadastrale'] == '013500000C1248')]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# meêm type local \n",
    "# même surface relle bati\n",
    " #pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "# même nombre de pieces principales\n",
    "\n",
    "# différence nature culture (variable texte )\n",
    "# surface terrain différente en fonction de la parcelle cadastrale \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cas ou 2 maisons , une dépendance et un terrain :\n",
    "# repérable par section et date commune \n",
    "\n",
    "df[df.index.isin([71,72,73,74])]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# code type lcoal différent pr dépendance absent pour terrain\n",
    "# différence de surface reelle bati  (0 dépendance et nan pour terrain)\n",
    "#pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "# différence sur nombre de pieces principales\n",
    "\n",
    "# différence nature culture (variable texte )\n",
    "# surface terrain différente en fonction de la parcelle cadastrale \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "house_prediction",
   "language": "python",
   "name": "house_prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "7e331f8398536a24290f874ffd288d2ba0d3938ea5219b401a264de1a09e8807"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
