{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from locale import atof, setlocale, LC_NUMERIC, LC_ALL\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "setlocale(LC_ALL, 'fr_FR.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GetData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class GetData:\n",
    "    \"\"\" Read data from csv and load it in a dataframe\n",
    "    accepted arguments : path to file , separator, chunksize and filter\n",
    "    option to load csv by filtering on house type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,path =\"../data/valeursfoncieres-2021.txt\",sep = \"|\", chunksize = 100000):\n",
    "        self.path = path\n",
    "        self.sep = sep\n",
    "        self.chunksize = chunksize\n",
    "\n",
    "\n",
    "    def read_csv(self, filtering_column='Code type local', filter=[1]):\n",
    "        \"\"\" pass option on which column to filter and filter value\n",
    "        if several filter value, pass the as a list\"\"\"\n",
    "        iter_csv = pd.read_csv(self.path,\n",
    "                               sep=self.sep,\n",
    "                               iterator=True,\n",
    "                               chunksize=self.chunksize,\n",
    "                               low_memory=False)\n",
    "        self.df = pd.concat([\n",
    "            chunk[chunk[filtering_column].isin(filter)] for chunk in iter_csv\n",
    "        ])\n",
    "        return self.df\n",
    "\n",
    "    def enrichissement_coordinates(self,df):\n",
    "        pass\n",
    "\n",
    "    def enrichissement_insee(self,df):\n",
    "        pass\n",
    "\n",
    "    def loading_data_db (self,df):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout l'objectif de cette étape est de tirer le maximum du dataset, en reconstituant ce qui correspond réellement à une transaction. \n",
    "Nous avons réalisé cela en regroupant chaque transaction selon 3 clés : Parcelle / date / montant car ceux ci sont répété sur les lignes communes. \n",
    "Ensuite nous avons utilisé la methode apply qui pour chaque ligne du dataset aggrégé applique une fonction lambda dans laquelle on inséré une série dont les paramétres étaient la ligne aggrée. cela nous a permis de réaliser des fonctions plus complexes au niveau des aggrégats et de couvrir les différents cas ammenant à la mutltiplication des lignes \n",
    "Les cas plus complexes à retrouver sont par exemple la nature culture basée principale en se basant sur la superficie de celle ci (--- piste https://stackoverflow.com/questions/23394476/keep-other-columns-when-doing-groupby \n",
    "L'autre cas complexe était de compter le nombre de dépendances différentes ainsi que le nombre de maisons différentes au sein d'une même aggrégation -**ajouter cas ou on a plusieurs dépendances et plusieurs terrains -- trop complexe** : \n",
    "* #dépendance - count du terme dépendance valuecount sorted[0]  / nunique type local si nunique nat culture = 1 & len value count ==2 else count terme dependance valuecount sorted[0]   if len_value count ==2 else 0\n",
    "* #maisons - count terme maison value count sorted [1] / nunqiue type local si nunique nat culture = 1 & len value count ==2 else count terme maison value count if len value_count ==2 else 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from house_prediction_package.data import GetData\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from more_itertools import chunked\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#sans doute à supprimer au lancement final du modele\n",
    "from locale import atof, setlocale, LC_NUMERIC, LC_ALL\n",
    "\n",
    "setlocale(LC_ALL, 'fr_FR.UTF-8')\n",
    "\n",
    "class Preprocessing :\n",
    "\n",
    "    def __init__(self,df) :\n",
    "        # self.df = get_data().read_csv()\n",
    "        self.df = df\n",
    "\n",
    "    def clean_columns(self,\n",
    "                      columns=[\n",
    "                          'Code service CH', 'Reference document',\n",
    "                          '1 Articles CGI', '2 Articles CGI', '3 Articles CGI',\n",
    "                          '4 Articles CGI', '5 Articles CGI', 'No Volume',\n",
    "                          'Identifiant local'\n",
    "                      ]):\n",
    "        \"\"\" drop useless columns\n",
    "        Customisation of columns to drop must be entered as a list\n",
    "        \"\"\"\n",
    "        # suppression of 100% empty columns - these columns are officially not completed in this db\n",
    "        self.df = self.df.drop(columns,axis=1)\n",
    "        # suppression of columns poorly completed\n",
    "        columns_to_drop = [column for column in self.df.columns if ((self.df[column].isnull().value_counts().sort_index()[0]/self.df.shape[0])*100) < 2 ]\n",
    "        self.df= self.df.drop(columns_to_drop,axis=1)\n",
    "        # suppression of nan value on target variable\n",
    "        self.df= self.df.dropna(subset=['Valeur fonciere'])\n",
    "        # pre processing avant groupby mais attention sortir valeures foncieres avant de mettre en POO\n",
    "        ob_columns= self.df.dtypes[self.df.dtypes == 'O'].index\n",
    "        num_columns = self.df.dtypes[(self.df.dtypes == 'int')\n",
    "                                     | (self.df.dtypes == 'float')].index\n",
    "        non_num_col = ['No disposition', 'No voie', 'Code postal', 'Code commune',\n",
    "       'Prefixe de section', 'No plan','Code type local']\n",
    "        num_columns = [value for value in num_columns if value not in non_num_col]\n",
    "        for column in ob_columns :\n",
    "            self.df[column]=self.df[column].replace(np.nan,'',regex=True)\n",
    "        #à adapter in v2\n",
    "        \n",
    "        self.df[num_columns] = self.df[num_columns].apply(pd.to_numeric,\n",
    "                                                              errors='coerce')\n",
    "        \n",
    "        #drop duplicates\n",
    "        self.df = self.df.drop_duplicates().reset_index(drop= True)\n",
    "        # by returning self, we can do method chaining like preprocessing(df).clean_columns().create_identifier()\n",
    "        return self.df\n",
    "\n",
    "    def create_identifier(self) :\n",
    "        \"\"\" Create a 'unique' identifier allowing us to group several lines corresponding to a unique transaction\n",
    "        \"\"\"\n",
    "        variables_to_clean = [\n",
    "            \"Code departement\", \"Code commune\", \"Prefixe de section\",\n",
    "            \"Section\", \"No plan\"\n",
    "            ]\n",
    "        size_variables= [2,3,3,2,4]\n",
    "        for i,j in zip(variables_to_clean,size_variables):\n",
    "            chunked_data = chunked(self.df[i], 10000, strict=False)\n",
    "            values = {\"Prefixe de section\": '000'}\n",
    "            self.df= self.df.fillna(value=values)\n",
    "            if i == \"Prefixe de section\" :\n",
    "                self.df[i] = self.df[i].apply(str).apply(lambda x: x[:3])\n",
    "            new_variable = [\n",
    "                str(value).replace(\".\",\"\").zfill(j) for sublist in list(chunked_data)\n",
    "                for value in sublist\n",
    "            ]\n",
    "            self.df[f\"clean_{i.replace(' ','_').lower()}\"] = new_variable\n",
    "            self.df= self.df.drop([i],axis=1)\n",
    "        self.df[\"parcelle_cadastrale\"] = self.df[[\n",
    "            \"clean_code_departement\", \"clean_code_commune\", \"clean_prefixe_de_section\",\n",
    "            \"clean_section\", \"clean_no_plan\"]].apply(lambda x: \"\".join(x), axis=1)\n",
    "        self.df[\"parcelle_cad_section\"]=self.df[\"parcelle_cadastrale\"].str[:10]\n",
    "        self.df = self.df.drop([\n",
    "            \"clean_prefixe_de_section\", \"clean_section\", \"clean_no_plan\"\n",
    "        ], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def aggregate_transactions(self):\n",
    "        self.df = self.df.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "            \"B_T_Q\" : x[\"B/T/Q\"].max()\n",
    "            ,\"type_de_voie\": x[\"Type de voie\"].max()\n",
    "            ,\"voie\": x[\"Voie\"].max()\n",
    "            ,\"code_postal\": x[\"Code postal\"].max()\n",
    "            ,\"commune\": max(x[\"Commune\"])\n",
    "            ,\"clean_code_departement\": x[\"clean_code_departement\"].max()\n",
    "            ,\"clean_code_commune\": max(x[\"clean_code_commune\"])\n",
    "           # ,\"surface_carrez_lot_1\" :  x[\"Surface Carrez du 1er lot\"].sum()/((x[\"Surface reelle bati\"].count()/x[\"Nature culture\"].nunique()))\n",
    "            ,\"Nb_lots\": x[\"Nombre de lots\"].max()\n",
    "            ,\"surface_terrain\" : ((x[\"Surface terrain\"].sum()/x[\"Surface reelle bati\"].count()) if (int(x[\"Surface terrain\"].nunique()) ==1 and int(x[\"Nature culture\"].nunique()) == 1 )else x[\"Surface terrain\"].sum())\n",
    "            ,\"surface_reelle_bati\" : (x[\"Surface reelle bati\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if (int(x[\"Nature culture\"].nunique() > 1)) else x[\"Surface reelle bati\"].sum())\n",
    "            ,\"nb_pieces_principales\" : (x[\"Nombre pieces principales\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if int(x[\"Nature culture\"].nunique()) > 1 else x[\"Nombre pieces principales\"].sum())      \n",
    "            ,\"dependance\" : x[\"Type local\"].unique()\n",
    "            ,\"main_type_terrain\" : x[\"Nature culture\"].max()\n",
    "            ,\"parcelle_cadastrale\": x[\"parcelle_cadastrale\"].max()}))\n",
    "        #drop rows with only dependances transactions as we focus on houses\n",
    "        self.df = self.df[self.df.dependance.apply(\n",
    "            lambda x: x.all() != \"Dépendance\")].reset_index(drop=True)\n",
    "        self.df[\"dependance\"] = self.df.dependance.apply(sorted, 1)\n",
    "        self.df[[\"Dependance\",\n",
    "                 \"Maison\"]] = pd.DataFrame(self.df.dependance.tolist(),\n",
    "                                           index=self.df.index)\n",
    "        self.df[\"Dependance\"] = [1 if value ==\"Dépendance\"else 0 for value in self.df[\"Dependance\"]]\n",
    "        self.df= self.df.drop([\"dependance\",\"Maison\"],axis =1)\n",
    "        return self.df\n",
    "\n",
    "    # to do : function calling enrichissement from data\n",
    "\n",
    "\n",
    "    def feature_generation (self):\n",
    "        # convert the 'Date' column to datetime format\n",
    "        self.df[\"month\"] = pd.to_datetime(\n",
    "            self.df[\"Date mutation\"],format=\"%d/%m/%Y\").dt.month\n",
    "        self.df= self.df.drop([\"Date mutation\"], axis = 1)\n",
    "        ## attention à ne faire qu'après avoir enrichi avec variables insee\n",
    "        dict_type_voie = dict()\n",
    "        for value in self.df[\"type_de_voie\"].value_counts()[self.df[\"type_de_voie\"].value_counts()<300 ].index.values :\n",
    "            dict_type_voie[value] = \"Autres\"\n",
    "        self.df=self.df.replace({\"type_voie\" : dict_type_voie})\n",
    "        self.df[\"type_de_voie\"]= self.df[\"type_de_voie\"].replace(np.nan,'vide')\n",
    "        return self.df\n",
    "\n",
    "    def zscore (self) :\n",
    "        # Calculate the z-score from scratch\n",
    "        #self.df['Valeur fonciere']= df['Valeur fonciere'].apply(lambda x: atof(x))\n",
    "        standard_deviation = self.df[\"Valeur fonciere\"].std(ddof=0)\n",
    "        mean_value = self.df[\"Valeur fonciere\"].mean()\n",
    "        zscores = [(value - mean_value) / standard_deviation\n",
    "                for value in self.df[\"Valeur fonciere\"]]\n",
    "        self.df[\"zscores\"]= zscores\n",
    "        # absolute value of zscore and if sup x then 1  :\n",
    "        self.df[\"outlier\"] = [\n",
    "            1 if (abs(value) > 3) else 0 for value in self.df[\"zscores\"]\n",
    "        ]\n",
    "        self.df=self.df[self.df[\"outlier\"] == 0].reset_index(drop=True)\n",
    "        self.df = self.df.drop([\"zscores\",\"outlier\"], axis = 1)\n",
    "        return self.df\n",
    "\n",
    "    def split_x_y (self):\n",
    "        columns_model = [\"type_de_voie\",\n",
    "            \"clean_code_departement\",\n",
    "            \"clean_code_commune\",\n",
    "            \"code_postal\",\n",
    "            \"surface_terrain\",\n",
    "            \"surface_reelle_bati\", \"nb_pieces_principales\",\n",
    "            \"main_type_terrain\",  \"Dependance\",\n",
    "            \"month\"]\n",
    "        # Séparation des variables catégorielles et numériques\n",
    "        categorical_features = [\n",
    "            \"type_de_voie\", \"clean_code_departement\", \"clean_code_commune\",\n",
    "            \"code_postal\", \"main_type_terrain\", \"Dependance\", \"month\"\n",
    "        ]\n",
    "        numerical_features = [\n",
    "            \"surface_terrain\", \"surface_reelle_bati\", \"nb_pieces_principales\"\n",
    "        ]\n",
    "        for column in categorical_features:\n",
    "            self.df[column] = self.df[column].replace(np.nan, \"\").apply(str)\n",
    "        X = self.df[columns_model]\n",
    "        y =self.df[\"Valeur fonciere\"]\n",
    "        # selection des variables\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            test_size=0.33,\n",
    "                                                            random_state=42)\n",
    "        return self.df,categorical_features, numerical_features, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "#from house_prediction_package.preprocessing import Preprocessing\n",
    "#from house_prediction_package.data import GetData\n",
    "\n",
    "\n",
    "class Pipeline :\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        # self.categorical_features = categorical_features\n",
    "        # self.numerical_features = numerical_features\n",
    "        # self.X_train = X_train\n",
    "        # self.y_train = y_train\n",
    "        # option 2\n",
    "        #appeler les méthodes\n",
    "        self.df, self.categorical_features, self.numerical_features, self.X_train, self.X_test, self.y_train, self.y_test = Preprocessing(\n",
    "            df).feature_generation().zscore().split_x_y()\n",
    "\n",
    "    def pipeline(self):\n",
    "        # création des pipelines de pré-processing pour les variables numériques et catégorielles\n",
    "        #ajout d'un parametre pour gerer les valeures non connues dans onehotencoder - il les passe à 0(autres options disponibles)\n",
    "        numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "        categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, self.numerical_features),\n",
    "            (categorical_pipeline, self.categorical_features))\n",
    "        model = make_pipeline(preprocessor, LinearRegression())\n",
    "        fitted_model = model.fit(self.X_train, self.y_train)\n",
    "        return fitted_model, self.X_train, self.y_train,self.X_test, self.y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zone de tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model sans aggrégation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = GetData().read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test = Preprocessing(df).split_x_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())\n",
    "fitted_model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_model_wo_aggregations.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model maison avec aggrégation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= GetData().read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).clean_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= Preprocessing(df).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).aggregate_transactions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"aggregatedfile_houses.csv\", sep='|', encoding=\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).feature_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).zscore() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test = = Preprocessing(df).split_x_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())\n",
    "fitted_model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R(MSE): %.2f\" % math.sqrt(np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_model_aggregations.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test model maison/dep with aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= GetData().read_csv([1,3])\n",
    "\n",
    "df = Preprocessing(df).clean_columns()\n",
    "\n",
    "df= Preprocessing(df).create_identifier()\n",
    "\n",
    "df = Preprocessing(df).aggregate_transactions()\n",
    "\n",
    "df = Preprocessing(df).feature_generation()\n",
    "\n",
    "df = Preprocessing(df).zscore() \n",
    "\n",
    "df,categorical_features, numerical_features, X_train, X_test, y_train, y_test = = Preprocessing(df).split_x_y()\n",
    "\n",
    "numerical_pipeline = make_pipeline(KNNImputer(n_neighbors=3), MinMaxScaler())\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "preprocessor = make_column_transformer(\n",
    "            (numerical_pipeline, numerical_features),\n",
    "            (categorical_pipeline, categorical_features))\n",
    "model = make_pipeline(preprocessor, LinearRegression())\n",
    "fitted_model = model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "test_y_hat = fitted_model.predict(X_test)\n",
    "print('Score r²: ', fitted_model.score(X_test, y_test))\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\n",
    "print(\"Residual  of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R(MSE): %.2f\" % math.sqrt(np.mean((test_y_hat - y_test)**2))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y_hat, y_test))\n",
    "\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'house_dep_model_aggregations.sav'\n",
    "pickle.dump(fitted_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cas de tests sur aggregations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= GetData().read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df).clean_columns()\n",
    "\n",
    "df= Preprocessing(df).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "\"surface_terrain\" : ((x[\"Surface terrain\"].sum()/x[\"Surface reelle bati\"].count()) if (int(x[\"Surface terrain\"].nunique()) ==1 and int(x[\"Nature culture\"].nunique()) == 1 )else x[\"Surface terrain\"].sum())\n",
    "    ,\"Nature_culture\" : x[\"Nature culture\"].max()\n",
    "    , \"su terrain 2\": x[\"Surface terrain\"].sum()\n",
    "    , \"suterrainmax\": x[\"Surface terrain\"].max()\n",
    "  #  , \"su_bat\": x[\"Surface reelle bati\"]\n",
    "    ,\"nat_terrain_unique\": x[\"Nature culture\"].nunique()\n",
    "    , \"suterrain_count\" : x[\"Surface terrain\"].count()\n",
    "    ,\"suterrain_unique\": x[\"Surface terrain\"].nunique()\n",
    "    ,\"su_bat_unique\" : x[\"Surface reelle bati\"].nunique()\n",
    "    ,\"su_bat_count\" : x[\"Surface reelle bati\"].count()\n",
    "            \n",
    "})).tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby([\"parcelle_cad_section\",\"Date mutation\",\"Valeur fonciere\"], as_index= False).apply(lambda x : pd.Series({\n",
    "    \"surface_reelle_bati\" : (x[\"Surface reelle bati\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if (int(x[\"Nature culture\"].nunique() > 1)) else x[\"Surface reelle bati\"].sum())\n",
    " ,\"nb_pieces_principales\" : (x[\"Nombre pieces principales\"].sum()/(x[\"Surface reelle bati\"].count()/x[\"Type local\"].nunique()) if int(x[\"Nature culture\"].nunique()) > 1 else x[\"Nombre pieces principales\"].sum())      \n",
    "    ,\"nb_piecemax\" : x[\"Nombre pieces principales\"].max()\n",
    "    ,\"Nature_culture\" : x[\"Nature culture\"].max()\n",
    "    , \"su bat 2\": x[\"Surface reelle bati\"].sum()\n",
    "    , \"sumax\": x[\"Surface reelle bati\"].max()\n",
    "  #  , \"su_bat\": x[\"Surface reelle bati\"]\n",
    "    , \"su_count\" : x[\"Surface reelle bati\"].count()\n",
    "    ,\"nat_cul_unique\": x[\"Nature culture\"].nunique()\n",
    "    ,\"subatiment_unique\": x[\"Surface reelle bati\"].nunique()\n",
    "            \n",
    "})).tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tot= GetData().read_csv([1,3])\n",
    "\n",
    "df_tot = Preprocessing(df_tot).clean_columns()\n",
    "\n",
    "df_tot= Preprocessing(df_tot).create_identifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cas ou type local identique mais nature culture différente: \n",
    "\n",
    "df[(df['parcelle_cadastrale']== '01289000AC0176') | (df['parcelle_cadastrale']== '013500000C1248')| (df['parcelle_cadastrale']== '01195000AD0050')]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# meme de surface reelle bati \n",
    "#pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "#meme nombre pieces principales \n",
    "# différente nature culture (variable texte )\n",
    "# différente surface terrain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['parcelle_cad_section']=='01001000ZH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# même maison surface reelle bati identique \n",
    "df[(df['parcelle_cadastrale'] == '013500000C1248')]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# meêm type local \n",
    "# même surface relle bati\n",
    " #pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "# même nombre de pieces principales\n",
    "\n",
    "# différence nature culture (variable texte )\n",
    "# surface terrain différente en fonction de la parcelle cadastrale \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cas ou 2 maisons , une dépendance et un terrain :\n",
    "# repérable par section et date commune \n",
    "\n",
    "df[df.index.isin([71,72,73,74])]\n",
    "#actions possibles : \n",
    "# meme valeur fonciere \n",
    "# code type lcoal différent pr dépendance absent pour terrain\n",
    "# différence de surface reelle bati  (0 dépendance et nan pour terrain)\n",
    "#pas d'info sur 1er lot\n",
    "# pas d info Nombre de lots \n",
    "# différence sur nombre de pieces principales\n",
    "\n",
    "# différence nature culture (variable texte )\n",
    "# surface terrain différente en fonction de la parcelle cadastrale \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "7e331f8398536a24290f874ffd288d2ba0d3938ea5219b401a264de1a09e8807"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
